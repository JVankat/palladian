config.version = 11
### version history:
### 11: added DatasetEvaluator to config
### 10: added date extraction model to config
### 9: SchedulerTask fine tuning monitoring 
### 8: model path to English and German wiktionary DB
### 7: errorMailNotification in SchedulerTask
### 6: renamed configuration options for ProxySwitcher
### 5: added SendMail config
### 4: added thread pool size for DocumentRetriever, changed timeouts
### 3: added thread pool size for MetadataCreator
### 2: added timeouts for DocumentRetriever
### 1: initial version

######################## Model Paths ########################

models.root = data/models/
models.opennlp.en.postag = opennlp/postag/en-pos-maxent.bin
models.opennlp.en.postag.dict = opennlp/postag/tagdict.txt
models.opennlp.en.tokenize = opennlp/tokenize/en-token.bin
models.opennlp.en.chunker = opennlp/chunker/en-chunker.bin
models.opennlp.en.coref = opennlp/coref/
models.opennlp.en.parser = opennlp/parser/en-parser-chunking.bin
models.opennlp.en.sentdetect = opennlp/sentdetect/en-sent.bin
models.opennlp.en.ner.time = opennlp/namefind/en-ner-time.bin
models.opennlp.en.ner.date = opennlp/namefind/en-ner-date.bin
models.opennlp.en.ner.location = opennlp/namefind/en-ner-location.bin
models.opennlp.en.ner.money = opennlp/namefind/en-ner-money.bin
models.opennlp.en.ner.organization = opennlp/namefind/en-ner-organization.bin
models.opennlp.en.ner.percentage = opennlp/namefind/en-ner-percentage.bin
models.opennlp.en.ner.person = opennlp/namefind/en-ner-person.bin

models.lingpipe.en.postag = lingpipe/pos-en-general-brown.HiddenMarkovModel
models.lingpipe.en.ner = lingpipe/ne-en-news-muc6.AbstractCharLmRescoringChunker

models.palladian.en.event.who = palladian/event/who.model
models.palladian.en.event.where = palladian/event/where.model
models.palladian.mio = palladian/mio/MIOClassifierLinearRegression.model

models.palladian.language.wiktionary_en = palladian/language/wiktionary_en/
models.palladian.language.wiktionary_de = palladian/language/wiktionary_de/

models.palladian.date.published = palladian/date/pubClassifierFinal.model
models.palladian.date.modified =  palladian/date/modClassifierFinal.model

######################## Dataset Paths ########################

# Path to the DeliciousT140 Dataset, downloadable from
# http://nlp.uned.es/social-tagging/delicioust140/

# download both archives (attention: over 7 GB unzipped) and put their contents 
# "fdocuments" and "taginfo.xml" in the specified directory

datasets.root = data/datasets/
datasets.delicoust140 = /Users/pk/Studium/Diplomarbeit/delicioust140/
datasets.boilerplate = /home/pk/PalladianData/datasets/ContentExtraction/L3S-GN1-20100130203947-00001/

######################## Database ########################

db.driver = com.mysql.jdbc.Driver
db.jdbcUrl = jdbc:mysql://localhost:3306/tudiirdb?useServerPrepStmts=false&cachePrepStmts=false&useUnicode=true&characterEncoding=UTF-8
db.username = root
db.password = 

# false = persistent, true = RAM only (10x faster) (only for H2 databse)
db.inMemoryMode = true 

######################## DocumentRetriever ########################

# timeouts in milliseconds
documentRetriever.connectionTimeout = 10000
documentRetriever.socketTimeout = 180000

# maximum number of retries if download request fails, default is 0 and means no retry
documentRetriever.numRetries = 0

# number of connections opened by the connection pool
documentRetriever.numConnections = 100

# enables feed auto discovery for every parsed page
# see feeds.conf to configure path to the file.
# disabled for now.
# documentRetriever.feedAutoDiscovery = false

######################## ProxySwitcher ########################

#  number of request before switching to another proxy, default is -1 and means never switch
proxySwitcher.switchRequests = -1

# list of proxies to choose from
# see: http://proxy-list.org/en/index.php
proxySwitcher.proxyList = 190.128.224.82:8081
proxySwitcher.proxyList = 201.38.16.234:8090
proxySwitcher.proxyList = 201.12.130.211:3128

######################## Classification ########################
# page classification configurations

# percentage of the training/testing file to use as training data
classification.page.trainingPercentage = 80

# create dictionary on the fly (lowers memory consumption but is slower)
classification.page.createDictionaryIteratively = false

# alternative algorithm for n-gram finding (lowers memory consumption but is slower)
classification.page.createDictionaryNGramSearchMode = false

# index type of the classifier:
# 1: use database with single table (fast but not normalized and more disk space needed)
# 2: use database with 3 tables (normalized and less disk space needed but slightly slower)
# 3: use lucene index on disk (slow)
classification.page.dictionaryClassifierIndexType = 2

# if page.dictionaryClassifierIndexType = 1 or 2, specify type of database
# 1: mysql (client server)
# 2: h2 (embedded)
classification.page.databaseType = 2

######################## API keys ########################
api.alchemy.key = b0ec6f30acfb22472f458eec1d1acf7f8e8da4f5
api.bing.key = D35DE1803D6F6F03AB5044430997A91924AD347A
api.google.key = ABQIAAAA7H0y5xnSFd668h5iwC-EqRQUCodsAOOOxFO2WQpCHSyvFhJDHxTj6f5-8hxq9u2Us0fLG1ZqBA6I-g
api.google.tranlsate.key = AIzaSyD-fJ-e3c70PMCdqB2q30BI9d_tFty25Lk
api.hakia.key = ROTDG-43NA1-HDBNF-M3V4O-NCDJ5
api.opencalais.key = mx2g74ej2qd4xpqdkrmnyny5
api.yahoo.key = Isx7uavV34E_iKMIuvEZB0pCUQPuBmxBttBxyXX3UtXNtl7uYkoDAKbFa6c-
api.yahoo_boss.key = R7ZPxa7V34FK8UPp4HFddNFJNdGcZoIHXsyvvlK1.5iPbiJ9cCS39LbWIfs-
api.bitly.login = qqilihq
api.bitly.key = R_2f0606b6760dd5e567afdf5e4ddbec2d
api.mixx.key = uoRkXpXlVZBNgQZfn7VR0w
api.reddit.username = qqilihq
api.reddit.password = amcocuymzu

# access to Collecta XMPP and REST API
# see: http://developer.collecta.com/APIindex/
api.collecta.key = aef13a5bc659dc610cf6ad300da7994e

# TODO I extracted API keys for majestic+compete from "Seo for Firefox" extension
# http://tools.seobook.com/firefox/seo-for-firefox.html
# we should get our own ones sooner or later
api.majestic.key = 2F85C7EF65

# Rankify API keys
# use backtweets.com to get number of tweets for a specific url
api.backtweets.key = 4f2b553e8de8a336a6a7
# BibSonomy is a bookmark service especially for publications/papers
api.bibsonomy.login = jumehl
api.bibsonomy.key = e954a3a053193c36283af8a760918302
# Google Buzz
api.google.buzz.key = AIzaSyBBuf8UDOkcq1Dg_NqfqF_Ggpc5F9hv0mM
# Facebook - Rankify App -> not used
api.facebook.key = 3e2b3878a8c85aa29b7aad1907f01499
api.facebook.secret = 6d6208bc31599b44cf856662b28341da
# Plurk.com - Rankify App
api.plurk.key = wNvoEnXhJei2IZat8f8YuAKHMDq79mTV
# Linked.in - Rankify App
api.linkedin.key = oSuIRh0O-pGFYcxW3pn43JXDygwfLL26-fhu3Y2hEffES9954u7pyuTJbLP8NDnQ
api.linkedin.secret = wvlBp_3_NOJdi76I4L82MZmArR7HX0O2HydYMHHYFnFOONSNjEELSnf55tE7EoxO
# ShareThis
api.sharethis.key = 94d2fab0-cbf6-4798-86ec-11a0fae8d48a
api.sharethis.secret = 5e06f20138b94a109ac1d079ef59b373

api.compete.key = 985582a7e0dc15d222f90423ed64f4ad

######################## Feed ########################
# search engine for discovery, see SourceRetrieverManager for avail. constants
# for example ...: Yahoo Boss = 5, Yahoo Boss News = 10
feedDiscovery.searchEngine = 6

# maximum number of threads for discovery
feedDiscovery.maxDiscoveryThreads = 10

# location of the list for feeds discovered by the DocumentRetriever
feedDiscovery.crawlerDiscoveryList = data/status/crawler_discovered_feeds.txt

# number of threads in the pool for the FeedReader
feedReader.threadPoolSize = 100

# number of threads in the pool for the MetadataCreator
metaInformationCreator.threadPoolSize = 500

# if false, do not check system limitations like number of available file descriptors 
feedReader.checkSystemLimitations=true

# if true, detected errors will be send by mail 
schedulerTask.errorMailNotification=false 

# The receipient of email notifications.
#schedulerTask.emailRecipient

# This many percent of the feeds processed per interval are allowed to be slow.
schedulerTask.maxSlowPercentage = 10

# This many percent of the feeds processed per interval are allowed to be unparsable.
schedulerTask.maxUnparsablePercentage = 2

# This many percent of the feeds processed per interval are allowed to be unreachable.
schedulerTask.maxUnreachablePercentage = 2

# Choose update strategy to evaluate. Supported values are: 
# Fix, FixLearned, 
datasetEvaluator.updateStrategy = Fix

# If datasetEvaluator.updateStrategy = Fix, choose a fix check interval in minutes >0, e.g. 60 for Fix1h
datasetEvaluator.fixCheckInterval = 60

# Set min check interval in minutes > 0. Update strategies must not poll more often than this interval. 
datasetEvaluator.minCheckInterval = 1

# Set max check interval in minutes > 0. Update strategies must not poll less often than this interval. 
datasetEvaluator.maxCheckInterval = 1440

# Choose benchmark mode. Supported values are: 
# poll, time 
datasetEvaluator.benchmarkMode = time

######################## Page Segmentation ########################

# length of q-grams
pageSegmentation.lengthOfQGrams = 9

# amount of q-grams
pageSegmentation.amountOfQGrams = 5000

# threshold needed to be similar
pageSegmentation.similarityNeed = 0.689

# maximal depth in DOM tree
pageSegmentation.maxDepth = 100

# number of similar documents needed
pageSegmentation.numberOfSimilarDocuments = 5

# threshold of variability
# from green to red; from low variability to high variability
pageSegmentation.step1 = 0.0
pageSegmentation.step2 = 0.14
pageSegmentation.step3 = 0.28
pageSegmentation.step4 = 0.42
pageSegmentation.step5 = 0.58
pageSegmentation.step6 = 0.72
pageSegmentation.step7 = 0.86

######################## Email Notification #######################
sendMail.smtpHost = 
sendMail.smtpPort = 
sendMail.smtpUser = 
sendMail.smtpPass =

######################## Threshold Dateextraction ########################
threshold.group1 = 0.15
threshold.group2 = 0.24
threshold.group3 = 0.18
threshold.group4 = 0.16
threshold.group5 = 0.14
threshold.group6 = 0.13
threshold.group7 = 0.17
threshold.group8 = 0.26
