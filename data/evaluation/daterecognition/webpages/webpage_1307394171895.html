The Economist

Question, 22 April, E

A question of balance
Japan will continue to identify more strongly with the West than with Asia.

Each recent surge in the yen has given fresh impetus to a newly modish
argument: that Japan is tilting inexorably towards Asia, binding itself into a
"yen block" there and distancing itself from the West. The argument takes as
its starting-point the growth of Japan's investment and trade links with Asia,
but goes on to claim that the patterns of Japan's diplomacy and even its
self-image are also changing -- that, as Asia's importance to Japan grows, so
Japan is ready to take Asia's side against America on issues such as human
rights and trade.
  Thus, it is said, Japan has been quick to forgive autocratic clamp-downs in
China and Myanmar; it sympathises with Asians who see free trade as the wrong
policy for developing countries; it identifies more deeply with Asian ways and
Asian values, breaking free of a century-long eagerness to imitate Europe and
America. A contrast is drawn with those other, much-trumpeted attempts to
create regional blocks, the North American Free Trade Agreement and the
European Union, which seem in danger now of coming apart at the seams: the
integration of Asia is held to be proceeding with much greater ease and much
less fuss.
  But when a voguish idea is in danger of becoming a conventional wisdom, that
is usually the time to question it. Such is the case with this view of Japan's
ties to Asia. Before too long, and before most people expect, these ties may
start to loosen. This is so partly because the pace of Japan's economic
integration with Asia may slow; but also because integration will not
necessarily push Japan to take Asia's side in global disputes. Indeed, the
opposite may prove true. Japan's growing stake in the region may lead it into
conflicts with other Asian countries.

Horrified by Hollywood
Start with the pattern of Japan's foreign direct investment. The stronger the
yen, the greater the pressure on Japanese firms to shift manufacturing to
cheaper bases overseas. Since 1992 nearly a fifth of Japan's foreign direct
investment has gone to Asia, and some think that share is set to grow at an
extraordinary rate. A survey conducted recently by Japan's Export-Import Bank
suggested that, over the coming three years, three-quarters of Japan's new
foreign direct investment will go to Asia.
  Hence the first proposition behind talk of Japan's Asian tilt: the higher the
yen, the more Japanese factories in Asia. Yet such a link is hardly automatic.
Until recently the rising yen was doing more to boost Japanese investment
farther afield. In the five years from 1985, when the yen began its sharp rise,
roughly two-thirds of Japan's direct investment went to America and Europe and
barely a tenth went to Asia. There is no necessary reason why, rather than
rising to ever greater heights, Japan's interest in Asia should not revert to
its old levels.
  Asia's boosters argue that Japanese investment in the region has risen
because of the economic boom there, and that this boom will continue. Yet some
of Japan's current enthusiasm for Asia looks suspiciously like a bubble. For
the past three years the Export-Import Bank's annual surveys have found China
to be much the most popular country for Japanese firms to invest in. But
confidence in China is a fragile thing. The country is governed, in so far as
it is governed at all, by old autocrats whose grip must one day fail. There are
no credible laws protecting foreign investors' money.
  Even without counting China, Asia has claimed a gently rising share of
Japan's direct investment: 15% in the half-year to September, up from 11% in
1985-89. But this has been a product less of any huge enthusiasm for the region
than of a marked disillusionment with Europe and America. Official surveys of
Japanese manufactures suggest that their Asian subsidiaries enjoyed a
respectable 4% profit margin in the year to March 1994; subsidiaries in America
made little or nothing; those in Europe made, on average, losses equal to 1% of
turnover.
  As a result, Japan's recession-battered firms are cutting back their
investments outside Asia: thus Matsushita, a consumer-electronics firm, is
selling most of its stake in MCA, the Hollywood entertainment group it bought
five years ago. It is this retreat which creates statistics showing a rising
share of Japanese investment heading into Asia. In absolute terms, the
Asia-bound flow hit a peak in 1989 that it has yet to regain.
  So Asia's boom has not in fact brought a sudden charge of Japanese investment
to the region. The advance may gather pace when Japan's economy eventually
recovers; but there is a limit to how many factories the Japanese will build in
cheap Asian sites to make basic products. Asia's boosters point out that
Japanese manufacturers still produce only around 7%  of their output abroad,
compared with 25% for American firms: this suggests lots of room for expansion.
Yet this comparison is misleading.
  Japan's low figure reflects, in part, its army of small firms, which lack the
resources to build factories overseas. Japan's bigger firms, especially those
operating in industries in which Asia excels, have already spread around the
globe. According to the Export-Import Bank, Japanese electrical firms with
foreign subsidiaries already do 18% of their assembly and 23% of their
component-making abroad. The bank expects these figures to rise fast over the
next three years. Soon after that, however, Japan's direct investment in Asia
will approach saturation.
  Moreover, as Japan's investments in Asia mature, their significance for
Japan's view of the world may alter subtly. The first years of a new factory
are alive with human exchange: not only managers, but blue-collars workers too,
are dispatched from the parent company to help train locals. As time goes by,
expensive expatriates are replaced by cheaper local managers. On paper, the
stock of Japanese investment in Asia will doubtless continue to grow; but
shoulder-rubbing between Japanese and other Asians is likely to diminish
nonetheless.
  Japan's commercial involvement in America, by contrast, is nowhere near the
point of saturation. As more basic manufacturing jobs desert Japan, so Japan
will be forced to earn its living in sophisticated ways. It will stand or fall
by its strength in applied science, its command of the latest marketing ideas,
its ability to design software or financial instruments. To thrive in these
brain-intensive fields, Japan will need to be part of an international exchange
of ideas, conducted among specialists dotted around the rich world, usually in
English. Some of these specialists will be in Asia. But most will be in America
or Europe.

Tangled up in trade
Just as recent investment trends may give a misleading view of where Japan's
future economic interests lie, so too may recent trade trends. Viewed over the
past decade, Japan's Asian tilt appears profound. In 1985 Japan exported a
third more to America than it did to Asia. Now it exports a quarter more to
Asia, than to America; and nearly three times as much to Asia as to Europe. The
question is how long this trend will be sustained.
  The rise of Japan's exports to Asia has been tied to a structural change. As
the strong yen has driven manufacturing abroad, so the share of consumer goods
in Japan's exports has fallen -- from 31% in 1985 to 20% last year. Japan's
cars and video cameras are sold mainly to the rich consumers of the West.
Conversely, most of Japan's green-field factories are in Asia: they have been
filled with machines and components supplied by their parent companies. As a
result, capital goods, including components, rose as a share of Japanese
exports from 47% in 1985 to 60% last year. But this trend will falter as
Japan's manufacturing investment in Asia approaches saturation point. Once a
new factory has been kitted out, its appetite for new machines is small. And,
as it matures, its managers tend to buy more components locally.
  The current boom in Japan's exports to Asia looks unlikely to last,
therefore. The growth in Japan's imports from Asia -- which have risen about
150% since 1985 -- seems considerably more durable. This is because Japan is
adjusting to the strong yen more gradually at home than in foreign markets.
Japanese firms have lost no time in building new factories abroad. But they are
slow to react in the other obvious way, which is to import more. Manufacturers
in Japan are reluctant to ditch local parts makers with whom they have worked
for years; retailers hesitate to abandon faithful suppliers. In a few areas,
such as agricultural products, regulations block or limit imports.
  Such factors meant that the yen's rise in 1985-88 was accompanied,
perversely, by a fall in imports as a percentage of Japan's GNP, from 9.6% to
6.4%. Japanese firms cut profit margins and costs in order to fight off the
foreign challenge. By now, however, these tactics have been largely exhausted.
Since the yen renewed its sharp rise in 1992 the share of imports in GNP has
stuck at around 6%, meaning that the fall in import prices brought about by the
strong yen has been balanced by rising import volumes.
  Having started late, the impact of the yen's rise on imports can be expected
to last for some time yet -- especially since the power of red tape and
relationships is gradually diminishing. This, together with slowing exports to
Asia, suggests that Japan's trade surplus with the region, which is currently
growing, will shrink and eventually disappear. This holds out an intriguing
possibility. Asia-boosters may be right that Japan's trade with Asia will
continue to grow -- just as Europe's and America's trade with Asia will do, for
that matter. But the effect, in diplomatic terms, may not be to promote a
Japanese-Asian entente.
  Rather than acquiring a stake in the region that leads it to take Asia's
side, Japan may instead acquire an irritating trade deficit. The trade rows of
the past 20 years pitting Japan against America and Europe may be displaced by
a new line-up in which the rich triad squabbles with manufacturing-intensive
Asia. Far from taking Asia's side, Japan may be driven by the development of
its Asian trade into siding more with westerners.

Blocked assets
The coming trade friction with Asia may be hastened by another of the
sub-trends that make up Japan's supposed Asian transformation. This is the rise
of a "yen block". Until now, Asian countries have pegged their currencies to
the dollar: rigidly in the case of Hong Kong, loosely in the case of most of
other economies. But the dollar's fall has prompted second thoughts,
particularly among Asia's more advanced economies. For a poor country such as
Indonesia, pegging the currency to a devaluing dollar may spur demand for
exports usefully. In richer economies already operating at full capacity, extra
demand is likely to boost inflation more readily than it does output.
  C.H. Kwan, an economist at the Nomura Research Institute in Tokyo,
illustrates this point by comparing Hong Kong and Singapore. Thanks partly to
the rigid pegging of the Hong Kong dollar, Hong Kong has suffered inflation
averaging 9.6% since 1990. Singapore more flexible exchange-rate policy has
helped keep inflation there down to 3% over the period. Other Asian governments
seem to have noticed this phenomenon. Last year South Korea raised the value of
the won against the dollar in order to staunch inflation. Thailand may revise
the basket of currencies against which it tracks the baht, so as to reduce the
dollar's weighting from a current level of 80% and give more weight to the yen.
  As Asian currencies move closer to Japan's, Asian firms will denominate more
of their trade in yen, particularly when dealing with Japanese partners. Thus
far, however, Japan's power and policies in Asia will be scarcely changed by
the emergence of a "yen block", whatever that phrase may be taken to imply.
From the point of view of Japan's position in the world, it is a third aspect
of the yen's internationalisation that matters most: the rise of
yen-denominated lending.
  Since 1985 Japan has been the world's biggest creditor. Last year foreigners
paid Japan a net $41 billion in interest and dividends -- a sum equivalent to
30% of Japan's trade surplus, according to Alex Kinmont of Morgan Stanley, an
American investment bank. Fortunately for the foreigners, most of the loans
they have taken from Japan are in dollars; the yen's rise has not increased the
burden of repayment. Japanese lenders, on the other hand, have suffered a
galling fall in the yen value of their investment income.
  Unsurprisingly, the Japanese have grown fed up with this arrangement.
Insurance firms and banks have grown keener to lend in yen, and wary of foreign
currencies. The most dramatic evidence of this change has come in the
international bond markets. Increasingly, Eurobonds are denominated in yen:
last year non-Japanese governments and firms issued �5.6 trillion worth of
Euroyen bonds, up from an annual �1.8 trillion in 1991-93. For the first time,
more Eurobonds were denominated in yen than in D-marks (although dollar bonds
remain easily the most common).
  As the stock of yen debt grows, a strong yen will inflict increasing pain
upon Japan's foreign debtors. This will present Japan with some awkward
options. It may have to accept that some debtors will default; it may try to
fight the yen's rise by encouraging more imports, in the expectation that as
its current-account surplus subsides, so the prospect of yet more upward
pressure on the yen will diminish.
  Either way, Japan will be lucky to avoid tension with Asia. If Japan opens
up, Asian imports will drive weak Japanese firms to the wall and so provoke
trade friction. If Japan stays closed, there is a fair chance that many of the
defaults on yen loans will take place in Asia.
  If a "yen block" does take shape, Asian firms may well borrow more in yen;
Japan's investment banks, which have a natural advantage in underwriting yen
bonds, have made cultivating Asian clients their top priority of late. But
Japan's past lending to other Asian governments already offers a glimpse of one
possible result. Asian governments hold 30% of their long-term debt in yen,
compared with 12% for all developing countries. This has not promoted
Asian-Japanese harmony. uite the reverse. The yen's appreciation has caused the
Philippines to demand the rescheduling of its repayments, and has left
Malaysia, Indonesia and China grumbling that Japan should compensate borrowers
for the misbehaviour of its currency.

The other half of the picture
The growth of Japan's investment in, and trade with, Asia, together with the
rise of a "yen block", may thus promote more conflict than friendship between
Japan and Asia. Japan's workers will resent any flood of cheap Asian imports.
Japan's financiers will seethe at the fecklessness of defaulting Asian
borrowers. And, once the current burst of manufacturing investment in Asia dies
away, Japan managers will turn their gaze back to the laboratories and business
schools of Europe and America.
  As Japan's economic interests swing back to the West, the fashionable talk of
common "Asian values" is also likely to fade remarkably quickly. Many of these
values -- respecting one's elders, for example, and emphasising social order
rather than individual freedom -- are not particularly Asian, and can be found
in many developing countries around the world. The difference is that
super-successful development has given Asians the confidence to assert these
values boldly.
  Remembering its own recent past as a developing economy, Japan tends to
tolerate Asian failings that westerners are rude about. Much of Asia is
autocratic; but then Japan is only now developing lively democracy. Much of
Asia is protectionist; Japan was too, and old habits die hard. As a newcomer to
the world's top table, Japan has a natural sympathy with other young countries
competing for places lower down. Kazuo Ogura, Japan's ambassador in Vietnam,
declares that, if the West wants Asia's rising economic powers to play a
responsible role in the world, it must be prepared to listen more politely to
Asian opinions.
  This sort of tolerance, mixed with commercial opportunism, lies behind
Japan's recent willingness to challenge American foreign policy in Asia. Japan
resumed aid to China only a year after the Tiananmen massacre; it broke
America's economic embargo on Vietnam; it is currently doing deals with
Myanmar's sinister junta; it is flirting with Malaysia's proposal for a
regional economic caucus that would exclude America. Generally, Japan has
avoided backing American and European criticism of protectionist countries such
as China.
  But the fact is that, however fresh its memories of development, Japan is now
the world's richest country. Money brings urbanism, travel and education,
things which erase the values promoted by Asianists. Young Japanese show no
particular respect for elders; the extended family has yielded to the nuclear
one; crime is edging up. In short Japanese society is evolving in a way that
promises more identification with western countries, not less.
  Japan's security interests are evolving likewise. In the aftermath of the
cold war some Japanese were quick to conclude that they need no longer depend
so much on American might, and so could allow themselves the luxury of siding
against America. But the cold war's end has turned out to mean more, not less,
incentive for Japan to line up behind America. The old Soviet Union posed a
direct threat to America, so the Americans needed an alliance with Japan as
well as the other way around. The new threats -- from North Korea, from China
-- are more local. If the America-Japan alliance is undermined by voguish
Asianism, Japan will have most to lose.
  The new conventional wisdom, in short, misses half the picture. Japan may be
tilting toward Asia -- but its centre of gravity is staying put. To manage the
coming conflicts, Japan will no doubt talk up its commitment to Asia. It will
back regional talk-shops such as the Asia-Pacific Economic Cooperation (APEC)
forum, the presidency of which it holds this year. But, unless it loses sight
of its own interests, Japan will not allow its Asian diplomacy to diminish its
ties with the West. Rather, Japan's stake in Asia is likely to make it a still
more valued ally of America and Europe in containing future Asian trade wars.
Or real wars, come to that.



Battle, 7 October, T

The battle for world power
Coal, gas and oil will not be the three kings of the energy world for ever. It
is no longer folly to look up to the sun and wind, down into the sea's waves.

Once it was the province of mad scientists and dreamers. Renewable energy, they
said, was the coming miracle. Anyone willing to listen would be told rapturous
tales about the benefits of sun power, wind power and sea power. These things
would free countries from their dependence on a handful of dodgy Middle Eastern
regimes; they would blow away smog and dismiss the fear that evil fumes might
be changing the globe's climate. And how bountiful the prospect is, the
dreamers would exclaim: each year the earth gets from the sun alone 10,000
times the energy it needs to use.
  Until recently, the dreamers would then be woken up to reality by economists
shouting "Costs!". For renewable energy has long been hopelessly more expensive
than energy produced by the fossil fuels -- coal, oil and gas. Without
subsidies, not a joule of renewable energy would have been generated.
  No longer. Little noticed, the costs of many renewables have recently been
tumbling. Fossil fuels are still almost always cheaper. But a battle has begun
on the fringes of the mighty $1-trillion-a-year fossil-fuel industry that could
force it into retreat early in the coming century.
  At first sight, the fossil-fuel empire looks impregnable. The bosses of oil,
gas and coal business in Tokyo for the World Energy Congress starting on
October 8th are in no mood for retreat. Fossil fuels supply over three-quarters
of the world's energy needs. Much of the remaining quarter is met by poor-world
fuels such as wood, crop residues and dung. As poor countries grow richer,
these primitive fuels will not suffice; oil, gas and coal will push them aside.
  In the energy needed to move transport, oil is still king, supplying 97% of
the fuel used. Only in the production of electricity have the alternatives to
fossil fuels yet made any sort of impact. Nuclear power provides 17% of the
world's electricity, and hydroelectric power 18%. But both are relatively old,
and both are controversial. Truly modern renewables, such as solar and wind
power, provide less than 1% of world electricity.
  So what could loosen the grip of fossil fuels? Not, let it be made clear, any
possibility that there will soon be enough of the stuff around the place.
During the oil shocks of the 1970s, many people confused supply restrictions
imposed by OPEC, the oil cartel, with the false idea that the world was running
out of oil. In fact, the potential supply of oil has increased since the 1970s.
  Proven reserves of oil are now enough to supply the world for 43 years at
current rates of production, compared with less than 35 years during the 1970s,
according to BP, one of the oil giants. Proven reserves of natural gas now
stand at 66 years' worth of current production, up from 44 years in 1970. Coal
reserves will stubbornly last for 235 years of production at current levels.
  Of course, rates of production will not stay unchanged. The world's demand
for energy could more than double by 2025, as populations soar and poor
countries industrialise. But, even given the growth in demand, the proven
reserve figures probably underestimate the longevity of fossil fuels. "I don't
believe we will be out of oil in 40 years," says Sean O'Dell, chief economist
of the International Energy Agency (IEA).
  The phrase "proven reserves" merely refers to already known reserves of oil
that energy firms reckon they can extract without losing money. Who knows how
much more oil will be discovered in the future? The new technology of oil
exploration allows firms to see three-dimensional seismic images of oil fields
and to drill into them horizontally. Since the 1970s the OPEC countries of the
Middle East have discovered that they sit on far more oil than they had
realised. The prospects elsewhere may grow too.
  And who can predict the future economics of fossil-fuel extraction? Any sign
that known reserves of oil, gas or coal are running out will push up their
prices. But that in turn will bring on to the market reserves that had
previously been rejected as too dear to exploit -- oil in the deepest waters of
the ocean, coal in the remotest parts of Siberia. Such "ultimately recoverable"
fossil-fuel reserves, the World Bank says, may contain over 600 years' worth of
current production.

There are reasons to be nervous
So fossil fuels are not about to be exhausted. Even so, three things may soon
undermine their dominance of the energy world: the market's fear that
fossil-fuel costs may rocket; worry about the environment; and the falling cost
of alternatives. Take the three in turn.
  "Energy insecurity" is another legacy of the 1970s' oil shocks. But, unlike
the fear that fossil fuels are running out, this one has some basis. The price
of a fossil fuel can go up quite a lot if demand grows faster than supply, even
though eventually the price rise encourages producers to increase output. It is
not much good finding a new gas field, for instance, if it takes ages to build
a new pipeline to carry the gas to the market. And in the oil market, in
particular, there is the daunting factor of oilgopoly power.
  OPEC has tried to raise the oil price in recent years by restricting its
output to around 25m barrels a day. So far its efforts have failed. The recent
rise in world demand for oil (around 2% since 1991) has been met by surging
output from non-OPEC countries. Even worse for OPEC, the oil price could tumble
if Saddam Hussein behaves well enough for the UN to decide that it can let Iraq
resume its oil exports.
  But in the longer run OPEC may regain the power to push up the oil price.
Even though the cartel today supplies only 40% of the world's oil, it sits on
over 75% of the world's proven reserves. Unless the non-OPEC countries discover
large new supplies -- and make sure they can get them to the market swiftly and
inexpensively -- they will one day not be able to meet their share of the
expected growth in demand. Then OPEC could be a troublemaker again.
  A sustained rise in the price of oil would make consumers look around
desperately for alternatives. Since the 1970s, the mere possibility of another
oil shock has led many countries to invest in electricity generation from
non-fossil sources, particularly nuclear power and hydro power. Over the past
20 years, oil's share of the world's electricity market has fallen from around
20% to about 10%. Alas, it has been harder to take precautions against a new
oil shock in the field where oil is most necessary. This is transport.
  Oil continues to power almost all of the 600m vehicles on the planet. About
half of the world's oil supply is consumed in the transport sector of the
economy. The IEA predicts that by 2010 the proportion could rise to more than
60%. This is where another leap in the price of oil could hit hardest.
  Next comes the green argument against continued reliance on fossil fuels. In
many cities, these fuels still cause unsightly and unhealthy smogs. And many
scientists, despite the hazards of trying to predict complex weather-systems,
say with increasing vehemence that the use of fossil fuels could change the
world's climate. The carbon dioxide emitted by the burning of these fuels, they
argue, may overheat the globe, with disastrous results.  At the 1992 "Earth
Summit" in Rio de Janeiro, the rich countries agreed to make sure their
emissions of greenhouse gases by 2000 were no greater than they were in 1990.
  To be sure, switching to non-fossil fuels is not the only way to deal with
the problem. Gas, itself a fossil fuel, is relatively clean. To use it in place
of oil and coal would reduce carbon dioxide emissions dramatically. And, since
elderly motor cars are among the worst polluters, one way for governments to
reduce urban smog might be to buy dirty old bangers from their owners, to help
them buy more virtuous new cars.
  But green concerns may one day bring a big switch away from fossil fuels. If
they do, what would be the environmentally best alternatives? Nuclear power and
hydroelectricity are both dubious options. The operation of nuclear reactors in
some countries -- remember Chernobyl -- poses a large danger of nuclear
accidents. In other countries, including Iran and North Korea, a supposedly
innocent nuclear power programme may be linked to the production of nuclear
weapons. Although nuclear plants in the rich world are much safer, there are
still legitimate worries, not least about how to dispose of waste that can
remain radioactive for centuries ahead.
  Hydro power also has its disadvantages. Dams uproot people and animals.
China's Three Gorges dam -- a gargantuan project costing, according to one
estimate, $26 billion -- will displace more than 1m people. And hydro power may
also be climatically dangerous. Green activists in Canada and Brazil have
recently argued that rotting vegetation in dam reservoirs gives off substantial
amounts of greenhouse gases.
  No source of energy is squeaky-clean. In California and Wales giant wind
turbines have killed birds. Wave machines can disrupt marine habitats. Yet
wind, sun and wave power seem on the whole far more green than their rivals.
Unlike nuclear power, they pose no risk of environmental catastrophe. And,
unlike hydro schemes, they do not need much space. According to World Bank
calculations, solar power could, in theory, supply between five and ten times
the present electricity demand of all the developing countries while covering
less land than today's hydro schemes.

The lights that failed
If, that is, the price is right. However strong the fear of another oil shock
or a green catastrophe, people will hesitate to invest in renewables if the
energy these produce will cost more than that from fossil fuels. And, though
the price of some renewables has been tumbling, the fall has been concentrated
in the electricity sector. In transport -- the fastest-growing portion of world
energy demand -- only the boffins have yet become really excited about
alternatives.
  Transport needs oil products for three reasons: they are cheap, they do not
weigh too much and they do not take up too much space. The other ways of
keeping transport moving have repeatedly failed one or more of these tests.
Many car scientists, for example, look with longing at the hydrogen fuel cell
-- a silent energy source that emits only steam and water. But hydrogen fuel
cells are still expensive, heavy and bulky. Fuels based on vegetable oils are
recommended by subsidy-hungry farmers, but they are far more expensive than
petrol or diesel.
  The largest and most foolhardy attempt to reduce transport's dependency on
oil was Brazil's alcohol programme, begun in the late 1970s. By 1983, as a
result of hefty subsidies, 90% of all new cars were built to be fuelled by
alcohol. By 1990 that was down to 5%. The scheme withered partly because it
proved hugely expensive; to make alcohol competitive in the 1980s, oil's price
would have had to almost double, to $45 a barrel.
  Undeterred, California is conducting its own experiment in oil-free cars.
From 1998, an increasing proportion of new cars must be "zero emission
vehicles". Unlike Brazil, which was trying to reduce its dependency on oil
imports, California wants to cause less pollution. Yet its experiment too could
prove expensive. An electric car needs a battery. Today's batteries are large
and heavy, and need regular and time-consuming recharging. A car that can run
for several hundred miles on one tankful of petrol could not, today, manage
even 100 miles on a single recharge. Zealous research has so far failed to
solve the problem. So far as one can see, electric cars may be no more than
useful second vehicles for families that make regular short trips to nearby
shops and schools.
  Turn to electricity production, though, and renewables look a much better
prospect. The cost of solar thermal electricity, in which sunlight is used to
heat air or water, has fallen. So has that of biomass power, in which plant
matter is burnt to make energy. But the most enticing technologies are
photovoltaic (PV) cells and wind power.
  PV cells (semiconductor devices that generate electricity directly from
sunlight) were first used in the 950s to power space stations. They now bring
power to tens of thousands of homes in rural areas of poor countries such as
Kenya, South Africa and Brazil, often without a subsidy. A combination of
better science and economies of scale has cut the cost of PV modules (clusters
of the cells) to a fiftieth of what it was in the 1970s.

Where the land is brighter
A unit of electricity generated from photovoltaics is still far dearer than a
unit generated in a fossil-fuel power station (30-40 cents per kwh, compared
with 3-6 cents, according to estimates by ERM, an environmental consultancy).
Yet PVS are often competitive because their distribution costs are so much
smaller. Extending electricity grids from fossil-fuel plants to new consumers
can be hugely expensive. PV modules can be simply stuck on homes. In Kenya, as
a result of a thriving (and unsubsidised) trade in PV modules since the
mid-1980s, more households now get their electricity from the sun than from the
national grid.
  Now look at wind power. Even ignoring distribution costs, wind power is
within nudging distance of price equality with fossil fuels. Over the past 20
years its price has fallen from around 30 cents per kwh to 5-6 cents in the
best sites. Wind turbines have become larger; blades have been designed so as
to catch the wind more efficiently.
  Wind power used to rely on subsidies, especially in California, which has
more than the world's grid-connected wind-turbine capacity. Yet the best wind
plants are now competitive, says Roger Booth, head of renewable energy at Shell
International Petroleum Company, part of Royal Dutch-Shell, a big oil company.
According to one scenario developed by Shell, new renewables such as wind and
sun power could be satisfying around half of the world's energy demand by 2060.
  The World Bank is looking around China and India for suitable sites for PV
and wind plants which, it hopes, will require no subsidies. Its economists
expect the costs of many renewables to fall much further as technology goes on
improving and the economies of scale get even more vigorously to work.
  Wind power and photovoltaics have their risks. The sun does not always shine,
nor the wind always blow. But this need not be fatal, according to a recent
article by Christopher Flavin of the Worldwatch Institute in Washington, DC. If
supplies of energy that fluctuate for these reasons are connected to a grid,
and if they are a relatively small part (less than 20%, say) of total power
generation, utilities can manage fluctuating supply just as readily as for
decades they have managed wild hourly swings in demand.
  There are also ways of storing energy that smooth out changes in supply. Some
utilities, for example, use any excess in the supply of electricity to pump
water into high reservoirs. They can then, by a hydroelectric process, release
this energy when it is needed. Scientists are developing other storage
technologies, such as hot rocks and mechanical flywheels, costly though they
are.
  Renewables are not perfect; but they may soon provide a dramatically larger
share of power, particularly in the poorer parts of the world. Two billion
people, almost 40% of the world's population, still have no access to
electricity. Most of them live in tropical or subtropical regions, with solar
energy levels often double those in the rich world. Such places are also good
prospects for renewables such as photovoltaics, which depend for their
competitive advantage on the absence of an elecrticity grid.
  Speed of construction is on the renewables' side. Wind plants, for instance,
can take less than a year to build; nuclear plants often need a decade or more.
Size is another bonus. Renewable generating plants tend to come in small units
(their capacity is usually less than 200MW, compared with around 1,000MW for a
typical nuclear plant). Smallness can be an advantage, because patterns of
electricity demand are hard to predict and big projects often overrun their
costs. "The electricity system of the future is going to be much less lumpy,"
says Walter Patterson of London's Royal Institute of International Affairs.
  Despite all this, the world's attempt to cut down its use of fossil fuels is
still based chiefly on two established technologies, nuclear power and hydro
power. According to IEA figures, governments in rich countries spend over half
of their $8-billion-a-year energy research budgets on nuclear programmes.
Renewables get less than 10%. Poor countries (excluding the former communist
block) already generate almost 5% of their electricity from nuclear energy or
hydro power, but only 0.3% from renewables; and yet they are building dozens of
new nuclear and hydroelectric plants.
  Are nuclear and hydro really the best alternatives to fossil fuels? It is not
only on environmental grounds that many people reject them. Their economic lead
may also be slipping away. Admittedly, the best hydroelectric plants turn out
splendidly cheap electricity. But others have proved hugely expensive:
politicians too often indulge their taste for grandiosity in plans for giant
dams that take budget-busting ages to build.
  Nuclear power is uncompetitive almost everywhere. Although the cost of
nuclear power varies from country to country, most recent studies show that the
electricity it generates is more expensive than that generated from fossil
fuels. Environmental regulations on nuclear power have become tougher.
Economists at the World Bank argue that nuclear power cannot compete with
fossil fuels once the costs of decommissioning old reactors and dealing with
spent fuel are included. Worries about cost, as well as safety, have led many
rich countries to abandon their nuclear programmes.

When governments wake up
Nuclear power and big hydroelectric dams are wrong for many poor countries for
the same reasons that make renewables right. They come in capital-intensive
lumps; they are slow to construct; and they need to be attached to a grid
system.
  So long as the price of fossil fuels stays roughly where it is now, they will
continue to provide the largest part of the world's energy. On the flanks of
the battle-front, though, matters are different. As the arguments for giant
hydro and nuclear plants grow weaker, the case for renewables gets stronger.
The policy of governments has yet to catch up with the changes produced by
science, says Dennis Anderson, an energy adviser at the World Bank. It is no
longer reasonable to snigger at the dreams of those mad renewable-energy
scientists.



Big battle, 18 February, E

How one big battle was won
William Cline, an authority on a subject currently in the world's eye,
describes how the debt crisis of the 1980s was overcome, and stays calm about
the new worries that have emerged in the 1990s.

The mexican peso crisis casts new light on the problem of international debt.
As fate would have it, the crisis blew up just as my new book on the subject
went to press, though still in time for a Mexico epilogue.
  The Mexican crisis poses the question of whether the international debt
crisis is back with a vengeance just when the Brady plan of 1989 was supposed
to have solved it. The provisional answer is No. Even for Mexico itself, where
debt ratios and fiscal balances are far better than in 1981-82, the crisis is
more in the nature of an Italian exchange-rate collapse than a debt crisis of
the Mexico-1982 variety.
  Instead, the current Mexican crisis reflects a policy error in one specific
area: excessive reliance on the exchange-rate anchor" strategy to fight
inflation, with a resulting overvalued exchange rate and an excessive
current-account deficit. This experience should retire once and for all Nigel
Lawson's thesis in the late 1980s that large current-account deficits are not a
problem if they are not caused by a large fiscal deficit.
  Rationally, spillover to the other big emerging markets from the Mexican
crisis should be limited, because few have the explosive combination of low
reserves, sizeable short-term government debt held by foreigners, a large
current-account deficit and a fixed exchange rate (not even Argentina, with its
legally fixed exchange rate and currency board). However, the markets are not
always rational: note the widespread runs on emerging markets and even such
industrial countries as Canada and Sweden in the wake of the peso crisis.
  In the face of systemic threat, the United States Treasury and Federal
Reserve mounted a $50 billion international support plan for Mexico after
Congress delayed passing a purely American loan-guarantee proposal. In addition
to insuring against systemic spillover, the unprecedented action sought to deal
with the large threat rom Mexico itself to American exports and illegal
immigration, and to help ward off a possible revulsion in emerging countries
from the "Washington consensus" model of economic reform with which Mexico was
closely identified.
  Thus, my working hypothesis is that the latest Mexican episode is a moderate
aftershock that does not fundamentally alter the lessons of the 1980s debt
earthquake. So what are those lessons, and what do they mean for future policy?
  By 1993, the 1980s crisis was over, at least in its original, mainly Latin
American, incarnation. For the 17 highly indebted countries identified in the
1986-88 Baker plan as worst hit, the ratio of net external debt to exports of
goods and services had fallen from a peak of 384% in 1986 to only 225%, close
to the conventional 200% threshold for creditworthiness. For the same group,
net debt relative to GNP had fallen from 67% to 42% and net interest relative
to exports from 25% to 11%.
  There was a renaissance in net capital flows to Latin America, which had
fallen from $41 billion in 1981 to an average of $10 billion a year in 1992-93.
Economic growth and, for most countries, price stability had also returned by
the early 1990s, after the macroeconomic retrogression of the "lost decade".
Brady-plan debt-reduction agreements were in place or tentatively agreed upon
for most of the big debtors that sought relief.

Are they illiquid -- or bust?
My new book re-runs the 1983-84 projection models I set up in an earlier book,
with the benefit of hindsight to see what went right and what went wrong. Those
models, and several similar academic, bank and official projections, reckoned
that the main debtors could export their way out the debt crisis, perhaps as
soon as 1987 or so.
  Accordingly, the diagnosis that drove policy in 1983-84 was that the debt
crisis was one of illiquidity rather than fundamental insolvency. This
dichotomy was first recommended to central bankers by Walter Bagehot in the
1870s as the test for when to lend more and when to declare bankruptcy. The
illiquidity diagnosis gave rise to the strategy of "concerted" (or
"involuntary") lending by large banks to roll over principal and finance part
of the interest coming due: a strategy that persisted until the Brady plan of
1989.
  What went right with the model projections was that the international economy
(and export markets) did recover from the 1982 recession, interest rates did
subside, the dollar did retreat from its overvalued levels of 1984, and,
although later than expected, commodity prices did revive.
  Several things went wrong. The price of oil collapsed in 1986, devastating
oil-exporting debtors such as Mexico and Venezuela without much helping oil
importers such as Brazil and Chile. World inflation fell sharply below the
projected 6%, so the nominal dollar export base rose more slowly (and commodity
prices recovered later) than expected, while real interest rates rose. The
combination of capital flight (over $100 billion for 19 big debtors in 1984-87)
and exchange-rate changes meant that dollar debt rose by far more than
cumulative current-account deficits.
  The first rescue packages, organised by among others the Federal Reserve's
Paul Volcker and the International Monetary Fund's Jacques de Larosi�re, had
orchestrated lending by banks and official agencies in return for IMF-guided
adjustment policies in debtor countries. Then, in 1985, the American treasury
secretary, James Baker, launched a three-year plan for the banks to lend $20
billion to key debtors in return for structural reforms.
  It is wrong, but conventional, to argue that the Baker plan failed because
banks did not come up with the money. Actually, they provided two-thirds of the
target, although their balance sheets did not show it because they were
simultaneously engaging in other market-related solutions to the problem (debt
equity conversions and discounted buy-backs) that reduced their exposure. The
banks might have provided more if key countries had adhered to IMF programmes.
  By 1987 Brazil's moratorium on its debt payments set off a spate of bank
loan-loss provisioning (led by Citibank). Soon after, secondary market prices
on debt plunged. That meant the emergence of a potential profitable gap between
face and market value that could be shared between debtors and the more nervous
creditors in the form of a debt-reduction workout, when and if debtor policies
improved to the point that such a solution was credible.
  By early 1988, the American Treasury's David Mulford believed the Baker plan
was flagging and suggested to his colleagues that it was time to shift to a
debt-reduction strategy. However, within the Reagan administration the fear
predominated that in an election year such a shift would incite domestic
demands for similar relief for farmers, school boards and municipalities. Yet
many in academia and Congress (where there was anguish about the fall in
American exports to debtor countries) were increasingly calling for some form
of debt-forgiveness, usually involving the buying up of debt at the secondary
market price by an international agency that would then forgive some portion of
the claim.
  Intellectual interest had shifted from "illiquidity" to "debt overhang".
There was increasing emphasis on the idea that there could be an "internal
transfer problem" of insufficient government mobilisation of resources from the
domestic private sector to service external debt, beyond the "external transfer
problem" of inadequate exports that had been the original focus.
  An American economist, Jeffrey Sachs, and others argued that both the banks
and the debtors could be made better off by partial forgiveness because
excessive debt was depressing domestic investment. Another economist, Paul
Krugman, produced his "debt Laffer curve", showing the market's expected value
of debt on the vertical axis rising to a peak and then falling again as the
nominal value of debt continued to increase along the horizontal axis. Mr
Krugman himself was agnostic about whether countries were in the zone beyond
the turning-point, where reducing debt would increase the chances of repayment
enough to ensure that creditors would actually find it was in their interests
to forgive some of it. Later econometric work showed that few countries were in
fact in that state, just as the bulk of subsequent work has been to reject
rather than support the "overhang" thesis about depressed investment.
  Nevertheless it was Wall Street's Nicholas Brady who began to deal in earnest
with the perceived overhang. In autumn 1988, after he replaced James Baker at
the Treasury, the Reagan administration took a hard look at its policy. It
reckoned that banks were not doing their share and that the burden was being
shifted to the public sector. Growing interest arrears by Argentina, Brazil and
other countries, and low secondary market prices, further contributed to a new
diagnosis that there would have to be a shift in the strategy towards debt
reduction. By the end of 1988, President-elect George Bush had accepted the new
approach.
  The key to the Brady plan, announced in March 1989, was the introduction of
official collateral from the IMF, the World Bank, the Inter-American
Development Bank and the Japanese government, in exchange for a reduction in
bank claims. The collateral swivelled the "debt Laffer curve" upwards, because
banks could keep the same expected value of repayment by accepting greater
security in exchange for lower claims.
  This exchange was the core of "voluntary debt reduction", advocated by my
colleague John Williamson and myself (though we expected more banks with a
longer-term view to opt instead to lend further new money). The main variants
on offer were 30-year conversion bonds that either kept full face value and
paid reduced ionterest rates (par bonds), or reduced face value by about a
third but kept LIBOR-based interest rates (discount bonds). The bonds had
collateral on principal and, typically, 18 months' interest. Most packages also
offered the alternative of new money, and by 1993-94 the few banks choosing
that option looked prescient.
  By 1994 there had been 18 Brady deals covering $191 billion in eligible debt,
forgiving $61 billion in debt equivalent and requiring less than $25 billion in
official agency enhancements. The results were favourable -- disproportionately
so. Thus, Mexico's $15 billion in forgiveness meant an interest saving of only
about $1.5 billion a year, less than �% of GDP and equivalent to about a 4%
rise in export prices. Yet the shift from debt stand-off to normalcy permitted
a sharp drop  in the risk premium carried by domestic interest rates, which
fell from 55% before the deal to 30% after it. Real investment grew rapidly in
response, after nearly a decade of stagnation. There were similar strong
effects in other Brady countries.
  Restoration of confidence was a central part of the Brady plan. Indeed, it
was the resumption of private capital inflows and the repatriation of capital
resulting from this confidence that made the plan fully financed in terms of
balance-of-payments flows, a point Mr Brady had stressed despite the scepticism
of economists in the international agencies.

Two and a half out of three
All in all, the operation was a success, even though it took longer than
expected. From the start, it had three goals: avoidance of an international
financial crisis caused by the collapse of large banks; restoration of the
debtor countries' access to voluntary capital markets; and achievement of
reasonable growth in debtor countries. It succeeded impressively on the first
and second counts. The ratio of claims on the 17 highly indebted countries had
stood at 194% of the capital of the nine largest banks in the United States in
1982, but was down to 51% a decade later. Whereas Latin American countries had
been locked out of the international capital markets for 40 years after the
defaults of the 1930s, this time there was massive re-entry to the capital
markets within a single decade.
  The market-oriented, quasi-voluntary nature of the Brady plan was almost
certainly one reason why countries regained market access. The plan was
market-oriented in reducing claims by amounts commensurate with the existing
secondary market prices on the one hand and the extent of collateralisation on
the other, and quasi-voluntary in permitting the alternative of new money.
Deeper, forced bank forgiveness, and the likely legal contests this would
cause, could have cast a pal on the bond and portfolio capital flows that
subsequently replaced syndicated bank lending.
  The weakest performance came on item three -- growth in the debtor countries.
Yet, even here the plan was successful, to the extent that it contributed to a
historic shift in the conventional model of a third-world development from
inward-oriented, state-interventionist, populist strategies towards trade
liberalisation, privatisation, deregulation and fiscal reform. It is doubtful
whether any alternative debt strategy could have achieved faster sustainable
growth.
  The strategy was contingent, in that at each stage it evolved to deal with
the changing situation. In retrospect, even the original solvency-based aim was
not too far off the mark, as the eventual Brady forgiveness required amounted
to only about a sixth of external debt for the countries involved and only 6%
for middle-income developing countries as a whole. In contrast, domestic
bankruptcies usually forgive 70% or more of the debt involved.
  Arguably, the strategy might have avoided the shift towards forgiveness if
the price of oil had not collapsed in 1986. For Mexico, the losses on oil were
about five times as large as the savings on debt service forgiven. Indeed,
those countries that had good policies and were reasonably lucky with external
economic factors did not need Brady forgiveness (including not only Chile and
Colombia but also countries such as South Korea and Indonesia).
  The common criticism that Brady relief came too late ignores the fact that
the objective conditions necessitating relief were less compelling before the
oil price collapse of 1986, and the banks more vulnerable before the 1987
set-asides. More fundamentally, at earlier stages most countries had not
decided to undertake fundamental policy reform, and a shift to forgiveness
could have delayed reform through the false hope of an easy solution from the
outside.

The seven commandments
Several important lessons emerge from a review of the debt crisis. First,
perhaps, solvency begins at home. The chief determinant of each country's
ability to wheather the global crisis set off by higher international interest
rates and world recession in the early 1980s was the quality of domestic
economic policy and the adoption of reforms.
  Second, fiscal policy matters most. Mexico carried out the equivalent of five
Gramm-Rudman reforms as it shifted from a primary (non-interest) deficit of 8%
of GDP in 1981 to a surplus of 8% by 1988-90; Argentina achieved a dramatic
fiscal reform after the trauma of hyperinflation made tax reform and the
privatisation of chronic-deficit statefirms politically possible.
  Third, an export-oriented model is much more resilient to debt risks than a
closed economy, with its weaker export base. Fourth, governments should not
socialise private debt (as, in effect, Argentina did in the early 1980s),
because to do so turns the external transfer problem into an internal one.
Fifth, it is essential to follow exchange-rate and interest-rate policies  that
avoid capital flight. Sixth, xternal deficits should be kept moderate even if
they are driven by the private sector (a lesson painfully evident now in
Mexico).
  Last but not least, politics and psychology are as important as economics.
J.P. Morgan was correct in saying that willingness to pay is more important
than ability to pay, and the disproportionately large favourable effects of
Brady relief suggest that a shift occurred from a willingness constraint to an
ability constraint once debtor governments had declared the arrangements
equitable.
  In the 1990s, the capital markets shifted away from bank lending to debtor
countries and towards direct investment and portfolio capital flows.
Statistical tests find that bond-market spreads in 1991-93 were rationally
related to such variables as export growth, per capita income and inflation;
and that the markets were exacting a penalty of 85 basis points for Brady
forgiveness. Another finding is that a major part of the capital inflow in the
early 1990s probably involved repatriation of flight capital. One corollary is
that temporary large current-account deficits did not incur a burdensome
obligation, but another is that in future these deficits should be trimmed, or
else alternative finance must be found.
  Transformation of the capital market has several implications. Volatility of
portfolio capital flows is one. Excess inflows were a problem for many
countries in the early 1990s, whereas the Mexican case has now shown the risk
of rapid reductions in these inflows, and they have moderated more generally
with the increase in American interest rates in 1994.
  There may also be a misplaced faith in bond seniority because bonds were too
small to require inclusion in the Brady cuts, whereas they now constitute a
majority rather than minority share of total debt (if Brady bonds are included)
and so on economic grounds could not expect privileged status in future crises.
An important implication of the changes is that involuntary or concerted
lending is likely to be hard or impossible in future because a huge number of
investors have replaced a limited number of large banks as the prime creditors.
However, this same transformation reduces the potential threat of
developing-country debt to industrial-country banking systems.
  To strengthen the system, I propose the development of an International
Bond-holders Insurance Corporation, probably housed in the World Bank complex.
It would have a mixture of private and public capital, and would vary its
coinsurance rate depending on countries' economic policy, thereby sending
appropriate signals to the private market while potentially widening the class
of investors.
  The Mexico package highlights another lesson from the debt crisis. As Bagehot
said, illiquidity is different from insolvency, and central bankers are paid to
tell the difference. When investors learned that Mexico's reserves had fallen
from $30 billion before Luis Donaldo Colosio's assassination last March to $6
billion in December, when the Chiapas rebellion flared up again, they compared
the remaining with the $17 billion in dollar-indexed tesobonos and $20 billion
in Mexican bank dollar obligations and concluded that the country had an
illiquidity problem and that they had better head for the exit. But Mexico's
economic reforms, and its improved overall debt position, make the country
solvent and thus a prime candidate for official support.
  Ironically, the illiquidity-insolvency dichotomy was in disfavour by the late
1980s, as some economists argued that if a country was known to be solvent it
could always borrow, or that the borrower could always mobilise funds by
offering a high enough interest rate. They ignored the theory of credit
rationing, proposed in 1981 by two American economists, Joseph Stiglitz and
Andrew Weiss. This says that private lenders will avoid a borrower offering
exorbitant interest rates because these rates by themselves are evidence that
the borrower is desperate. The international package for Mexico has now
refurbished the illiquidity-insolvency dichotomy. In effect, it is a $50
billion wager that this time the problem really is illiquidity.



Message, 22 April, T

Message in a bottle
Scientists are under growing pressure to use fewer animals in their 
laboratories. Increasingly, there are alternatives.

At present Britain is in the middle of an enormous fuss over the cruelty of 
exporting calves in crates. An issue that raises similar moral questions, and 
which divides the antagonists just as bitterly, is vivisection. That debate has 
a longer history. More than a century and a half ago Fran�ois Magendie, a 
visiting French physiologist, ruffled London's sensibilities by carrying out 
experiments on animals in public. There were cries of protest, comments in the 
press, questions in Parliament and dark hints of cruelty on the Continent. 
Then, as now, there was little meeting of minds.
  The quarrel between those who would endow animals with "rights" and those 
who say that animals must be used if science is to move forward remains as 
heated as ever. Many of the familiar arguments will be aired again during a 
week of protest and discussion in the United States and Europe that starts on 
April 24th -- which an alliance of animal-welfare groups is calling 
International Laboratory Animal Day.
  Yet the gap between those who care most about the welfare of animals and 
those who care most about scientific advance may soon be narrowed: not through 
agreement on the undrlying moral question, but because fewer experiments on 
animals are going to be needed. Scientists, businessmen and policy-makers have 
been developing alternatives to the use of animals for biomedical research and 
toxicology testing. They have made impressive progress.

The three Rs
Their approach can be summed up in three words: reduce, refine and replace. 
That is, use fewer animals in tests and experiments; make informative and more 
humane; and develop procedures in which animals no longer have to be used at 
all.
  Worldwide figures for the animals used each year in research and testing are 
a matter of guesswork, since few countries require researchers to keep any 
records. National statistics for the countries that do, patchy though they 
are, point to a substantial decline in animal use over the past decade. 
British research "employed" 2.8m animals in 1993, 100,000 fewer than the 
preceding year and 20% fewer than in 1987 ( see chart on next page ). Holland 
and Canada have reported similar reductions in their use of mammals.
  It is harder to be sure about what is happening in the United States because 
the commonest experimental animals, mice and rats, are not under federal 
protection and are therefore not counted. Researchers estimate that, as in 
Britain, such rodents are used in 85% of current experiments. Andrew Rowan, 
director of the Tufts Centre for Animals and Public Policy in Massachusetts, 
reckons that the total number of animals used in American research has dropped 
by a half -- to no more than 30m a year -- since 1968. To an animal-welfare 
extremist, 30m is still a holocaust. Nonetheless, the trend is clear. At the 
end of the 1970s a single pharmaceutical company such as Hoffman LaRoche was 
consuming 1m animals a year in production and testing. Now they and other big 
corporate users such as Procter & Gamble, a consumer-goods company, and Avon, 
a cosmetics firm, have typically reduced their requirements by 90%.
  One reason for the change is that governments have insisted on it. Western 
Europe leads the way in legislation: since 1986 a European Union directive, 
enforced by national committees, has instructed researchers to choose 
non-animal methods if they are "scientifically satisfactory [ and ] reasonably 
and practically available." The European Commission will ban almost all 
cosmetics testing on animals from 1998 and aims by 2000 to cut the number of 
animals used in research to half its present level.
  In America, years of permessive legislation have been slowly amended to 
encourage reduction, refinement and replacement. The National Institutes of 
Health recently produced a plan for reducing the use of animals in its 
research, although it has been reluctant to speak of "alternatives". Louis 
Sibal, director of the NIH Office of Laboratory Animal Research, says that 
many scientists prefer the term "adjuncts", emphasising the role of new 
techniques as complements to rather than substitutes for animal-based research.
  Public qualms over the use of animals seem unlikely to go away. Even in 
Switzerland, that bastion of pharmaceutical research, a 1992 referendum came 
within 7% of curtailing animal experiments. And with mounting public concern 
has come a flow of money to fund the development of alternatives. Barbara 
Orlans of the Kennedy Institute of Ethics in Washington, DC, says there are 
now tens of millions of dollars of financial support from more than 60 sources 
worldwide, ranging from companies and governments to humane societies. 
  It is one thing to call for alternatives to animal testing, and to pour 
money into the search. Can science deliver?

Spare the rod
With the advent of molecular biology in the 1970s came the rise of 
micro-organisms as experimental tools. It has become cheaper and faster to see 
whether a substance is toxic by testing it on fungi and bacteria than by 
testing it on living animals.
  Photobacterium phosphoreum, a luminescent bacterium, is particularly 
effective at detecting chemical irritants. During its normal metabolism, this 
bug converts part of its cellular energy into light. Certain classes of 
chemicals can interfere with this process and dim the glow, providing a way to 
measure their toxicity. A test based on it, first developed by scientists at 
Microbics Corporation in Carlsbad, California, is now used by companies such 
as Boots, a British drug firm, in their battery of tests.
  Microbics has also introduced a dark version of this bacterium which has its 
lights turned on by mutation-inducing substances. Preliminary studies suggest 
that this system may prove as useful as the venerable Ames test. And yet it 
took nearly ten years of assessment to convince America's Enviromental 
Protection Agency that the Ames test was sensitive and specific enough to 
predict huamn toxicity. The Ames test uses salmonella bacteria which respond 
in clearly recognisable ways to changes in their DNA. When the bacteria are 
exposed to a chemical which is being tested their response reveals any 
tendency in the chemical to produce mutations. Even today, it is one of the 
few microbial toxicity tests that international regulators have approved.
  This sort of delay prompts people like Michael Balls, head of the new 
European Centre for the Validation of Alternative Methods in Ispra, Italy, to 
claim that the present barrier to replacing animals is as much bureaucratic as 
it is scientific.
  But microbes can play the part of mammals in only so many settings; theirs 
is really a supporting role to the new star of test-tube methods, which is 
cell culture. Science has recently made rapid advances in understanding what 
it takes to make cells grow. This means that almost any cell can be taken from 
the body of an animal or a human and kept alive for days or even years. These 
increasingly sophisticated techniques are the mainstay of modern cell biology 
because they allow scientists to study basic cellular processes in settings 
that are more accessible and flexible than whole animals.
  They can also lead to substantial cuts in the numbers of animals used to 
produce biological substances. Some vaccine reagents, which once demanded 
fresh tissue from animals, can now be grown in cell culture. Three decades 
ago, for example, Holland used 3,500 monkeys a year to produce its polio 
vaccine antibodies; now only 20 are needed.
  Monoclonal antibodies, guided missiles of the immune system that are widely 
used in medical diagnosis and treatment, have usually been harvested from 
special tumours implanted in the abdoments of mice. Now new technologies, such 
as the hollow-fibre bioreactor, are allowing these antibodies to be produced 
efficiently in vitro and not in the bellies of beasts. Britain recently 
reported a one-year drop of 30% in the number of animals used to generate 
monoclonal antibodies. The use of such alternatives is compulsory in 
Switzerland.
  Cell-culture methods are also having an impact on the use of live animals in 
drug testing and safety evaluation, a legal requirement in almost all 
countries. Before 1985, the American National Cancer Institute used to try out 
potential anti-cancer drugs on cancerous mice; by switching to human tumour 
cells, they have reduced the number of animals in these tests from more than 
4m to about 1m.
  A large number of cell-culture systems are now being developed to test 
substances quickly and easily for a tendency to cause cancer or birth defects. 
Some of the most innovative ideas are emerging from laboratories at Xenometrix 
in Boulder, Colorado. There, scientists have created lines of human cells 
which contain different "stress" genes whose activation by toxic chemicals is 
easily detected. These tests can tell not only whether, but also how, a given 
chemical harms a particular type of cell.
  There is, however, a limit to what can be learnt from studying isolated 
cells. Most avdocates of alternatives to animal testing accept that test-tube 
methods cannot replicate the intricacy of mammalian organs. 
  Attepts to reconstruct complex tissues -- such as the liver -- in the test 
tube have made slow progress. But there have been successes with other 
complex organs. Human skin, with its complex three-dimensional structure, is a 
case in point. Skin is no single layer but a stratification of specialised 
cells, proteins and facts: more of a biological millefeuille than a pancake. 
Maria Ponec's group at the University of Leiden in Holland has created a 
living test-tube equivalent of the outer two layers. With this they can study 
the skin's function as a biological barrier.
  One of the most commercially successful of these artificial constructs is 
Skin, developed at Advanced Tissues Sciences in La Jolla, California. 
Researchers there use cells from the off-cuts of routine circumcisions to 
construct sheets of fully differentiated three-layered human skin on nylon 
scaffolds. By observing microscopic changes in tissue structure and measuring 
the rate at which cells release inflammatory substances, researchers can see 
whether compounds irritate the skin, as well as studying wound healing and 
psoriasis.
  Skin has proved so effective at detecting corrosive chemicals that the 
American and Canadian transport departments last year approved it as a 
substitute for the traditional test used to classify such substances applying 
them to the backs of shaved rabbits and watching them burn through.

A beam in your eye
Such rabbits had it easy compared with those on which the notorious Draize 
test for eye irritation is performed. This test consists of squirting 
chemicals into the eyes of a number of rabbits and seeing how the tissue 
changes. The cosmetics industry has spent the past decade looking hard for 
alternatives. Some consist of refining the present technique so that fewer 
rabbits or less of the chemical can be used without compromising the test's 
results. Others use cell cultures or the membranes of fertilised hens' eggs to 
measure a compound's potential irritance.
  As yet, however, not one of more than 60 alternative techniques has been 
officially accepted as a substitute for the Draize test, although 
international validation studies ( of a sort, ironically, that the Draize test 
itself never had to go through ) are in full swing. One of the simplest 
alternatives -- EYTEX from In Vitro International in Irvine, California -- is 
also among the most reliable. Many of the substances which cause this clear 
mixture of plant proteins and sugars to go cloudy also turn out to be 
irritants in the Draize test, a correlation which bodes well for its prospects 
as a replacement.
  A more recent innovation is a technological spinoff from cataract research 
at the University of Waterloo in Canada. Jacob Sivak and his colleagues have 
devised a laser-scanning system to detect alternations in the optical 
properties of cattle eyes fresh from abbatoirs. 
  The lenses are placed in a solution which mimics their natural fluids and 
keeps them alive. Then a laser light is shone through them. Toxic insults to a 
lens will distort it, altering the path of the light in a way that can be 
recorded with a video camera. This provides a reproducible and objective 
measure of how much damage can occur to this essential part of the eye, which 
the Draize test overlooks. Lenses can last for weeks in culture, so this 
system can also be used to gauge how long and how well the tissue will recover 
when an irritant is removed, a significant advantage over many other proposed 
alternatives.

Screen test
Some scientists see the future of in vitro alternatives in the glass of a 
computer screen as well as in the bottom of a test tube. This is particularly 
true in toxicology, where mathematical models and computer programs are now 
used to predict the biological activity of compounds.
  Such systems depend on a simple premise, that it is the atomic organisation 
of a molecule that accounts for its toxic properties. "Expert systems" such as 
a piece of software known as DEREK, developed by Chris Earnshaw and Derek 
Sanderson at Schering Agrochemicals, can spot suspect bits of atomic structure 
by comparing a molecule drawn on a computer screen to a database of molecules 
whose characteristics are known. Any matches with features associated with 
toxicity are highlighted as hotspots, for special attention in subsequent tests.
  Equations known impressively as quatitative structure activity relationships 
-- QSARS -- are also used to give a precise description of the relationship 
between a compound's characteristics and a given toxic effect. Scientists at 
Unilever's laboratories in Bedford, England, have taken information about 
known eye irritants and created a QSAR which relates their molecular size, 
electronic charge and other properties to their capacity to cause injury in 
the Draize test. With this analysis in hand, it becomes possible to predict 
whether a similar chemical is likely to be an irritant without recourse to a 
rabbit.
  However, computer models of this sort are only as useful as the quality of 
the data with which they have been built. And although the world is brimming 
with databases of structures and toxicities, their quality varies widely. 
Some, like that of the toxicology programme at America's National Cancer 
Institute, are pretty reliable, but others are duds. As a result, there are 
now calls for an international chemical reference databank to consolidate 
dependable information from industry and the public domain, making the most of 
results from earlier live animal experiments so that unnecessary repetition 
can be avoided.
  Computer methods such as these give snapshots of a compound's potential 
toxicity. But newer simulation models aim to provide a dynamic picture of its 
interactions throughout the body. The impact of a drug, for example, does not 
depend only on how it docks with a receptor on a target cell. The ways in 
which it is taken up into the body, passed through the circulation, altered in 
the liver, and finally expelled also matter. Using data from older, 
whole-animal research and more recent test-tube techniques, these processes 
can be represented mathematically and integrated to create a PBBK, or 
physiologically-based bio-kinetic model.
  At present, computerised PBBK models are unable to predict the effects of 
novel substances, but an abitious multi-centre European scheme aims to develop 
them. As part of this programme, Bas Blaauboer at the University of Utrecht is 
putting his PBBK models through their paces with known neurotoxins to see 
whether they can represent their interactions in the body. Robert Combes, 
director of the Fund for the Replacement of Animals in Medical Experiments, a 
charity which runs Britain's leading animal-alternatives centre, is hopeful 
that the time will come when the merest gleam in a pharmacologist's eye can be 
designed, tested, modified and approved all on the basis of a combination of 
these computerised models and in vitro alternatives.
  But it is not just high-tech wizardry that is needed to reduce the number of 
animal experiments. In many cases, a reduction depends on the willingness of 
regulators to accept the alternatives, and to comply with another's rulings. 
Hence the current flurry of validation studies to prove that the new options 
are adequate substitutes for established whole-animal methods. 
  In basic research, the emphasis is slightly different. Dr Orlans believes 
that the most immediate gains will be made in refinement, not replacement, of 
experiments, making them as humane and informative as possible using the 
minimum number of animals. Biomedical science will continue to ask questions 
that, for the moment at least, only live-animal experiments can answer. The 
information they yield, however, can lead to new non-animal methods. With 
their professional societies, electronic bulletin boards, peer-reviewed 
journals, and a second "world congress" next year, advocates of the three Rs 
are growing in scientific respectability, as well as in numbers.



Patient, 4 February, S

Patient, heal thyself
The cult of the doctor is under attack. Patients want a bigger say in choosing 
the treatment they get. Amen, say the firms and governments that pay the bills.

For centuries the sick have placed their trust in the wisdom of doctors, 
presuming themselves too ignorant to deal with such mysteries. They could 
not understand what was wrong with them, they felt, let alone decide what 
should be done to make them healthy again. The doctor's power over the patient 
can be absolute: in some countries, such as Japan, a patient may not even be 
told that he has cancer, or is about to die, for fear that a fuss may be 
caused.
  Behind doctors stand governments willing to weigh in with huge slabs of 
regulation designed, ostensibly, to protect the patients. Regulators decide 
what new drugs can be sold, and how -- which often means by doctor's 
prescription only. Manufacturers are persuaded not to advertise prescription 
drugs directly to consumers, though they can lavish incentives on doctors. 
Some of this does protect patients. But much of it serves mainly to protect 
the medical profession. This is a state of affairs that outsiders are 
beginning to question, and sometimes to challenge successfully.
  Some of the pressure is generated by changes in the way medicine is paid 
for. In Western Europe, welfare states are running out of money, so patients 
are asked to pay more for medicines and treatment out of their own pockets. 
They rightly want more say over what they get in return. In America, 
governments, insurers and employers are struggling to contain huge doctors' 
bills by returning to "managed-care" firms such as health-maintenance 
organisations which offer a complete health-care package for a pre-paid fee. 
These firms want to be sure that doctors are giving measurably good value; 
their patients want to be sure that their individual needs are not being 
overlooked in the corporate pursuit of an acceptable performance "on average".
  Groups seeking changes in the way particular illnesses are treated, or drugs 
are marketed, have found an encouraging model in the work of AIDS activists in 
America in the late 1980s. The AIDS lobbyists achieved far more than any 
similar group had ever done before.
  They mobilised public concern, so ensuring that more money went to 
AIDS-related research. They made Wellcome, a British drug company currently 
subject to a takeover bid from a rival, Glaxo, cut the prices of a pioneering 
drug, AZT. They helped to persuade America's Food and Drug Administration 
(FDA) to release unapproved drugs that were thought to have possible 
applications in treating AIDS. They obtained the release of information about 
experimental new medicines, overcoming the FDA's insistence that to do so 
would violate commercial confidentiality. They even managed to impose their 
own view of how AIDS should be defined, despite protests from some authorities.
  AIDS activists were able to achieve so much partly because they included 
many affluent, articulate and influential people, but also because the medical 
establishment was so visibly powerless against the disease. Where they led, 
other groups are now following. One of these was founded by Michael Milken, an 
American investment banker and junk-bond specialist who was imprisoned for 
fraud in 1990. 
  Mr Milken discovered on his release from jail early in 1993 that he was 
suffering from prostate cancer. He also learnt that little treatment was 
available for his condition, one of which is often categorised as "begnin" but 
is not. He responded by setting up an organisation called CaPCURE, which he 
endowed with some $3m-5m of his own money and to which he recruited fellow 
sufferers including Frank Zappa, a musician (who died in December 1993) and 
Senators Robert Dole and Ted Stevens.
  CaPCURE vigorously lobbies politicians, scientists and doctors. The American 
government declared October 1994 to be "prostrate cancer awareness month", 
and several drug companies have been jolted into undertaking new research on 
possible treatments.

An apple a day
The demand for more consumer control over health care is bringing new firms, 
new products and new ideas into the market. The use of herbal medicines, for 
example, sold without prescription, has risen sharply in the West: in Britain, 
sales are to up 70% since 1988. Books, magazines and CD-ROMS are 
proliferating, among them a new Health Pages that advises American consumers 
where to shop for the best, and best-value, managed care and hospitals.
  People with home computers can buy number of programs designed to make it 
possible for them to diagnose their illness (though most such packages are 
still pretty crude). Easy-to-use medical-testing kits, some based on new 
technologies, are bringing laboratory standards into the home. 
Cholesterol-testing kits, blood-pressure monitors and pregnancy testers are 
already commonplace. Within the next year, AIDS testing may be done easily by 
mail order, with telephone confirmation. Hospital hardware, from intravenous 
drips to special beds for the elderly, is now readily available to the private 
shopper.
  Put these together with the spread of computerised communications (the 
"information superhighway") and it is not so far-fetched to imagine the 
affluent and wired consumer of a decade hence organising most of his medical 
needs from his own home, or wherever else he happens to be, and dialling up a 
doctor or hospital or pharmacy only if the need arises.

Drugs on the market
Although these things do not point to a flat rejection of health care centred 
on the general practitioner and the hospital, they do reflect a growing trend 
to do things yourself where you can -- whether because it is cheaper, or more 
convenient, or because you feel you ought to be able to look after yourself. 
According to the Wilkerson Group, a New York consulting firm, there is a 
corresponding disposition on the part of health-care companies to find ways 
that enable them to by-pass doctors and deal direct with the consumer.
  American drug manufacturers have been spending billions of dollars buying 
so-called "pharmacy benefit managers", firms that specialise in collecting and 
selling information about the procurement, dispensing and use of drugs. As a 
result, even when a drug is prescribed by a doctor, it may be the manufacturer 
rather than the doctor who checks later that the drug is being taken correctly 
and that it is doing its job.
  Drug firms are also looking for legal loopholes that will allow them to use 
press and television advertising, in effect, to promote prescription drugs -- 
for example, by framing an advertisement around a disease, rather than around 
the drug the company offers for dealing with it. Other ingenious ploys are 
being devised: in October Merck, the world's biggest drug firm, offered 
patients who were taking its Proscar drug (for prostate cancer) a refund of 
$275 if they were unhappy with their treatment.
  As individuals are drawn into a more active and intelligent involvement with 
their own health care, with technology to help them, so some of the old 
regulatory barriers that were erected in the name of patients' interests are 
coming to look redundant, even counter-productive. For example, the obligation 
to obtain many types of drug by prescription only, or through a licensed 
pharmacy, gives rise to huge anomalies explicable only in terms of national 
prejudice or successful lobbying.
  In France, even vitamin pills have to be bought from a licensed pharmacist; 
yet in Thailand you can buy sedatives over the counter, and in Hong Kong 
birth-control pills. In countries with generally liberal regimes -- Denmark 
and Spain, for example -- the freedom to choose this heart pill or that 
ointment without a doctor's intervention does not seem to do obvious damage 
to the patient or to the national well-being.
  Nor it is clear even to regulators that, where prescription rules apply, 
doctors should be allowed a monopoly over prescribing. Janet Woodcock, who is 
in charge of regulating prescription drugs of the FDA, acknowledges that 
doctors are not always the best-equipped people to make decisions about 
prescribing. So much new medical information is constantly being created that 
no doctor can keep up with more than a fraction of it.
  According to Thomas Inui, a professor of medicine at Harvard Medical School, 
a newly graduated doctor is probably on top of his discipline for ten years, 
after which most of the things he has learnt at medical school start to become 
redundant. Doctors are under no obligation to continue their education after 
they qualify. So why not extend prescribing authority to other people likely 
to be at least as well-equipped to make informed medical decisions -- such as 
health-care administrators, nurses and technicians? (Already, many doctors who 
prescribe are doing little more than endorsing a choice inspired by an accountant.)

A cure for bureaucracy
Any liberalisation of medicine will have to shake up regulators at least as 
much as it does doctors. Some will say: not before time. 
  The General Medical Council, which regulates Britain's doctors, is also the 
doctors' trade union, and rarely neglects its constituency. In America, the 
FDA, which regulates the drug industry, is increasingly criticised for doing 
its job in a way that no longer reflects the needs of much of America's modern 
health-care industry. In particular, it has resisted being drawn into the 
business of cost-appraisal, even though most health care in America is now 
delivered with cost in mind.
  The FDA and other regulatory agencies also usually assess the safety and 
efficacy of drugs by a system of clinical trials that is starting to show more 
and more shortcomings. It is now being recognised that, because clinical 
trials measure only a small, select group of patients, they may have little to 
say about what happens when a medicine is used across a large, diverse 
population.
  The FDA says that it may consider assessing cost-effactiveness, and may 
overhaul its clinical trial system. Learning from the experience of trying to 
regulate AIDS-treatment drugs, the agency also says it plans to be more 
sensitive to consumers' needs. That is a break from its past occupation with 
the needs and views of doctors. It accepts that consumers want more and better 
medical information, and in the light of this it is looking at how to relabel 
over-the-counter pills bottles so as to make them more informative to 
purchasers (as it has already done for food labelling).
  For serious would-be liberalisers of the health-care industry, the question 
of principle is not whether regulators are doing a useful job; it is whether 
market forces might do that job better. Bob Easton, of the Wilkerson Group, 
believes that the American market is already demonstrating its capacity to 
assume some of the FDA's responsibilities.
  As the big buying-power in American health care shifts to managed-care 
firms, so manufacturers are obliged to conduct more exacting clinical trials 
of their own to persuade managed-care firms to prescribe their products. And, 
with the managed-care firms basing their purchasing on objective data, the 
drug-firms' salesmen can no longer rely on dropping in to see a doctor and, 
behind closed doors, offering him anything from a fountain pen to a trip on 
the Orient Express as an incentive to prescribe a certain pill. The FDA, 
although nominally responsible for overseeing medical promotion, has been 
helpless to intervene in such practices because it could almost never discover 
the substance of a doctor's private discussions. Market forces are thus 
enforcing more transparency, and with it a greater efficiency, than regulation 
ever could.

The doctor won't see you now
To hold that medicine is an industry, not a priesthood, does not imply a 
rejection of all regulation. People's lives often depend on the medical 
industry's products and services. But they also depend on the quality of the 
car industry's products -- and the fact that the car industry is far more 
lightly regulated, on the basis of general standards of safety and 
performance, does not seem to upset people who buy cars.
  To that, doctors -- and many patients -- will reply that medicine is 
different from cars. For one thing, people do not commonly die because they 
bought the wrong sort of car. For another, the more acutely a person needs 
medical treatment, the less likely he is to be in a position to make lucid 
judgments; that is why doctors argue for ultimate authority over the 
individual. The Hippocratic oath, to which all doctors are signatories, 
insists that they "look after" their patients and do the best for them -- not 
that they do whatever the patient thinks is best. Doctors can also argue, 
plausibly, that for a patient to trust blindly in the treatment he is given 
can itself assist his healing.
  It must also be recognised that, when patients do insist on dealing with 
medical problems their own way, and knowingly or unknowingly damage themselves 
or their family, there is a strong public reaction that "something must be 
done". Scare stories abound: take the case of Ron and Barbara Whitehead's two 
children, who, with a nephew, were rescued from a serious motor accident in 
January 1994.
  All three children needed emergency surgery; but the Whitehead parents, 
being Jehovah's Witnesses, refused to allow the blood transfusions that 
doctors urged for them. "The children might die," acknowledged the Whiteheads, 
"but their souls would be safed." In the event, the children did survive; but 
there is a strong argument in cases like this for having more rather than 
less, freedom for physicians to dictate the treatment of those who, like the 
Whitehead children, cannot take decisions for themselves. The same would apply 
in respect of the mentally ill, or, indeed, anyone who positively wished to 
give responsibility for his health to someone else.
  More usually, however, the problem is that people who disagree with their 
doctors tend to want more, not less, treatment than that which the doctor 
prescribes. Patients think that more medicine is better medicine largely 
because doctors have led them to believe so. In the doctor's traditional 
fee-for-service system, recommending useless tests and writing never-ending 
prescriptions help to bump up earnings. Now, as large firms and other 
health-care outfits with strong commercial objectives take over more and more 
of the running of health care in America and Europe, there is a natural 
concern that the search for profit will restrict the availability of medical 
care. It may well do so. The organising principle of managed care, and most 
insurance plans, is that of setting limits to the types of treatment made 
available.
  The patient's appetite for treatment can take surprising forms. Some recent 
studies suggest that most women with breast cancer will, if given the choice, 
opt to have an entire breast removed, rather than have a less disfiguring 
operation that removes only a small part of one. The two operations are 
equally effective. Again in relation to breast cancer, American courts heard a 
number of cases in 1994 in which patients demanded that their health insurers 
pay for them to have costly autologous bone-marrow transplants. Most of the 
courts found in favour of the patients. Yet there is little hard evidence that 
such transplants help.
  Clearly, there is scope for infinite legal wrangling over where to draw the 
line when a patient wants more care than a doctor recommends. Tom McLaughlin 
and Stephen Soumerai, two doctors at Harvard Medical School who are studying 
doctor-patient relationships, argue that patients should not be denied 
treatment that they want unless it is unduly expensive. If it is too costly, 
they should be required to pay for it themselves; and if it does not work, 
they should not be able to sue the doctor who supplied it on specific demand.
  Dr Inui suggests that "with the growing body of good medical information it 
would probably be more effective to look at medicine as a set of processes 
doctors undertake together with patients where neither has a controlling 
influence." There are reasons to think that a more co-operative approach to 
treatment can yield net gains to patient and doctor alike.
  For example, Kate Loring of the Stanford University Medical Centre, in Palo 
Alto, has studied people being treated for acute arthritis. She grouped some 
together so that they could learn about their disease and manage its 
treatment, drawing on doctors and nurses as they wished. Over a period of four 
years her group of patients visited their doctors 40% less frequently than 
patients who were treated in the traditional way.
  Perhaps the two things march together. A more intelligent, self-guided 
approach on the part of the individual towards health care may be reinforced 
by the resistance of employers, governments and insurance companies to signing 
blank cheques for future treatment. If the individual is to find the 
availability of free or subsided health care ever more restricted, then he 
should be entitled, indeed encouraged, to make sure that he is getting what 
amounts -- in his own estimation -- to the best treatment available: neither 
too much, nor too little. If he does it without going to a doctor, and if he 
thereby suffers no ill effect, it will be hard to say that something wrong has 
happened.



Man, 13 May, P

The man who overstayed
This week Fran�ois Mitterrand ends his two terms as president of France. He 
has helped clarify French interests, in Europe above all. He has also shown 
that 14 years is too long for one man to rule.

If history seems likely to rank Fran�ois Mitterrand as something less than a 
great president of France, it will assuredly remember him as a most 
extraordinary one. Promising one thing and delivering another was not so much 
a weakness of his presidency as its organising principle. Elected to bring 
socialism to France, he presided instead over something closer to the death of 
French socialism. As he prepares to step down from two consecutive seven-year 
terms in power he leaves behind him a country more nearly reconciled to market 
capiatlism than ever it was under Gaullism; yet he has never ceased to talk as 
if he believed himself to incarnate the orthodox values of the left.
  He leaves behind him, too, a political landscape littered with the wreckage 
of the parties, Socialist and Communist, whose votes carried him into office. 
They thought Mr Mitterrand was their ticket to power, and found to their cost 
that the reverse was the case. In order to prove that he could manage without 
the Communist Party, Mr Mitterrand divided and humiliated it so deeply during 
his first term that the Communist candidate's share of the vote fell from 
15.4% in the first round of the 1981 presidential election to 6.7% in 1988. 
Less forgivably, Mr Mitterrand picked his own Socialist Party to pieces during 
his second term, seemingly out of spleen at his inability to order his 
succession there as reliably as he wished. Lionel Jospin's recent vigorous 
presidential campaign has been applauded as a revival for the Socialists, but 
may yet prove to have been more a death-throe.
  The years of Mr Mitterrand's presidency have, on the other hand, been a 
golden age for the immigrant-hating National Front, giving the extreme right 
its best run since Vichy. Some argue, plausibly, that Mr Mitterrand valued 
privately the National Front's rise as a means of dividing the mainstream 
right. What is certainly true is that much of the National Front's 
near-respectability dates back to the interlude of proportional representation 
that Mr Mitterrand employed to protect the left from too crushing a defeat in 
the 1986 parliamentary elections, and which allowed the National Front's 
representation to soar briefly from 1 to 35 seats. Jean-Marie Le Pen, the 
party's leader, won 15% of the vote in the first round of this year's 
presidential election -- and might have won 20% had it not been for the 
intervention of an ephemeral Catholic monarchist, Philippe de Villiers. Mr Le 
Pen is a guileful orator, the most accomplished television performer of any of 
the current party leaders, and, at the age of 66, is young enough to run again.
His will be a long shadow over Jacques Chirac's presidency.
  The rise of Mr Le Pen apart, more conspicuous legacies of Mr Mitterrand's 
presidency include some $6 billion-worth of new public buildings scattered 
around Paris, the treaty of Maastricht, a proliferation of private radio and 
television stations across France, the channel tunnel, the franc fort, the 
abolition of the death penalty, and one of Western Europe's highest 
unemployment rates. The devolution of power from the central government to the 
regions and departments, enacted in 1982, has been less noticed outside 
France, but may prove to have been Mr Mitterrand's most far-reaching piece of 
legislation. In this he has undone the work of Colbert and Napoleon, and 
undone well. For all the cynicism and fragmentation that now characterises 
national politics in France, and perhaps as a corollary of that, democracy at 
a local level has become vigorous and plentiful.
  Mr Mitterrand has also succeded in proving, to his own satisfaction and 
everyone else's, that 14 years is too long for one man to rule France. At 78 
he is tired, and ill with prostate cancer; his second septennat has been 
directionless. But it is unlikely that Mr Chirac, or any other future 
president, will see the wisdom of preventing the repetition of that particular 
error.

Temps perdu
Mr Mitterrand found his way to power in 1981 thanks mainly to his right-wing 
predecessor, Val�ry Giscard d'Estaing, whose economic policies swung from 
laxity to austerity as France suffered the stagflation common to many western 
countries in the 1970s. By the time Mr Giscard d'Estaing ran for re-election 
he had succeded in disappointing just about all his supporters' expectations 
at one stage or another, Mr Mitterrand promised a return to growth and full 
employment, and too few voters were inclined to call his bluff. 
  France had had no full-throated government of the left since that of L�on 
Blum's short-lived Popular Front of 1936. The Socialists and their Comminist 
allies arrived bursting with half a century's frustrations: they shortened the 
working week, lengthened workers' holidays, raised the minimum wage, increased 
welfare benefits, imposed a wealth tax and nationalised most of the banking 
system plus five big industrial groups. Their supposition was that still 
higher public spending, higher incomes for the poor and more government 
control over investment would translate into economic growth and the creation 
of new jobs. They were right about the growth, but wrong about the jobs, and 
they had not foreseen the higher inflation, depreciating franc and 
catastrophic trade deficit that their attempts to stimulate growth in France 
would bring when most of Western Europe was crawling out of recession.
  The moment of crisis came in March 1983 when, despite two devaluations of 
the franc within the European Monetary System, the Bank of France found itself 
down to a fortnight's reserves. One option was to take the franc out of the 
EMS and move to a siege economy -- something France was still reasonably 
well-equipped to do in 1983, but which would have wrecked France's standing in 
the European Community. The other option was to accept that France could not 
spend its way to prosperity, and abandon the Socialist programme. Mr 
Mitterrand's endorsment of this second course marked the watershed of his 
presidency.
  To see the juncture as a crucial one solely in terms of economic policy, 
however, and to see it as a defeat for Mr Mitterrand, would be to miss a 
broader point. It was a reversal from which Mr Mitterrand suffered nothing. 
It was, rather, something of a liberation for him, the moment at which he 
began to take stock of the strength of his own position. He would be president 
for at least another five years whatever happened, and thanks to the 
constitutional magic of the Fifth Republic, which made of its president, ex 
officio, a statesman and not a politician, he could interfere in the 
management of the economy as much or as little he liked. Having "done" the 
economy and found it less tractable than he had imagined, he was content after 
the 1983 crisis to leave the direction of it to ministers and civil servants 
-- even when the governments were right-wing ones that set about reversing 
some of the Socialists' work, notably in privatising nationalised industries.
  Mr Mitterrand did, however, continue throughout his two terms to make 
periodic pronouncements about the economy that were populist and damaging. 
He spoke out against higher taxes and in favour of expensive social welfare 
programmes. The left applauded him, the right lacked the convintion or the 
political courage -- suicidal courage, perhaps -- to argue otherwise. The 
result was a government debt which tripled in real terms during Mr Mitterrand's 
presidency. Nor did such profligacy do anything to help unemployment. France's 
unemployment rate was under 8% when Mr Mitterrand took office, and it is more 
than 12% as he leaves it. The fact that the rate has remained remorselessly 
above OECD average (it is half as high again as the British rate) suggests 
that at least part of it is a monument to the combination of restrictive 
labour laws and high payroll taxes that has become part of the French 
political consensus.

Under western eyes
In foreign policy as in economic policy, Mr Mitterrand was nothing if not 
pragmatic. He bristled instincively, as de Gaulle had done before him, at 
America's habitual presumption that Europe's interests in the world were the 
same as, or capable of being subordinated to, America's own. He saw Britain as 
America's agent in Europe. But he was willing to acknowledge more clearly than 
de Gaulle had ever done the degree to which France and Europe needed America. 
He sometimes indulged rhetorically the Gaullist conceit of France as the swing 
voter between the superpowers and the developing world, but he recognised it 
as a conceit to be discarded as the need arose.
  When Britain and Argentina squared off to fight for the Falkland Islands in 
April 1982, Mr Mitterrand was quick to telephone Margaret Thatcher and assure 
her of France's support in the United Nations Security Council. He saw that 
European solidarity mattered so much to French interests that no amount of 
applause across America could compensate France for a deep schism with Britain.
  The falklands war foreshadowed on a smaller scale the choice Mr Mitterrand 
would make eight years later when faced with the prospect of hostilities in 
the Gulf. He was obliged to decide whether France stood to gain more by 
supporting America's war, or by staying aloof from it and so perhaps winning 
friends elsewhere. Again, he saw that France's standing among the western 
powers mattered a great deal more than its sentimental links with the third 
world. In particular,he saw that a good showing in the Gulf would bolster 
France's claim to retain its permanent seat on the UN Security Council, and 
with it a disproportionate influence over world affairs.
  The French nuclear deterrent was a matter which Mr Mitterrand kept close to 
debate: he deemed it an essential guarantee of France's security, and held 
that its use was a matter for the president of France alone. On NATO he was 
more accomodating. He restored French attendance at some meetings of the 
alliance's military committee and defence planning committee. But he also 
hankered after a pooling of Europe's forces outside American command, through 
an extension of the "Eurocorps" that France formed with Germany or through a 
more muscular Western European Union.
  Where Mr Mitterrand did break, momentously, with Gaullist foreign policy, 
was in the matter of European integration. De Gaulle thought Western Europe 
should stand together the better to resist American and Russian domination, 
but he refused to consider co-operative structures that might compromise his 
broad definition of French sovereignity. Mr Mitterrand was prepared to take a 
much narrower view of what constituted irreducible French sovereignity, and so 
was able to commit France to the process of European integration that became, 
with the abandonment of socialism at home in 1983, the great cause of his 
precidency.
  Mr Mitterrand's support for European union was a product of his 
preoccupation with Germany -- an inevitable one for any French president given 
that Germany is at once France's closest ally and its historical enemy. But on 
Mr Mitterrand the historical baggage seemed to weigh especially heavily.
  He saw himself as the last French president, and the last European leader, 
whose political career stretched back to Hitler's war. He also believed -- or 
so he told Helmut Schmidt in October 1981 -- that the Soviet Union would 
weaken in the mid-1990s, permitting German unification. His aim was to perfect 
a special relationship between France and Germany stable and deep enough to 
survive another redrawing of the map of Europe. He found Mr Shmidt sceptical, 
but discovered a kindred spirit in Mr Schmidt's successor, Helmut Khol. His 
scheme, hatched with Mr Khol, was for a re-launch of European integration with 
France and Germany as the joint architects.
  The integration envisaged by Mr Mitterrand and Mr Khol was, in practice, a 
more demanding agenda for France than it was for Germany. France had all its 
Gaullist arrogance towards Europe and the world to master, it distrusted free 
trade and it feared for the erosion of its culture and language. Germany had 
much less sovereignity to defend, and it had many more inhibitions about 
defending it.
  France also had a lot of catching-up to do with Germany in economic terms, 
if the two countries were to advance into the new Europe as equals -- a 
precondition for any advance at all, since neither country was likely to 
accept a Europe dominated more by the other. From this followed much of Mr 
Mitterrand's passion for a strong franc, another feature of his presidency. 
The economic benefits of the franc fort to France are much debated, but that 
is only part of the point. The idea is that France should be able to look 
Germany straight in the eye when talking about monetary union, the intended 
next step in Europe's integration. Mr Mitterrand came to power convinced that 
economics could and should be subordinated to politics, and in this matter at 
least he carried his point.

Meanwhile, in another country
The commonplace of a decade ago was that Mr Mitterrand would go down in 
history as having proven the capacity of the left to govern France. If that 
boast now rings somewhat hollow -- the last parliament of his presidency has 
the biggest right-wing majority in 75 years -- he has probably helped 
stabilise and entrech the Fifth Republic nonetheless. He has shown, through 
two periods of "cohabitation" with a right-wing prime minister, that the 
institutions of the republic can accomodate the demands of democracy in ways 
that were never imagined by their autocratic inventor, de Gaulle. This is not 
a structural change, but it is an example that will carry weight.
  Mr Mitterrand's socialist experiment in 1981-83 will be remembered as a 
failure; but it may be argued that he did France a service by cutting its 
losses quickly, and so packing into two years an educative process of 
exploration and disenchantment that had taken Britain, for example, much of 
the 1960s and 1970s to complete.
  In helping France to recognise itself as an essentially European power, and 
in removing many of the complexes that remained about its relations with 
Germany, Mr Mitterrand's record was unambiguously positive. Sadly for him, his 
hopes of ending his presidency not merely as the elder statesman but also as 
the sage and wizard of European union began to crumble with the collapse of 
East European communism in 1989. He responded so clumsily to these events, and 
to the fall of Mikhail Gorbachev in 1991, that it was hard to make sense of 
his tactics except as a measure of human frailty.
  The biggest reservation that history seems likely to enter against Mr 
Mitterrand will concern less his public than his private face. If it may not, 
in principle, be necessary for a national leader to possess any recognisable 
or consistent ideology, it tends to be thought desiderable in practice. But Mr 
Mitterrand, having flirted in a long life with almost every credo imaginable 
from Catholicism to agnosticism, monarchism to republicanism, P�tanism to 
socialism, came by the end to resemble a sort of geological formation in which 
the visible new layers concealed and preserved the old layers.
  It is open to argument, notably, whether Mr Mitterrand ever believed very 
deeply in many of the doctrines of socialism that he preached for so long. It 
may have been enough that when the Fifth Republic was being created and the 
battle-lines of French politics were being redrawn in the late 1950s, Mr 
Mitterrand set about annexing the left because de Gaulle's followers had 
annexed the right. If France as a whole may not feel particularly betrayed by 
Mr Mitterrand on that count, the Socialist Party may have cause to feel 
differently.
  New light has been shed recently on Mr Mitterrand's life before he 
discovered socialism, and it has not been flattering. His decision to 
co-operate with a book revealing him as a fellow-traveller of the far right in 
his teens and an enthusiastic servant of the Vichy government in his twenties 
might have won him marks for frankness: but he spoilt the effect by betraying 
his lingering loyalty to Ren� Bousquet, an acquaintance for more than 40 years,
who as Vichy police chief sent tens of thousands of Jews from France to the 
Nazi death-camps. In old age, Bousquet may have been saved from prosecution 
by Mr Mitterrand's influence.
  If Bousquet was emblematic of one sort of moral failing in Mr Mitterrand, 
another failing was signalled by the high incidence of financial scandals that 
marred the latter's reign. At least two of them involved the closest of all 
his surviving friends, Patrice Pelat, who grew rich when a state-owned company 
bought his family firm for ten times its worth in 1982, and richer still 
trading shares on tips from government sources. There were, too, moments 
when criminal charges seemed to threaten half the leaders of the Socialist 
Party, as judges uncovered records showing that the party had been financing 
itself largely on illegal kickbacks from local government contracts. An opinion 
poll last year found that more than half the French public thought it was time 
for a "clean hands" operation on the Italian model, to purge politics of 
entrenched corruption.
  Not that Mr Mitterrand is likely to be bothered much by such verdicts. Asked 
once by Jacques Attali, his adviser and amanuensis, what he thought was the 
most important quality in a plitician, he gave a reply almost frightening in 
its bleakness. The secret, he sais, was "indifference".



Europe, 9 December, E

Europe learns its alphabet
Is monetary union a necessary part of the European Union?

At the pretty Dutch town of Maastricht,way back in December 1991, it had 
seemed so obvious. In the quest for "an ever closer union among peoples of 
Europe", there would be "a single and stable currency". This would complete 
the final stage of an economic and monetary union (EMU) that might lead -- 
automatically, some hoped and others feared -- to some political union. EMU, 
Europe's leaders solemnly declared, would be fulfilled by the end of 1997 or, 
at the latest, by the start of 1999.
  Thye obvious was never likely to be easy; there are too many social and 
economic disparities between the European Union's member states: There are 
also deeper rifts; sceptical Britain and Denmark have both won the right to 
opt out from EMU.
  Now the question is whether the difficult is becoming the impossible, and 
what consequences, good or ill, will follow. After all, the value of economic 
and monetary union has been an article of faith for France and Germany ever 
since President Val�ry Giscard d'Estaing and Chancellor Helmut Schmidt met 
in Aachen (the seat of Charlemagne, an earlier Euro-enthusiast) in September 
1978 to agree on a "European monetary system". Their successors, Fran�ois 
Mitterrand and Helmut Khol, were notably devout. Even Jacques Chirac, France's 
new president, who has little of Mr Mitterrand's closeness to Mr Khol and not 
much instinctive zeal for le projet Europ�en, feels obliged to preach EMU's
 virtues.
  No wonder: for both France and Germany hang great hopes on the project, 
even if these are not exactly the same. To France, such a union is a means to 
harness German economic might to European (especially French) purposes. To 
Germany, it is a way of confirming a return to the neighbourly fold, with the 
prospect of supranational institutions to keep a federalising Europe in 
German-style order. For both countries, enemies in three wars in a mere 100 
years, it ensures a "European Germany".
  The faith has been tested in the past, not least in speculative attacks -- 
most spectacularly in September 1992 and then again in July 1993 -- against 
the exchange-rate mechanism, a currency grid which is supposed to prepare the 
way for a single currency and demands that central banks support each other 
when currencies come under pressure. Today, there are new tests, ones which 
cannot be blamed (pace Jacques Delors, former president of the European 
Commision) on "Anglo-Saxon speculators".
  This time the pressures reflect the wider doubts within the European 
heartland, among both politicians and the people they supposedly speak for. 
The 1997 target for EMU has already been abandoned. It demanded that a majority 
of the EU's 15 members meet strict criteria of economic convergence, and by 
now it is clear that not enough of them will. By contrast, the 1999 target 
requires no majority: the treaty instructs those who do meet the criteria 
(save opt-out Britain and Denmark) to move to EMU willy-nilly. On that date 
the conversion rates of their currencies "shall be irrevocably fixed": the 
single currency will then exist in banking computer programmes and, within at 
most three years, in people's pockets.
  All this may have seemed clever to the bureaucrats who drafted Maastricht's 
tortuous prose. The trouble is, the closer the 1999 deadline gets, the tougher 
the problems. Indeed, the true deadline arrives up to a year earlier. If 
Europe's banks are to be ready, they must know roughly that far in advance 
which countries have met the criteria. Since one criterion is to have 
maintained a stable exchange rate for two years, the years 1996 and 1997 
become a decisive test of fiscal and economic rectitude as defined at 
Maastricht. The test is especially nerve-stretching for the Franco-German 
alliance because in 1998 France will hold a parliamentary election and 
Germany will choose a new chancellor.

The diverging definitions
No wonder, then, that politicians in Bonn and Paris are so worried. 
Frightened, thanks to paranoid memories of the Weimar republic, that the loss 
of the D-mark will resurrect inflation and bring un-German sloppiness into the 
conduct of monetary policy, at least 60% of the Germans oppose the mark's 
replacement by some form of Eurocurrency (the actual name of which is meant to 
be chosen at next week's EU summit in Madrid). To reassume them, the president 
of the Bundesbank, Hans Tietmeyer, and the financial minister, Theo Waigel, 
are competing with each other to stress that the economic convergence criteria 
must be strictly adhered before EMU can happen: there will be no nods and 
winks for Germany's friends.
  Mr Waigel does nor stop there. Last month he proposed a "stability pact" for 
those countries that do qualify for EMU. The essence is that, once the 
monetary union exists, its members will have to follow even tighter economic 
criteria, at the risk of punitive fines if they do not. Indeed, Mr Waigel's 
strictures are so severe that some suspect him of duplicity; his aim (and that 
of some people in the Bundesbank), they say, is not to achieve EMU, but to 
prevent it.
  That may be a conspirancy theory too far. More plausibly, Mr Waigel is just 
ensuring that the opposition Social Democrats in Germany now headed by the 
combative Oskar Lafontaine, cannot accuse the government of opening the door 
to a dangerously floppy Eurocurrency. Even so, one look at France is enough to 
see the pain that the Maastricht criteria already bring, even without Mr 
Waigel's planned tightening.
  Crippling strikes by public-sector unions, ill-tempered student protests, 
plummeting poll ratings for President Chirac and his prime minister, Alain 
Jupp� -- all are seen as the result of France's need to reduce government 
borrowing to the level decreed at Maastricht. France would have had to cut its 
budget deficit anyway, even if Maastricht had never happened. But, in most 
people's eyes, it is Maastricht that has made the government start cutting 
now. And that will not help to make EMU popular among the French.

The emerging objections
Is the potential gain worth the present pain? Advocates of a single currency 
can reel off the advantages. For business, EMU would eliminate the cost 
(reckoned to be some $30 billion a year) of foreign-exchange transactions and 
exchange-rate hedging. For individual Eurocitizens, it would rid them of 
currency-exchange robbery (go round the Union changing money at each 
border and you will end up with half the amount you started with). For 
governments, it would help to stabilise the international currency markets. 
For the Union's single market, inaugurated at the end of 1992 to allow the 
free movement of people, capital, goods and services, it would mean added 
efficiency and an end, within the monetary union, to "competitive" 
devaluations.
  Maybe so. But there are serious counterarguments. One is that the effect of 
transaction costs is exaggerated; if you want greater prosperity, there are 
other things that will provide much more of it. The single market, on this 
view, could be better improved by liberalising Europe's energy and telecoms 
industries, and by ensuring open public-sector procurement policies.
  Another objection, which awakens the deepest instincts of national identity, 
is that EMU would mean a loss of sovereignity. Monetery policy would be set by 
a supranational European central bank, and governments would lose the ability 
to devalue their way out of a crisis. 
  Eurosceptics fear that EMU would be the start of a "United States of 
Europe", with a European central bank acting as its Federal Reserve and with 
something like a European ministry of finance potentially emerging out of the 
consequences of a single currency. This, of course, is exactly what 
Euro-enthusiasts would like. There is also a school of thought which says that 
for a country to join EMU would mean not so much losing sovereignity as merely 
pooling it with others. Indeed, it is the opportunity to escape the 
Bundesbank's current dictatorship over other people's exchange rates that so 
attracks the �narques who run French policy and who are determined that Europe 
should be administered in France's interest.
  More prosaically, in a monetary union uncompetitive workers could no longer 
be cushioned by a falling exchange rate. Instead, they would either lose their 
jobs or have to accept lower wages. This too causes pain, personal and 
political. Nor could the sufferers' governments expect much help from their 
EMU colleagues: Article 104b of the Maastricht treaty specifically rules out a 
financial rescue by one country of another.
  In theory, of course, nobody should need any help. The Maastricht criteria 
were drawn up to prevent such disparities within EMU. Economies are to 
"converge" by attaining certain bench-marks: exchange rates must be stable 
within the exchange-rate mechanism for at least two years before the selection 
for Emu membership is made; long-term interest rates must be within two 
percentage points and inflation within 1.5 percentage points of "the three 
best performing member states in terms of price stability"; public deficits 
must be no more than 3% of GDP; and gross government debt must be no more 
than 60% of GDP.
  It is these last two that are causing most of the trouble. A habit of 
welfare-state generosity, plus the recession of the late 1980s and early 1990s 
and the high interest rates that followed Germany's unification, has stretched 
most government balance-sheets beyond the Maastricht norms.

It's not just France
The theory which Mr Jupp� and others now chant like a mantra is that meeting 
the criteria will start a virtuous circle. Lower deficits wiil mean lower 
interest rates, which will mean more investment, more jobs and more tax 
revenue to lower the deficit once again and pay off the accumulated debt.
  The paradox, of course, is that lower deficits will come only by raising 
taxes or reducing public spending -- and neither of those things will in the 
short run produce more jobs. They will mean fewer jobs, and will therefore 
produce more of the sort of street protests now being endured by Mr Jupp� as 
he belatedly begins to reform the social-benefits system that France, with or 
without Maastricht, can no longer afford.
  One look at the appraisals just produced by the European Commision shows 
that Mr Jupp� is not the only prime minister with problems. At the moment only 
Germany and tiny Luxembourg meet the Maastricht criteria in full. Ireland is 
allowed to join this dismally short list, despite a government debt of 85.9%, 
only because this figure is, in a loophole provided by the Maastricht-drafters, 
"approaching the reference value at a satisfactory pace".
  How long will the list grow by decision-time, a bit over two years from now? 
Yves-Thibault de Silguy, the Frenchman who is the EU's commissioner for 
monetary affairs, reckons that if they stick to their present policies 
Finland, France and Holland will all qualify, along with the two potential 
spoilsports, Britain and Denmark. With a bit more effort, Austria, Spain 
Portugal and Sweden could get there,too. So could Belgium, if you make a 
hugely generous interpretation of that Maastricht loophole for its predicted 
debt level of some 130% of GDP.
  Mr de Silguy's optimism may be right -- but only if governments are willing 
to tough it out and if, as the commissioner hopes, some happy economic 
fundamentals "spur renewed healthy growth".
  The first "if" is particularly tricky, since it means further austerity at a 
time of already appailingly high unemployment. Across the EU, a tenth of the 
workforce is without jobs. In France, the jobless rate is almost 12%, in 
Finland 17%, and in Spain nearly 23%. In such circumstances is it sensible, 
many wonder (including Philippe S�guin, the speaker of French National 
Assembly and a potential prime minister should Mr Jupp� stumble), to cut 
budget deficits and maintain currency parities for the sake of Maastricht's 
EMU timetable?
  Such heretics have a point. The convergence criteria have the appearence of 
good economic housekeeping, but they are nonetheless arbitrary. Belgium 
manages to have a long currency and low inflation despite its steepling debt 
ratio, the highest in the OECD. Most advanced economies, including Germany's, 
accomodate budget deficit well beyond Maastricht's 3%-of-GDP limit at some 
time in the economic cycle. In short, the convergence criteria are a 
straitjacket unadjusted to the patient's size.
  This is why Mr Waigel's stability plan is open to more than one 
interpretation. Once EMU happens, it is clearly sensible that its members 
should co-ordinate their economic policies, lest their economies diverge and 
split the whole venture apart. That is why disobedient members can be required 
under the Maastricht treaty to place non-interest-bearing deposits "of an 
appropriate size" into the communal coffers, and even pay fines "of an 
appropriate size". In the same fashion, those who do not at first qualify for 
EMU will still have to be well behaved (there are punishments if they run 
"excessive deficits"), lest they threaten those within EMU by self-serving 
devaluations.
  But Mr Waigel's definition of obedience, and of the penalties for 
disobedience, is very tough. He says that, over the course of the whole 
economic cycle, EMU countries should have to keep their budget deficits 
limited to an average of 1% of GDP. Moreover, for every single percentage 
point that a country strays beyond the Maastricht limit of 3%, it should lodge 
a non-interest-bearing deposit equal to 0.25% of its GDP. Stay beyond the 
limit for two years and the deposit becomes non-returnable: in short, the 
offending country pays a fine.
  This is strong stuff. If it had applied in 1991, Britain (which then met the 
entry criteria) would by now had handed over some 3% of its GDP, of which 
two-thirds would have become fines, and France 2.25% of its GDP, with half 
becoming a fine. Here, clearly, are the beginnings of supranational control 
over a country's economy.
  It could also be self-defeating. Why worsen a budget deficit by fining the 
country that commits it? That will only make it harder for the country to 
accept EMU's disciplines. No wonder France's finance minister, Jean Arthuis, 
and his counterparts from other EU countries were careful in Brussels on 
November 27th to praise only the principle of Mr Waigel's proposal, not its 
details.
  If getting to EMU will be difficult, so too, it seems, will be staying there.
For some, that is another reason, if not to abandon the goal of a single 
currency, at least to postpone it. They are worried about the discrimination 
inherent in a timetable  which assumes that only some of the EU's members 
will at first be fit to join.
  Spain and Portugal, in the Union only since 1986, fear they will be lost in 
the mist on Europe's periphery if, as seems likely, they cannot meet the 
1999 deadline. Italy, a founder-member of the EU, feels insulted that Germany, 
a fellow founder-member, now openly states the obvious: Italy will not meet 
the deadline even if the lira re-enters the exchange-rate mechanism. Publicly, 
the Latin trio will question neither Maastricht's timetable nor its criteria. 
Privately, they are poised to press the case for "flexibility".
  Will they succeed? They do not want a Europe in which some, mainly north 
Europeans, are more equal than others. But, in preparing for EMU, Europeans 
have already accepted the idea of a multi-speed process. In allowing apt-outs 
to Britain on EMU and social policy, and to Denmark on EMU, defence and 
European citizenship, they have also allowed different classes of membership. 
The amount of such "variable geometry" will doubtless increase if and when the 
EU expands eastwards. One way or another, whatever the verbal camouflage, some 
countries will almost certainly end up in a hard core that omits other 
countries.
  That is too tricky an issue to be dealt with by an overburdened Madrid 
summit. So are the questions raised last month by Britain's prime minister, 
John Major: "How would Europe's institutions serve the interests of those 
countries which adopted the single currency and those who didn't? What would 
it mean for the Community budget? What would it mean for the single market?" 
Mr Major was making a fair point: countries outside EMU might respond to their 
weaknesses by allowing their currencies to depreciate against the single 
currency, to the detriment of the single market.
  But Mr Major and his fellow leaders will give answers only for the EMU 
"scenario", as it is called: the date on which the qualifiers will be selected 
(France wants the selection well ahead of its March 1998 election; Germany 
wants it delayed long enough into 1998 to have accurate economic data for 
1997); the types and amounts of public debt and banking transactions to be 
denominated in the non-cash single currency; the date for minting coins and 
notes; the date on which existing currencies will cease to be legal tender; 
and, for the sake of a bemused public, the name of the single currency.

More haste...
Will their work be in vain? The answer will come not from Madrid, but from 
Bonn and Paris. Before EMU can happen, it has to be approved by the German 
parliament -- and so, indirectly, by a still profoundly sceptical German 
people. That is why Germany's politicians are so determined to be strict about 
Maastricht.
  But the same politicians know that EMU will be nothing without France. Over 
then to Messrs Chirac and Jupp�: can they muster the will to outlast the worst 
social unrest since 1968, and bring the budget deficit within the Maastricht 
limit by 1997? 
  The longer the answer remains in the balance, the more tempting it will be 
to change either criteria or the timetable. The first option would never be 
accepted by German opinion. That is why, in October, Germany's economy 
minister, G�nter Rexrodt, held out hope of the second, endorsing a possible 
delay of "a year or two". The next day, he backed off: "A moratorium increases 
the danger that economic and monetary union will be talked away."
  Maybe. But delay may be better than clinging stubbornly to a timetable set 
by people who had not thought through the difficulties of the project, and who 
have not persuaded their voters of the need for it. In any event, EMU is not 
necessarily the only way of moving towards that "ever closer union". The 
building of Europe requires rock-steady foundation. Does this pass the test? 
It is not just the stability of France that is now being decided on France's 
streets.



Alone, 25 February, P

It can't be done alone
Europe should do more to defend itself and its interests. It is planning to do 
so. But the cold statistics suggest it cannot do the job by itself.

The trouble with talking about a "European defence policy" is that so many 
important questions still have no answers. Will NATO find a new role for 
itself in the post-cold-war era? How will the bitter experience of 
peacekeeping in ex-Yugoslavia, especially the row it has caused between 
Europe and America, influence the debate? What will come out of next year's 
constitutional conference of the European Union, the EU, which is due to 
examine (among many other things) the Maastricht treaty's vague commitment to 
"the eventual framing of a common defence policy which may in time lead to a 
common defence"?
  The debate over the redesigning of Europe's defence has begun in NATO and in 
the Western European Union (WEU), a group of ten countries that belong to both 
NATO and the EU. It has been stimulated by a change in American thinking about 
Europe. President Bush, like his predecessors, took a dim view of the quest 
for a "European defence identity", fearing that it could lead to America's 
disengagement from Europe. President Clinton is more relaxed. Robert Hunter, 
his ambassador to NATO, explains: "The cold-war argument that the alliance 
needed centralised military direction, and that a robust WEU could interfere, 
no longer applies. We support the WEU as a means of preventing the 
renationalisation of defence. The WEU will help to focus minds on security, 
and thus aid the EU's attempts at common foreign and security policies; and it 
will, like NATO, provide a home for the Germans. Furthermore, the more the 
European allies help themselves, the more Congress is likely to pay for 
transatlantic defence."
  The change in Washington has encouraged the British government to take a 
less suspicious view of European defence co-operation than it once did. The 
transatlantic spat about the war in ex-Yugoslavia has also had its effect. The 
Americans criticise the Europeans for refusing to make up their minds about 
the rights and wrongs of the war, and thus in effect letting the wrong side 
win. The Europeans retort that the United States is willing to take a moral 
stand but unwilling to risk its own soldiers' lives. "I feel like saying to 
the Americans, Why don't you put your troops where your bloody mouth is?", 
says a senior British diplomat. But, being British and a diplomat, he does not.
  This had already strained the alliance when, in November, Congress made Mr 
Clinton announce that American warships in the Adriatic would no longer help 
to stop arms getting through to ex-Yugoslavia -- in practice, to Bosnia. Never 
before had a NATO member declared it would cease to carry out an agreed NATO 
policy. Tempers calmed a little in December, when Mr Clinton said that 
American troops would take part in any evacuation of peacekeepers from 
ex-Yugoslavia. But the quarrel could erupt again if the new Republican 
majority in Congress insists that America should supply arms to Bosnia, a step 
which -- say France and Britain -- would necessitate the withdrawal of their 
peacekeeping troops.

Why change had to come
Behind this particular quarrel, however, lie the two halves of a bigger 
reality. After the collapse of the Soviet Union, America was bound to take 
home many of the troops it had previously stationed in Europe. At the same 
time the Europeans came to recognise that, if their talk of unity meant 
anything, they had to shoulder more of the burden of protecting themselves. 
Between them, these two things have changed the old ways of thinking. In 
particular, they have persuaded France and Britain, long at odds on this 
subject, to grope towards a new understanding.
  Nowadays, "we get on fine with the French so long as we can stop them from 
talking about fancy 'architecture', says one British official. Last November 
the two countries agreed to set up a joint planning group for their air 
forces, at High Wycombe in southern England. Britain plans to join France, 
Germany, Italy and Spain to make the "Future Large Aircraft", a military 
transport. British and French troops have worked harmoniously together in 
Bosnia. And, in a more general way, Britain now accepts that there will be 
times when it makes sense for European armies to act together through the WEU.
  Meanwhile France, for its part, is moving closer to NATO than at any time 
since 1966, when General de Gaulle walked out of the alliance's military 
organisation. France has hesitantly accepted the idea that NATO should be 
ready to undertake military operations outside its own territory -- something 
that may be increasingly necessary in the post-cold-war world. It also now 
agrees that the Eurocorps, which includes German, French, Spanish and Belgian 
troops, will come under NATO command in a crisis. Fran�ois L�otard, the French 
defence minister, unprecedentedly turned up at a NATO defence ministers' 
meeting in Seville last October. And France has allowed its warships and 
aircraft to take part in two admittedly not very glorious NATO-run 
operations: "Sharpguard", the Adriatic blockade of ex-Yugoslavia, and "Deny 
Flight", an attempt to keep the skies peaceful over that warring region.
  "The extent to which we participate in the alliance will reflect the degree 
to which it changes," says a French diplomat. The French government wants the 
Supreme Allied Commander in Europe to remain an American. But it thinks that 
NATO's 1,200-strong bureaucracy in Brussels should be shaken up, for instance 
by creating new departments to deal with peacekeeping and with the 
ex-communist countries that have signed "Partnership for Peace" agreements 
with NATO.
  The French-British rapprochement is an excellent thing. Yet one large 
difference still seems to separate Western Europe's two nuclear powers.
  Jean-Marie Gu�henno, France's ambassador in the WEU,notes with approval that 
Britain has agreed to several measures of co-operation, but then sighs: "We 
say that these steps are to promote the political goal of European Union; but 
the British will not say that." (When those words were relayed to a British 
diplomat, he exclaimed: "We can't say it because of the Daily Mail."). Mr 
Gu�henno argues that the idea of a common defence has to be presented in a way 
that wins public support. He reckons that, when German soldiers of the 
Eurocorps drove down the Champs Elys�es in armoured cars on July 14th last 
year, the average Frenchman liked what he saw. It is not clear that German 
soldiers marching down Whitehall would have the same effect in Britain

A rather thin pillar
It is now generally agreed, by America as well as by all the big countries of 
the European Union, that Europe should have its own way of organising a 
military operation in which America does not want or does not need to be 
involved, and which therefore cannot be a NATO operation. This is what is 
meant by a European "defence identity". The organisation through which this 
identity will be expressed is the WEU.
  This long obscure body, born in 1948 and for years apparently moribund, has 
been brought back to life. Its headquarters were moved from London to Brussels 
in 1993, so that its staff can work closely with officials at NATO and at the 
secretariat of the EU's Council of Ministers. Its parliamentary assembly and 
think-tank are in Paris, which pleases the French. It has a new leader, Jose 
Cutileiro of Portugal.
  Even so, it remains something of a runt in the menagerie of European 
institutions. Its secretariat, with a staff of 60, organises the twice-yearly 
meetings of its council, which consists of the foreign and defence ministers 
of the ten member countries. Five more countries, Denmark, Ireland, Austria, 
Sweden and Finland (members of the EU but not of the WEU), attend as observers.
Iceland, Norway and Turkey (in NATO but not the EU) have associate 
membership. Nine ex-communist countries are "associate partners". The full 
complement of 27 ambassadors meets every two weeks.
  The WEU's planning cell consists of 40 people. That is tiny (though remember 
that the precursor of the European Commision had a staff of under 100 when 
Jean Monnet fouded it in 952). The planners are drawing up contingenty plans 
for operations under three headings: humanitarian relief, peacekeeping and 
crisis-management. In November they ran a joint planning exercise with the 
Nato naval headquarters at Northwood, near London. British, French, Dutch and 
Portuguese officers took part. A theoretical taskforce of soldiers, warships 
and military aircraft was sent to carry out a relief mission in an imaginary 
African country. One British participant says the experiment worked "quite 
well".
  The WEU did not take the responsibility for a real, live military operation 
till 1988, when it dispatched a force of minesweepers to the Gulf. It sent 
another lot of warships there in 1990, for the Gulf war. The WEU now shares 
responsibility with NATO for the Adriatic blockade, and helps Hungary, 
Bulgaria and Romania to apply the (leaky) Danube blockade against Serbia. It 
has shared, with the EU, the administration of the Bosnian town of Mostar.
  Even when the Europeans have worked out their "defence ideni�tity", the 
WEU's operations are unlikely to be a whole lot grander. The Europeans might 
send troops to supervise the provision of food to a famine-hit African 
country; or to rescue Europeans trapped in, say, Algeria's civil war; or to 
try to keep the peace in an area of ethnic tension -- for example, in the 
Hungarian-speaking part of Romania, if things there got out of control. 
Another rung up the escalation ladder, they might conceivably try to separate 
the adversaries if Serbia and Bulgaria looked like getting into a fight over 
Macedonia. But it is unlikely that they could organise a purely European 
force, for any purpose, even a twentieth of the size of the alliance that won 
the Gulf war. The bigger the crisis, the more likely that the Europeans would 
have to turn to America for help.

But what connects the pillar?
So the European pillar of the new Euro-American military bridge is not exactly 
stout. And one awkward question about it has not yet been answered. The 
question arises from NATO's decision, just over a year ago, to create 
something called "Combined Joint Task Forces", CJTF. This clumsily named 
concept is NATO's most radical piece of new thinking in its 45-year history.
  For most of that time, NATO's principal responsibility has been the defence 
of its members' territories. Article 5 of its founding treaty obliges each of 
the 16 members to help each other if attacked. That commitment ramains, so 
long as Russia or any other country might pose a threat to the allies' safety. 
But the new taskforces are a leap beyond NATO's old definition of itself. 
They will enable the alliance to mount military operations outside Western 
Europe and North America; and -- another big innovation -- troops from 
non-NATO countries, such as Poles, Czechs and Hungarians, might join in these 
operations.
  Each taskforce would consist of chosen units from various countries, 
depending on the nature of the job to be done. To save money, the taskforce 
would be controlled by an existing headquarters, such as that of a national 
army corps, or the Eurocorps, or the British-led Allied Rapid Reaction Corps, 
or the French-Spanish-Italian Mediterranean fleet. These "borrowed" 
headquarters would be adapted -- adding Frenchmen, say, or dropping Germans 
-- according to whose soldiers were involved.
  Some of these taskforces could be the WEU's, not NATO's. But most of them, 
even those with no American soldiers, would have to use NATO "assets" -- 
transport aircraft, command-and-control systems, the intelligence NATO 
collects by electronic and other means. America would have to give its 
blessing to the use of these things. As yet, however, there is no agreement 
either on how that blessing is to be obtained or on how even the NATO-led 
taskforces would mesh in with the wider NATO organisation.
  The French do not want American staff officers telling a taskforce what to 
do if no American troops are involved. "The mixing-up of nationalities in the 
alliance's integrated structure is efficient, but would not be politically 
desiderable in one of the new taskforces," says a French diplomat. So the 
French would cut the Supreme Allied Commander and SHAPE (the alliance's 
military headquarters at Mons, in Belgium) out of the chain of command.
  The French also complain that NATO's current organisation would give too 
much freedom to a taskforce's military commander. They accept that, in a 
genuine fight, soldiers do not have time to consult politicians. But they see 
no reason why peacekeepers should not be subject to political control. They 
think that the North Atlantic Council (NATO's foreign ministers) or its 
military commitee might have authority over the headquarters managing a CJTF.
  Most of the other Europeans do not care for the nationalism they detect in 
some of these French ideas. The Germans, in particular, are deeply committed 
to NATO's integrated military structure. "For us Germans, being integrated is 
part of our whole purpose," explains a German diplomat.

The cost of self-sufficiency
The French also fret about the fact that each of NATO's 16 members would have 
to approve the WEU's use of its assets -- the 16 including, of course, America. 
What if America said no? "No one has yet thought of a scenario in which the 
WEU would want to do something and America would oppose it," says America's 
Mr Hunter. "But, if you Europeans do want to borrow our intelligence, 
transport or communications capacity in NATO, please ask on a case-by-case 
basis: the answer is likely to be Yes."
The French insist that it makes no sense to talk of a "European identity" 
unless the WEU can decide whether to mount an operation without NATO strings 
attached. They worry that, for all Mr Hunter's assurances, Congress would not 
want American military equipment to be used in a confrontation over which 
America had no control. "One lesson of Bosnia is that joint authority does not 
work," runs the French argument. "There, the problem has been the tug-of-war 
between the United Nations and NATO. We do not want that to happen between 
NATO and the WEU."
  Yet the reality is that in present circumstances most WEU military 
operations will need some American support. The French, unlike the British, 
think it is quite possible that Americans will one day pull out of Europe. 
This is why they still say that "in the long run" Europe should be able to 
look after its own defence. Could it?
  Without America, a serious Europeans-only defence would require huge 
increases in spending on, among other things:
That vital thing called logistics, especially air transport.
Intelligence, including spy satellites.
Europe's nuclear forces and, probably, an anti-missile defence system.
Computerised communications systems.
More ships, including aircraft carriers, for the Mediterranean and beyond.
The creation of common standards for everybody's tanks and other heavy weapons.
  Estimates of how much this could cost vary hugely, but all are high. The 
Royal United Services Institute in London guesses that such a policy might 
require the European countries to raise the proportion of GDP they spend on 
defence, now 2.5% on average, by 1.5 percentage points -- $107 billion a year 
at current prices -- and sustain that figure at least for the rest of the 
decade. It could be some time into the new century before it started to fall 
below 4% again. Others believe the increase would have to be even bigger.
  The French acknowledge that there is little prospect of European taxpayers 
being prepared to put up that kind of money. They reckon, however, that there 
are some sorts of military equipment Europe can do more about: not the least, 
military satellites.
  Within the next few months France is due to launch Helios 1, a 
reconnaissance satellite. It has so far cost Ffr11 billion ($2.1 billion), 
some of which Spain and Italy provided. Germany may take a stake in Helios 2 
and Osiris, French-planned infra-red and radar satellites. The WEU satellite 
centre at Torrejon in Spain will process the data from these satellites. A 
recent WEU study suggested that a complete family of satellites, including one 
for communications, would cost $19 billion over 25 years. Even this would 
probably not give Europe a complete command-and-control system as good as the 
one America has now.
  When Frenchmen talk of a European defence identity, they insist it must have 
an industrial component. France argues that, unless the European powers 
collaborate on defence projects, the continent will have no choice but to buy 
American. The French are willing to co-operate with the Americans only on the 
largest projects, such as a defence  against ballistic missiles. The Germans 
tend to agree that a European defence policy need some sort of joint 
military-production policy, and have consented to the creation of a 
French-German arms-procurement body. The British, though they frown at the 
mention of an industrial policy, have agreed to join a European armaments 
agency that will try to "co-ordinate" weapons-making within the WEU.

If you want to be practical
And where is it all leading? The French regularly say -- and sometimes 
Germans do too -- that in the end the WEU should become part of the European 
Union. When British sceptics point out that some of the EU's present members 
do not belong to the military body, and may never want to, and that some of 
its future members may feel the same way, the French say that they are 
looking "a very long way ahead".
  The future, of course, is another country; they may do things differently 
there. But for as far ahead as one can see Europe will have to accept some 
hard realities. There may indeed emerge, out of the mistiness of its 
"identity-building", the concrete shape of a European defence pillar. But that 
pillar will be pretty modest in size. And it will still need a solid 
connection to America.



Counter-attack, 8 July, S

The counter-attack of God
Soon after his rescue from his ordeal in Bosnia, Captain Scott O'Grady 
returned to a hero's welcome at Aviano air-force base in Italy. After his F-16 
was shot down by a Bosnian Serb missile, the American pilot had spent six 
perilous days in hiding before being plucked to safety. "Right off the bat," 
he told his Aviano colleagues and the world's media, "the first thing I want 
to do is to thank God. If it wasn't for God's love for me and my love for God, 
I wouldn't have gotten through it. He's the one that delivered me here, and I 
know that in my heart."
  If a British or a French pilot had said that, it might have been mildly 
surprising. From an American, the words seemed only natural. Americans, be 
they politicians, generals or victorious sportsmen, habitually invoke God at 
times of trauma or triumph. Like the overflowing church car parks on Sundays 
and the evangelical messages along southern roadsides and on radio stations 
throughout the land -- not to mention the national motto, "In God We Trust", 
on every piece of American money -- such moments are powerful reminders of the 
strength of religious faith in America.
  For proof of God's existence in the American mind, look at opinion polls. 
Almost all Americans (the figure typically hovers around 95%) say they believe 
in God. Four out of five believe in miracles, in life after death and in the 
virgin birth. Belief in the devil has risen sharply, to 65% says a recent 
poll, and 72% of Americans believe in angels. A survey by the American Bible 
Society reports that nine out of ten own a bible, and 27% own more than four 
copies.
  Quite how often or attentively Americans actually read their bibles may be 
another story ( many cannot name any of the four gospels). Nevertheless. their 
religiosity stands in marked contrast to the rest of the developed world, on 
the evidence of a World Values Survey conducted in 1990-93. In America, 82% of 
respondents said they cosidered themselves "a religious person", compared with 
55% in Britain, 54% in western Germany and 48% in France. In the same survey, 
44% of Americans said they attended a religious service at least once a week, 
against 18% in western Germany, 14% in Britain, 10% in France and mere 4% in 
non-worshipping Sweden. There are more places of worship per head in America 
than anywhere else in the world, with new ones constantly being built.
  America would seem to have anything but a "Culture of Disbelief", as the 
title of a 1993 book by Stephen Carter of Yale University put it (Anchor 
Books, $14.95). The country oozes religion.
  How many other places have a National Day of Prayer (the day in May, 
instituted by a congressional resolution in 1952, that now sees a growing 
number of events around the country)? Where else are formal or festive 
occasions so solemnly marked with an "invocation" before dinner? Whose 
politicians so liberally deploy the language of the scriptures, from George 
Bush's enlistment of God in the Gulf war and Bill Clinton's "new covenant" to 
the Satanic understones of Ronald Reagan's "evil empire"? It seems bizarre 
that America, of all places, should be viewed as somehow cramping God's style. 
It would be far more accurate, surely, to talk of a "culture of belief".
  Yet the popularity of Mr Carter's work, a call for religion to come out of 
the closet where it had supposedly been consigned, is one of many signs that 
something significant is stirring in America's spiritual life.

A culture of belief
People who pray and believe in God, Mr Carter writes, "are encouraged to keep 
it a secret, and often a shameful one at that." He would like religion to be 
treated with more respect: for instance, the notion that devout believers are 
somehow less "rational" than non-believers deserves to be jettisoned. Although 
Mr Carter is a firm supporter of the separation of church and state and the 
ban on organised prayer in public schools, he objects to what he sees as a 
trend among politicians and lawyers to treat religious beliefs as arbitrary 
and unimportant, not really to be taken seriously. Faith, he argues, is being 
trivialised in modern America.
  Newt Gingrich would agree. Not long before he became speaker of the House of 
Representatives, Mr Gingrich gave a lecture on religion and politics to the 
Heritage Foundation, a right-wing think-tank in Washington. He caricatured the 
"secular, anti-religious view of the left" in which religion is fine as a 
"tamed hobby" at the weekend. By contrast, Mr Gingrich's vision is of an 
America " in which a belief in the Creator is once again at the centre of 
defining" what it means to be an American.
  These are big words, and they herald a big debate. The gathering argument 
over the proper place of religion -- whether in private lives, in public 
places or in politics -- may be one of the defining issues of the age.
  The strength of religion should be no surprise in a country many of whose 
early colonisers were dissident religious enthusiasts. The founding fathers 
uninhibitedly invoked God in their endeavours -- all men, says the Declaration 
of Indipendence, are "endowed by their Creator" with certain inalienable 
rights -- and their suspicion of overbearing government led them to keep 
church and state separate. The Bill of Rights precludes any law "respecting an 
establishment of religion, or prohibiting the free exercise thereof".
  The combination of freedom and competition proved extraordinarily 
invigorating. Very early on, the churches proved adept at organising and 
marketing themselves; indeed, says John Butler, also of Yale, America had "a 
national market in religion long before there was a national market in 
economics." Now, with no state church, a rich variety of creeds -- albeit 
mostly branches of Christianity -- vie for their share of the market. New ones 
can arise and thrive (the Mormon church, for example, remains one of the 
fastest-growing); offshoots of old ones can adapt themselves so as to appeal 
to new audiences.

Pilgrims' progress
The ever-perceptive Alexis de Tocqueville observed in 1835: "Religion in 
America takes no direct part in the government of society, but it must be 
regarded as the first of their political institutions." Americans of all 
classes and parties, said the French visitor, regarded God as a force for 
order amid the potentially destructive freedoms of democracy. "What can be 
done with people who are their own masters, if they are not submissive to the 
deity?" (As if Americans needed a reminder, in the 1950s the Pledge of 
Alliagiance was amended to read "one nation under God, indivisible".) Modern 
European observers cannot fail to be struck by the extent to which America, a 
nation founded on lofty ideas and forever following a Dream, remains less 
resistant than cynical old Europe to idealism and faith.
  In America, unlike much of Europe, religious belief has strengthened down 
the years. Only 17% of adult Americans belonged to a church when the country 
broke away from Britain. That rose to 37% by the 1861-65 civil war, to 50% in 
the first decade of the present century and (counting synagogue members,too) 
to nearly 70% in the 1990s. The rise was remarkably smooth until the 
rebellious 1960s, when the numbers fell somewhat, prompting a 1966 Time cover 
story to ask provocatively, and prematurely, "Is God Dead?".
  A resurrection is now apparent. Baby-boomers with families have returned to 
church in large numbers. David Roozen, a religion professor at Hartford 
Seminary in Connecticut, notes that among people born between 1945 and 1954 
regular church attendance rose from 33% in 1975 to 41% in 1990. 
  The troubles of the American family, and all the social ills associated with 
family breakdown, have prompted people to turn to religion in search of moral 
moorings. The approaching millennium is also being invoked as somehow adding 
to the impetus for spiritual renewal. "There is a stirring across our land 
some are calling pre-revival," says an advertisement for a national clergy 
conference in 1996. "Since God has brought revival in every century of our 
country's history, we are hopeful that He will do so again soon."
  Examples abound of spectacular growth in the religion business, evidence 
that a receptive market awaits those who can identify new niches. In his 
"state of the union"address to Congress this year, two people Bill Clinton 
singled out for special mention were the Reverends John and Dian Cherry of 
Temple Hills, Maryland. From small beginnings in a living-room in the early 
1980s, their church has grown to 17,000 members, one of the biggest in the 
country, and it is still adding 200 members a month. One of its chief aims is 
to keep families together. "This is the kind of work that citizens are doing 
in America," said the president, to loud applause. "We need more of it, and it 
ought to be lifted up and supported."
  In the anonymous expanses of American suburbia there is a growing fashion 
for "megachurches". In places where the shopping mall long ago ousted the 
village centre, places of worship that are more like stadiums or convention 
centres retail religion carefully tailored to middle-class lifestyles and 
tastes. These megachurches have lots of convenient amenities and the modern 
audio-visual technology to reach thousands of worshippers at a time.
  Traditionalists may scoff at this Religion Lite. Enthusiasts see it as 
moving with the times and, by drawing people in the secular suburbs to 
religious values, as a way of changing America for the better.

Parties of God
This spring and summer, stadiums across America have been filled with 
Christian men gathering in their thousands to hear how individual virtue can 
be the starting point for the transformation of home, community and, 
ultimately, the country they all belong to. The men pray, bond in small 
groups and listen en masse as preachers tell them what it means to be "real 
men" in today's confusing world. Many return to their local churches having 
pledged to keep a set of promises, such as following Christ, practising sexual 
purity, building a strong marriage and forming "vital relationships" with 
other Christian men.
  The idea of Promise Keepers, as the organisation behind this movement is 
called, came to Bill McCarney, at the time the University of Colorado's 
football coach, during a car ride five years ago. Since 4,200 men showed up 
for the first conference in a Bouder stadium in 1991, Promise Keepers has 
taken off dramatically. It outgrew Boulder and went national:280,000 men 
attended conferences in seven cities last year, and at least double that 
number are expected this year. A gathering of 1m men in Washington , DC, is 
planned for 1997.
  When new generations of "consumers" shop around for religion, some brands 
will prove more successful than others. Generally, the conservative varieties 
have been doing best. The total membership of churches that can be loosely 
categorised as liberal and moderate (such as Episcopalians, Presbyterians and 
Methodists) shrank somewhat between 1950 and 1990. Over the same period, the 
number of Roman Catholics in America doubled, as did the membership of 
conservativeProtestant groups, such as Baptists and Lutherans.
  There has long been a close link in America between religion and voting. 
Roman Catholics and Jews tended to vote for the Democrats, for example; 
mainline white Protestants formed the backbone of Republican support. But not 
only has the country's religious composition been changing, so have the 
traditional party allegiances. Shifts in the voting behaviour of America's 
largest religious groups helped ro produce the political earthquake of 1994, 
when Republicans recaptured control of the Senate and -- for the first time in 
40 years -- the House of Representatives.
  "Religion was more powerful than economics in 1994," concludes a careful 
analysis of exit polls by four politics professors (Lyman Kellstedt, John 
Green, James Guth and Corwin Smidt) in the current issue of The Public 
Perspective, a sociology magazine. White Evangelicals and white Catholics, 
once core components of the New Deal coalition, have both to varying degrees 
deserted the Democrats. Together with the lingering Republican loyalty of 
mainline Protestants, and the general groundswell of concern about the 
survival of the family, this created the new Republican majority.
  Evangelicals, the sort of Christians who go out clutching a bible to spread 
the good word, have been moving away from the Democrats since the 1960s. 
Last November they established themselves as the weightiest chunk (29%) of the 
Republican coalition. Three out of four people who identify themselves as 
white Evangelicals voted Republican in the House election. Among white 
Catholics, the Republican tilt was less pronounced (53% voted Republican) but 
no less striking, because for so long the Catholic vote had seemed dependably 
Democratic. Younger Catholics were especially likely to vote Republican. In 
all, white Catholics constituted a hefty 22% of the Republican coalition 
last November. The Republicans did particularly well among Evangelicals and 
Catholics who go to church regularly.

The advance-guard's armoury
A new pattern of politics is emerging, according to the professors' exit-poll 
analysis, with Republicans "drawing the more religiously observant voters, at 
least among whites, and the Democrats attracting the least observant in the 
major traditions, seculars and various minority groups."
  Committed people with shared beliefs who gather together frequently in the 
same place: under the right circumstances regular churchgoers lend themselves 
splendidly to political mobilisation. The Republicans have been mobilising 
like mad -- none more effectively than Pat Robertson, tele-evangelist, failed 
presidential candidate and, after his 1998 bid for the White House, fouder of 
the Christian Coalition. In tandem with 33-year-old Ralph Reed, the 
organisation's cherub-faced, Washington-savvy executive director, he has 
turned the Christian Coalition into a formidable force.
  It boats 1.6m members and a dominant voice among Republicans in perhaps 18 
states. It has money, grassroots lobby power and, whatever its real ability to 
sway voters, a lot of influence over Republican politics. It worked hard 
against Mr Clinton's health reforms last year, helped to turn out the 
Republican vote in November, and then spent more than $1m to support Mr 
Gingrich's (mainly economic) legislative programme in the first 100 days of 
the new Congress.
  Now the Christian Coalition is presenting its bill for these services. Mr 
Reed has threatened not to support any Republican presidential candidate or 
running-mate in 1996 who tolerates abortion. And in May, flanked by top 
conservative lawmakers, he presented the Coalition's "Contract with the 
American Family", a loose outline of a legislative agenda.
  The agenda in part covers some standard conservative themes, such as 
family tax relief, school choice, stricter prison regimes, a crackdown on 
pornography. It would seek eventually to abolish the welfare state and 
replace it by channelling the money, with the help of tax incentives, to 
private charities (an idea enthusiastically supported by Mr Gingrich). The 
Christian Coalition is also asking for a constitutional amendment to allow 
prayer in public places, and it wants restrictions on late-term abortions.
  Taken in isolation, this programme would not look particularly potent. But 
the Christian Coalition can count on the backing of a number of other 
influential family-values groups. It has pledges of help from top Republicans 
in Congress, and in June some 60 lawmakers formed a Congressional Family 
Caucus, to restore "the traditional values of family and faith". Each item on 
the agenda was carefully tested in focus groups and found to enjoy at least 
60% public support, so the Christian Coalition could present its proposals as 
mainstream.
  Arch-conservatives such as Pat Buchanan, who is campaigning for the 
Republican presidential nomination, criticise Mr Reed's "contract" as too 
timid. From the other end of the Republican spectrum Arlen Specter, who is 
Jewish and also a would-be president, attacks it as devious, foot-in-the-door 
effort intended eventually to get abortion banned outright, to overturn the 
1962 Supreme Court ruling that keeps school-sponsored prayer out of public 
schools, and generally to undermine the separation of church and state.
  Some 80 representatives of a broad spectrum of religious groups 
responded to the Christian Coalition's"contract" by signing and delivering to 
Congress a "Cry for Renewal", which affirmed the desirability of injecting 
moral direction into the political process but deplored the Coalition's means. 
"The almost total identification of the Religious Right with the new 
Republican majority in Washington is a dangerous liaison of religion with 
political power," they said. "We testify that there are other visions of faith 
and politics in the land."

Belief has to stay free
This is hardly the first time that religious leaders have ventured 
controversially into the political breach. They were prominent (on both sides) 
in the battle over slavery, and in the civil-rights movement. But this time 
religious values are themselves at the heart of the argument. The argument 
threatens to be extremely divisive, in the country generally and in the 
Republican Party in particular. If the Christian right pushes its luck too 
far, it could lead to schism in the Republican coalition. Ironically, the rise 
of religious conservatism may turn out to be as much of a challenge for the 
Republicans as it has already proved to be for the Democrats.
  The culture of belief, so vividly on display at Aviano air base last month, 
is thriving in America, without the help of school prayer. The people who 
worry aloud about the country's shortage of moral values are part of the 
process that will probably ensure that the culture continues to thrive. 
Unless, that is, they go too far and attempt to impose rules based on a 
certain set of beliefs on everyone. Then the culture of belief clashes with 
another culture that runs even deeper in America: the culture of freedom.



Creation, 22 August, S

Also a part of Creation
This year's unlikely sight of middle-aged Britons taking to the streets to 
fight the trade in live animals reflects a moral debate that is not going to 
go away.

The organisers expected a thin crowd, but the church hall in Dover is 
overflowing. There are few suits or ties here, and hardly any rainbow 
warriors or professional activists. The average age in the crowd of perhaps 
300 must be 50; the faces are earnest and concerned, the clothing workaday. 
After some introductory words ("Live transport must come to an end and be 
replaced by a meat trade!") the lights dim, and a screen begins to glow with 
images.
  Such images! Calves shoved from lorries, and thrown from lorries' upper 
decks. More calves, teething the bars of tiny stalls where they are lucky if 
they can turn around. A bull suspended by one broken leg, then dropped from a 
height to a hard deck below. A man in an apron kicking a pig, again and again, 
as the pig shrieks from the blows. ("Sadistic, sick little man!" screams a 
voice in the crowd.) Pigs and sheep suspended upside-down, fully conscious 
(they are supposed to have been stunned but have not been) and still blinking 
and gulping for a few excruciating seconds as blood floods from their throats. 
In the crowd, faces are covered, a woman is crying and shouting something 
about "liars" and "murderers".
  Peter Whittam is watching it all, with his wife Marion. He is a 61-year-old 
who rents out flats, has voted Tory most of his life and has never before 
attended a meeting about animal rights; his face, stoic and craggy, is the 
very picture of middle England. As a man near the front stands to announce 
that he is "prepared to defy the law on this issue", Mr Whittam remarks 
quietly to a journalist that he has never seen footage showing humans in Nazi 
concentration camps being treated as badly as these animals.
  Would he, then, defy the law? "Basically I'm a very honest person, believe 
it or not, " he says. "But I feel very angry that we can allow that kind of 
thing to go on. I think the time has come when some form of civil disobedience 
is definitely justified."
  To the surprise of the country's political elites, animal rights has emerged 
as Britain's closest thing to a mass social movement. Daily demonstrations 
have gone on for months in Dover, Brightlingsea, Shoreham and other places 
whence calves and lambs are shipped to Europe. In the town of Brightlingsea 
(population about 8,000), the arrests this year number about 550 and the daily 
police mobilisations have cost around �2.5m ($4m). During the winter and 
spring, newspapers and television regularly brought pictures of middle-aged 
women struggling with policemen, waving signs and fists, and shouting 
furiously at lorries filled with live lambs and calves. Now the news coverage 
has died down, but the demonstrations continue: at dockside, in London's 
Parliament Square, and at the European Union's offices in Brussels.
  The key to understanding what drives these people is to realise that most of 
them are quite ordinary. Their unusual behaviour stems not from any 
obsessiveness or lunacy, but from the nature of the argument about animal 
rights. This issue is becoming, for Britain, what abortion is for America: a 
deeply divisive social conflict in which the morally simple positions are the 
extreme ones, and the middle is morally treacherous. Most people, in Britain 
and elsewhere, fancy themselves animal-friendly centrists. But how considerate 
of animals is considerate enough? Between complete indifference on the one 
side and radical compassion on the other there is no stable stopping point. 
That is why, although the debate happens to have broken out in the ports of 
England, it could in principle erupt anywhere. And that is why the protests, 
once started, will persist.

In their face
Animal-welfare activism is nothing new to Britain. The Royal Society for the 
Prevention of Cruelty to Animals, founded in 1824, was the world' first group 
of this kind, preceding the National Society for the Prevention of Cruelty to 
Children (which, indeed, was modelled on the RSPCA) by half a century. Today 
the RSPCA has 500,000 supporters, 200 branches in England and Wales, and 178 
affiliated groups around the world. It is only one, albeit the largest, of 
Britain's pro-animal groups, whose number probably runs to hundreds.
  The RSPCA anchors the moderate end of the animal-welfare spectrum. Although 
on paper it opposes shooting and fishing for sport, in practice it shuns 
extremism. A more radical brand of activism arose in the 1970s, catalysed by 
the ideas of Peter Singer, a philosopher, and others who questioned whether 
the line dividing animals from people was quite as sharp as had been assumed. 
The newer groups spoke of  "oppression" and "liberation" of animals, and 
contemptuosly attacked the "welfarist" approach as favouring "longer chains 
for the slaves".
  In America, where there are now perhaps 400 animal advocacy groups, the 
focus has been on laboratory animals. Britain has its share of this 
controversy, but the recent eruption concerns cruelty in farming.
  Perhaps the most significant part of what animal-rights groups do is to 
educate, lifting the level of ignorance that obscures the realities of 
farming. The public responds. Since 1984, the percentage of Britons who are 
vegetarians has more than doubled, to 4.5% (in a Gallup poll in February). 
Women are almost twice as likely as men to be vegetarian, and young women are 
most likely of all: one in eight professes vegetarianism.
  Yet factory farming has been a fact of life for decades. For that matter, so 
has been the trade in live animals. In 1963, Britain exported 655,000 live 
animals to farms and slaughterhouses on the continent, where customers will 
pay a premium for freshly slaughtered meat; those animals, like many today, 
might travel 40 hours without food, water or rest. The number of exported 
animals has risen in recent years, but neither the trade nor 
animal-welfarists' objections to it are new. Turning vegetarian, the most 
personal kind of political statement, is one thing. Getting arrested is quite 
another. What, then, has propelled bourgeois house-holders to the streets of 
Dover? In brief: video cameras and accessible targets.
  In 1990, Compassion in World Farming, a pro-animal group in the vanguard of 
the recent wave of British protests, sent people to make video footage of a 
slaughterhouse in Spain. The results were among the horrendous scenes 
described above. To be told what happens to chickens in giant poutry sheds is 
merely out-putting. To see the birds squeezed into tight battery (group) 
cages, barely able to move and all but featherless from neuronic pecking, or 
to see a goose with a funnel rammed down its gullet for the force-feeding used 
to make foie gras -- that is not soon forgotten.
  What television did for the opponents of the Vietnam war the videocassette 
recorder has done for the animal-rights movement. "Silent Scream", which 
graphically depicted a fetus as it was vacuumed from the womb, had a similarly 
strong effect. 
  Then, last year, when Dover's big ferry companies bowed to protests and 
stopped carrying  live animals, the trade was diverted to small ports, where 
roads to the docks lead right through town streets. If you stand at the kerb 
in Colne Road in Brightlingsea, the lorries come nearly close enough to touch. 
Through slats in the sides, week-old calves stare out, often bound for a 
lifetime in narrow crates that are illegal in Britain. "You can see, hear, 
smell the cruelty you're protesting against," says Philip Lymbery, of 
Compassion in World Farming.
  Mr Lymbery, like many animal-rights organisers, is a vegetarian: he consumes 
neither meat nor animal products. His T-shirt says "Stop Factory Farming". The 
people lining the streets mostly would not go so far ("I'm not a dippy 
vegetarian," says one man in Brightlingsea). Most of them want a ban on 
live-animal transport, though some want an end to factory farming generally. 
What is most striking, however, is not what they want but how. They view their 
demands not as an ordinary political goal but as a crusade, and they view the 
response of the system as a burning indictment of the system itself. All this 
for the sake of a few lambs? No: for the sake of moral consistency.

Like people, a little
Consider a human fetus. One way to view it is as a bit of tissue in a woman's 
body, no more special than a scraping of fingernail or, at most, a liver with 
a future. On that view, a woman ought to be free to dispose of the fetal 
tissue as she pleases, so long as she does no harm to others(for the fetus 
itself is not "an other"). Another way to view the fetus is as a partly 
formed human being which happens still to be enclosed in another person's 
body. In that case, the woman's right to dispose of it is virtual nil, because 
a second party is involved: one which cannot defend itself.
  For most Americans, the former view is unacceptably crass, the latter 
unacceptably intrusive. So they flail for stable ground in the middle. But the 
middle is not solid rock but treacherous scree. If a fetus is at all 
human-like, should one not give it the benefit of the doubt? Only this 
delicate question separates the befuddled centrist from the anti-abortion 
crusader. A bit of thought, a sermon or two and a few activist friends are 
more than enough to convince many middle Americans -- not the sort who believe 
that the United Nations is invalid Idaho -- that holocaust is happening in 
their midst.
  That is why abortion is, in America, the social issue from hell. Its 
politics are strikingly like those of animal rights in Britain. Hundreds of 
people, unremarkable except for their anger, blockade clinics/docks, seeking 
to stop women/calves from reaching the door/ship. The resemblance is not 
coincidental. Indeed, it is inherent.
  A calf, like a fetus, is helpless and has no man-like traits. It has 
consciousness, it can feel fear and pain: in short, it can suffer. Like a 
fetus, it is more than an object, and so should not be casually abused. So 
far, pretty well everybody agrees. If you are content to leave animals in this 
grey zone, somewhere between human and object, there is not much of a problem. 
But there is not much moral clarity, either. Those who seek to locate animals 
more precisely on the moral spectrum soon face a choice, with not much 
philosophical manoeuvring room.
  One view -- which one might recognise as traditionalist or disparage as 
speciesist-- sees animals as morally different from human beings, so that 
kicking a child and kicking a pig can never be equated without gravely 
demeaning the child and ludicrously fetishing the pig. The other view -- call 
it universalist -- sees higher animals as different from people (and from each 
other) in many ways, but fundamentally alike in that they are sentient and 
share the capacity to suffer.
  People of both views may agree that improving animal welfare is a good thing 
to do. But for whose sake? Traditionalism sees humane treatment of animals as 
good insofar as it dignifies and civilises human beings, which it does do; but 
there can be no moral "conflict" between a human and an animal, because there 
is only one party to the case. The question is always, "What is best for the 
moral and economic condition of people?" Animals in human custody are 
therefore to be regulated as a form of property in which the larger community 
takes an interest. You can do as you please with the animals you own, except 
where the process of democratic lawmaking restricts you.
  That is the current regime, more or less. It is quite coherent, and allows 
for as much (or as little) animal-welfare regulation as the public can be 
persuaded to support. But it is emotionally rickety, for it flies in the face 
of the way most people actually feel about animals (or, at least, about the 
pleasanter kinds of animals). Beholding a frightened calf confined in a tiny 
box for life, part of every humane person rebels against the notion that the 
calf has no moral standing except that which an all-human political system 
happens to give it. Surely, in some sense the calf is a party to the 
proceedings?
  Here too, however, the ground is slippery. Concede that man-calf relations 
involve two interested parties, not one, and radicalism is but a nudge away. 
If animals are in some way humanish, perhaps they could be given the benefit 
of the doubt. But how much consideration is enough?
  The activists' answer is anthropomorphic. If the animals had a voice, they 
would want to be treated as well as suits themselves, instead of merely being 
treated as well as suits people. Unless you are willing to make an arbitrary 
distinction between one level of cruelty and another, you soon arrive at the 
conclusion that humans have no business harming animals except on the most 
urgent necessity. "It boils down to as little interference from people as 
possible," says Mark Glover, of Respect for Animals, a hard-line British 
animal-rights group.

Animals and Auschwitz
Now adversaries blink at each other across a deep breach. If you believe in 
the standard liberal model, the animal-welfare policies that are in force at 
any given moment are by definition reasonable, because people have chosen them 
and it is people who count. But if you are drawn towards the universalist 
model, you will start to see any policies foreseeably in effect as oppressive, 
because they make animals suffer. Once headed in that direction, you will be 
tugged towards two types of radicalism. That is what is happening now.
  The first type of radicalism points to the plight of the animals. On the far 
fringes of the animal-rights world are a few who embrace violence. In Britain, 
three extremist groups appear to have been active in a variety of attacks, 
including the planting of about 50 explosive devices, mostly incendiary bombs. 
This year animal-rights activists are thought to have been behind a letter 
bomb and two packages booby-trapped with razor blades that were sent to 
William Waldergrave, who was then the agriculture minister. But the police put 
the number of violent extremists at little more than 200, and most other 
pro-animal activists fervently disclaim violence (though some have fewer 
scruples against vandalism). Their passion is expressed peacefully, but its 
depth should not be in doubt.
  Wendy Bragg is 52 years old and runs a small bed-and-breakfast in 
Brightlingsea, a town now festooned with signs saying "Ban the Exports" and 
"Ban the Crates". Never before had she been politically active or involved in 
animal issues. Yet she waits on the road to the docks almost every morning and 
has been arrested twice for obstructing the highway. She says: "I can't bear 
animals suffering. I can't bear anyone suffering. They've got no one to stand 
up for them except us. No one else will stop this evil trade -- and," she adds 
with emphasis, "it is an evil trade."
  Often these protesters say that animal activism has taken over their lives, 
or close to it. They say that as long as the lorries keep moving, they will 
continue campaigning.
  "Even if there was just one lorry, I'd be involved, it's such a horrific 
trade," says Heather Macwilliam, a 50-year-old who never misses a morning at 
the Dover docks.
  And if the lorries stop rolling in Britain, there is still Europe, where the 
dedicated go in busloads for demonstrations at the European Parliament. 
Comparisons to slavery, to brutality against children and to Nazi atrocities 
are often heard. Between shouts of "Scum, the lot of you!" as the calves roll 
by in Brightlingsea under police escort, Marie March, who is 56, remarks that 
she would not be surprised to see children escorted through in such lorries. 
"When it comes to animals, our species acts like Nazis," says Andrew Tyler of 
Animal Aid, a group based in Kent. He says he speaks as a Jew.
  The issue is radicalising in a second way. It begins as a protest against 
abuse of animals. But if the law permits outrages, can it claim moral 
legitimacy? And if the police protect atrocities, are they not complicit? Thus 
pro-animal soon becomes anti-government. "We've started to realise just now 
how corrupt the people in power have got," says Mrs Bragg.
  This sentiment is heard again and again: animal abuse has opened up eyes to 
the insensitivity of government. At the Dover meeting more is said against the 
government and the police (whose sometimes heavy-handed tactics have not 
helped) than against the abuse of animals. One man speaks wildly of "fascist 
state". Another complains of being treated worse than animals by "the scum in 
Westminster", and adds, to laughter and applause: "I want to know what I can 
do to cause as much trouble as I can." The local member of the European 
Parliament, a Labour Party man, Mark Watts, joins in the spirit of the 
occasion by referring to "our so-called democracy".

Get that granny
Such talk is all the more extreme in that it comes from otherwise mild people. 
At a meeting in Brightlingsea, when 60 or so people discuss the pros and cons 
of going limp when arrested, many of the listeners are grandmothers. "It's 
changed the people totally," says Maria Wilby, a leader of the Brightlingsea 
protests. "People are so much more aware of what can be done against them, and 
how little power we have."
  Of late, America's abortion controversy has gone from nasty to violent. In 
1993 David Gunn was shot to death as he crossed a clinic's car park. The next 
year, a well-known activist named Paul Hill killed another doctor and his 
assistant, and showed no remorse; he was saving babies, he explained. The 
number of arson attacks on clinics has gone up sharply.
  The British animal-rights protests have not been as bloody. But they have a 
martyr: Jill Phipps, a 31-year-old mother who in February was hit by a lorry 
carrying calves in Coventry. In Brightlingsea, police worry about a young 
woman who, they fear, is determined to dive in front of a lorry. For their 
part, protesters complain of crude and bullying policemen. All agree that if 
the police were not on hand there would be violence. The two sides are 
determined to wear each other down.
  It will be a long wait. "It's horrific, unbelievable, the cruelty that is 
done to animals," says Clare Baumberg, an organiser of protests in Dover 
(and yet another middle-aged, middle-class woman). Asked if there is a 
stopping point, she thinks before replying: "Probably not. The more you work 
in animal rights, the more cruelty you find goingon." Her story could stand 
for many others. She is 49 years old and became active on animal issues when 
her children moved away. It is the galvanisation of people like her that makes 
the animal-rights issue morally compellig and politically intractable.



Bomb, 22 March, P

Between the bomb and a hard place
Curbing the nuclear ambition of Israel, India and Pakistan is proving hard 
enough. Rolling them back will be even harder.

The threat of a Russian-American nuclear Armageddon may have lessened with the 
ending of the cold war, but fears about the spread of nuclear weapons have, if 
anything, intensified. "The bomb" remains the power-symbol of choice, coveted 
by nervous governments around the world. 
  Next month the signatories of the Nuclear Non-Proliferation Treaty (NPT), 
which has helped to impede the spread of nuclear weapons and weapons 
technology for the past 25 years, are due to decide whether to extend the 
treaty for a period yet to be fixed, and, if so, on what terms. The factors 
likely to shape their thinking are less clear-cut than they might seem. Few 
countries, nuclear or not, dispute the need to resist attempts by rackety and 
deliquent states, such as Iraq and North Korea, to put together nuclear 
arsenals. The bigger problems start with countries that have stayed outside 
the treaty, and yet have managed to build their bombs -- a category now 
comprising Israel, India and Pakistan, the three "threshold" states widely 
thought to be nuclear-capable, if not already nuclear-armed.
  These three countries, of good standing for most diplomatic purposes, have 
nonetheless thumbed their noses at the NPT, refusing to sign it or to be bound 
by its terms. Each has succeded in building up nuclear expertise behind the 
thinnest of veils, and with the tolerance of other world powers. The result 
has been an undermining of the moral force of the NPT, and encouragement to 
other governments that have come to resent the NPT's pinch. Why, critics 
wonder, should the "threshold three" qualify for special treatment? Egypt, in 
particular, has won a measure of backing from other Arab governments and from 
some African ones for its refusal to back an extension of the NPT unless 
Israel signs up.
  Many developing countries (such as Indonesia) sympathise with India's 
argument that the NPT is a discriminatory vehicle, because it divides the 
world into nuclear haves (the original nuclear five: America, Russia, Britain, 
France and China) and have-nots (the rest), and keeps the have-nots in their 
place. The West is suspected of applying double standards when it suits its 
interests -- as with diplomatic and financial incentives offered to 
"denuclearise" Belarus, Kazakhstan and Ukraine after the collapse of the 
Soviet Union, and with last year's generous deal to help North Korea develop a 
civilian nuclear programme in exchange for an end to weapons development. 
Iran, which has signed the NPT but is thought to harbour nuclear ambitions of 
its own, is trying to capitalise on such sentiments in urging developing 
countries to take a common stand against the treaty.
  The undermining of the treaty goes on by deed as much as by word. Countries 
outside the NPT have long been linked, via secret supply networks, not only to 
each other but also to treaty-benders within it. Iran is thought to have 
acquired recently the centrifuge technology it would need to enrich uranium 
for bomb-making, possibly with the commercial connivance of Pakistan. Iran 
also has close military and scientific links with North Korea. Israel and 
South Africa (the latter built but later destroyed six nuclear weapons before 
joining the NPT in 1991) are  thought to have collaborated on at least one 
nuclear test, although both deny it. Over the years all of these countries 
have had dealings with China, which signed the NPT only in 1992 and is 
notoriously lax about exporting sensitive equipment.
  For all these reasons, but also because the Middle East and South Asia are 
volatile regions where nuclear bombs might someday be used, attempts to 
strengthen the barriers to the spread of nuclear weapons need to address the 
position of these three nuclear hard-cases. But how?

The early birds got the bomb
Most countries that export nuclear equipment refuse now to supply countries 
that have a dodgy reputation or else do not apply full International Atomic 
Energy Agency (IAEA) safeguards to all their nuclear installations. For 
example, because India allows inspectors to track only part of its nuclear 
programme, it had trouble obtaining new supplies of enriched uranium for its 
Tarapur power plant -- until China, which has not adopted the new code, 
stepped in earlier this year. A series of nuclear might-have-beens, including 
Syria, Lybia and Algeria, all of whose ambitions have at times given cause to 
alarm, have been too strapped for cash to get far, or have been hampered by 
increasingly tight export controls. Even where such controls are not quite 
water-tight, they drive up the price of the materials available to painful 
levels.
  Iraq has been the exception that proves the rule: the international 
inspectors who frisked it after the Gulf war found it had come within a year 
or so of having a bomb of its own. But to do so had cost it probably at least 
$10 billion, and years of skulduggery acquiring parts and know-how on the 
international black market.
  The threshold states, and their former associate, South Africa, were able to 
put essential elements of their programmes in place when the world was less 
worried about the dangers of proliferation. Canada sold India a nuclear 
research reactor in 1956, a year before the founding of the IAEA, without even 
setting subsequent inspection as a condition of sale. America provided heavy 
water to operate the reactor. Quite a bit more foreign help later, India used 
the plutonium produced from the reactor for a supposedly peaceful nuclear test 
in 1974, and it is thought now to have enough bomb-making material squirrelled 
away for at least 30, and perhaps 60 (some say 100) weapons.
  Israel got its start in the bomb business in the heat of the 1956 Suez 
crisis, when France (which signed the NPT only in 1992) secretly agreed to 
supply it with a plutonium-producing reactor to be built at Dimona, in the 
Negev desert; Norway later provided the heavy water needed to operate it. 
France also supplied a reprocessing plant for extracting plutonium from 
Dimona's spent fuel, along with much information on the design and manufacture 
of nuclear weapons (including, possibly, data from France's first nuclear 
test). Israel is thought to have had one nuclear device assembled by the time 
of the 1967 Middle East war. By the early 1990s its stockpile of warheads was 
being conservatively estimated at between 60 to 100; information leaked in 
1986 by Mordechai Vanunu, an Israeli nuclear technician subsequently jailed, 
suggested that the total could be as high as 200.
  Pakistan's secret military programme to develop nuclear weapons got under 
way after the defeat by India in 1971; it accellerated after India's nuclear 
test. By nefarious means, Pakistan eventually obtained from firms in Western 
Europe and North America the technology and equipment to produce highly 
enriched uranium. It got a lot of catch-up help later -- including, it is 
thought, the complete design for a bomb -- from China, its pal. Pakistan 
started producing weapons-grade uranium at its Kahuta enrichment plant in 
1986, and is thought to have the makings, including fabricated components, for 
at least 15 bombs.
  Looking further down the list, Iran is by most reckoning some 5-15 years 
away from being able to build a nuclear weapon. But it is provoking concern 
nonetheless, because it can draw on skills and equipment accumulated by the 
Shah in the 1970s, as well as on some newer "research" facilities provided by 
China. For the moment Iran is short of cash; but if its finances were to 
improve, and if it were to be helped along by Pakistan or China, it could 
advance quickly in the nuclear lists.
  Of the other countries that have approached the nuclear threshold, Iraq is 
the only one to have been denuclearised by force. In the 1970s America 
successfully pressed South Korea and Taiwan to stop their bomb programmes. 
Rival efforts in Argentina and Brazil were abandoned when democratic 
governments came to power there in the 1980s; the two countries have since set 
up a mutual inspection regime in co-operation with the IAEA, and have ratified 
the Tlatelolco treaty, which pledges signatories to keep Latin America free of 
nuclear weapons. In 1993 South Africa announced that it had voluntarily 
dismantled six bombs (which it had long denied having) prior to joining the 
NPT in 1992. The deal reached late last year between North Korea and America 
will give North Korea more modern nuclear reactors that are easier to monitor, 
and will allow -- fingers crossed -- checks, eventually, on North Korea's past 
plutonium production.
  But if South Africa could decide to abandon its nuclear deterrent, say 
advocates of a strong NPT, why should the same not be expected of Israel? And 
if Argentina and Brazil could step back from the nuclear threshold, why not 
India and Pakistan? 

Trouble with the neighbours
Israel's stock answer to such questions is that it is a tiny state, surrounded 
by populous enemies sworn until recently to expunge it from the map, and 
outnumbered (if not outclassed) in every sort of weapon bar one. That 
argument has persuaded America, which, to Egypt's irritation, has long refused 
to take Israel to task about its bombs.
  Israel has never admitted to having nuclear weapons, but has phrased its 
statements on the subject in such a way as to leave its neighbours in little 
doubt as to the reality. It has also been willing to take pre-emptive action 
against potential nuclear rivals (notably an air-raid on Iraq's Osiraq 
research reactor in 1987) to keep its all-important technological lead.
  Yet the political realities of Israel's position are changing in ways that 
may force new doctrines upon it. The Israeli-Palestinian agreement of 1993 
has led to a peace agreement with Jordan to add to the much earlier agreement 
with Egypt. Pacts with Syria and Lebanon should be next. This has fuelled a 
debate within Israel about whether the country's future security is best 
ensured unilaterally, as in the past, or in a new regional framework. No such 
framework yet exists. But it is notable that Egypt and Israel alike, and other 
states in the region, including Iran, have long supported resolutions at the 
UN to create a nuclear-weapon-free zone in the Middle East. Egypt has seen the 
declaring of such a zone (with Israel's signing of the NPT as a first step) as 
a way of building Arab confidence in the peace process. Where Israel and Egypt 
part company, however, is that Israel, using the case of Iraq as safeguards, 
has rejected the NPT in favour of a tougher regional regime with mutual 
inevitable problems of scope and timing.
  Israel would have little confidence in any sort of security regime that did 
not encompass Iraq, Iran and Libya, all still its sworn enemies. It wants all 
weapons of mass destruction, including chemical and biological weapons, to be 
covered. Egypt recently stepped up the pressure by securing Arab League 
backing for a draft treaty on precisely such a zone.
  Then there is timing. Palestinians involved in the peace talks acknowledge 
privately that it is in part the possession of nuclear weapons that has given 
Israel the confidence to pursue peace with -- and make territorial concessions 
to -- its neighbours. As Israel contemplates such a deal with Syria, it seems 
unlikely that it will be ready to give up its nuclear shield simultaneously. 
Indeed, Israel argues that the nuclear component of any security regime in the 
Middle East can be addressed only after a comprehensive peace is established 
and is shown to endure. Prior to that it wants less contentious 
confidence-building measures to be given time to take effect.
  Thus, for the time being, the global NPT timetable and the regional, Middle 
East one are badly out of sync. And, realistically, Egypt can scarcely expect 
Israel to submit to full-scale NPT-like inspections, the effect of which would 
be close to that of nuclear disarmament, until the concerns that led Israel to 
build the bomb have diminished and genuine security is seen to be achievable 
by other means. But nor can Israel afford to block discussion of nuclear 
issues until every brick and cornice of a peace process has been cemented into 
place. It is not just woolly-minded peaceniks who believe that Israel cannot 
go it alone indefinitely.
  As the necessary technology becomes more accessible, Israel's possession of 
the bomb makes it that bit harder to block the ambitions of Iraq and Iran. 
Israel's raid on Iraq's nuclear reactor was a robust rebuff to a rogue state; 
but the real lesson handed down by Iraq may be that a regime determined to get 
its hands on nuclear-weapons technology will find ways to do so. Israel's 
barely veiled threats to Iran to desist from bomb-building may serve only to 
harden Iran's determination. Israel must presume that some day someone nearby 
will get a bomb, augmenting the already terrifying arsenal of chemical and 
biological weapons in the region. Indeed, the Iraqi missiles fired at Israel 
during the Gulf war, though zipped with conventional explosives, carried the 
intimations of a new vulnerability. One way or another, the deterrence-gap by 
which Israel has set so much store is closing.
  At the same time, Israel is under pressure from America and other countries 
to make some gesture to Egypt that will help ensure the indefinite extension 
of the NPT. Egypt has turned down the offer of a tour of one of Israel's 
nuclear reactors that is already under safeguards. Another idea is for Israel 
to join a (yet to be negotiated) international agreement to cut off the 
production of fissile materials. Israel has said it is "not opposed" to the 
idea, so long as any such deal capped only future production of plutonium and 
highly enriched uranium, leaving existing stocks untouched. That sort of 
formula would probably not satisfy Egypt, unless coupled with the early 
shut-down and inspection of the Dimona reactor.

Fissionable instability
The idea of a fissile materials cut-off agreement is one of the few ideas for 
nuclear constraint that has the support of the other two threshold states, 
India and Pakistan. Unlike Israel, neither country has deployed nuclear 
weapons, though both are capable of doing so at short notice.
  This can-but-haven't mode of deterrence is often credited with bringing 
stability to the military rivalry between India and Pakistan. Wrongly so. 
Veiled nuclear threats have been issued between the two countries on more than 
one occasion in the late 1980s and early 1990s, with America intervening to 
talk the sides back from the brink. Now both countries are preparing to deploy 
missiles: in India the home-made Prithvi, in Pakistan's case M-11 missiles 
which it denies having brought from China, but which are reportedly sitting in 
crates at an airfield. Capable of carrying nuclear warheads, such missiles 
would add hair-trigger instability to a future crisis. America has therefore 
been pressing both sides not to deploy the new missiles -- but with no sign of 
success yet.
  As to rolling back nuclear programmes, Pakistan has long said it would be 
prepared to do so, and to sign the NPT, if India did likewise. But there is 
little prospect of this commitment being tested. India has ruled out signing 
so long as other countries are allowed to keep nuclear weapons. It would 
accept only those measures, such as a comprehensive test-ban (CTB) and a 
fissile materials cut-off convention, that accorded equal rights to all and 
imposed equal obligations; and such instruments would constrain development of 
new weapons only.
  If India will not sign the NPT, might it be prepared to accept the sort of 
deal struck by Argentina and Brazil -- a joint, and strictly verifiable, step 
back from the nuclear threshold? Unlikely. In India and Pakistan alike there 
is strong popular support for the sheer technological prowess represented by 
the nuclear programmes.
  Moreover, India's original motives in building the bomb were to achieve 
regional and international status and to counter China's bomb. Although India 
and Pakistan have fought three wars since 1947, and though Pakistan has moved 
up India's nuclear worry-list since it got the bomb, geography dictates that 
Pakistan is the one that finds itself at a military disadvantage. But might 
political change in the wider region push India to rethink its nuclear policy?
  India's relations with China have improved considerably (witness the uranium 
deal between the two countries). So have India's relations with Pakistan's old 
ally, America, and Pakistan's relations with India's old ally, Russia. But 
despite a recognition in some official circles that nuclear weapons, taking in 
the cost of developing new ones, may be becoming more of a burden than the 
benefit they once seemed, India has remained hostile to both bilateral and 
regional talks on the issue.
  That has not stopped others trying. In 1991 Pakistan won Russia's support at 
the UN (over India's objection) for its proposal for a nuclear-weapons-free 
zone in South Asia. America has proposed five-nation talks, to include itself, 
India, Pakistan, China and Russia (though China says, unhelpfully, that its 
own nuclear arsenal cannot be discussed round this or any other table). India 
has stayed aloof, in part out of suspicion of everyone else's motives, but 
also because, for all the recent changes, it still banks on the influence it 
feels the bomb brings it. To those who despair of curbing nuclear 
developments in South Asia by any other means, that leaves only one avenue to 
explore: whether India might renounce its nuclear option if that were part of 
the price for alloting it a permanent seat on the UN Security Council.

Out of the closet
Short of a miracle, there seems little prospect that any of the "threshold 
three" would be prepared to negotiate away their nuclear option soon. Hence 
western attempts to get them to sign up to future global constraints, such as 
the CTB and a fissile materials cut-off convention. But if these three 
countries are determined to keep their nuclear options open, why not simply 
declare themselves nuclear powers and have done with it?
It may come to that. Some people argue that greater openess about nuclear 
capability would reduce the scope for miscalculation between adversaries and 
allow regional rivals to embark on potentially useful discussion of nuclear 
doctrine, deployment and crisis management. Yet "coming out" of the nuclear 
closet would exact a high prize from any of the threshold states. They would 
expose themselves automatically to a variety of counter-measures from 
countries, primarily America, that tie aid and trade closely to nuclear 
status, and which would no longer be able to turn a blind eye. It is hard, 
too, to see how the NPT could be adapted to fit the new situation, since it 
cannot be easily amended to account for new nuclear powers unless all 
members agree (which could be unlikely in the case of Israel, India and 
Pakistan).
  Ironically, none of the threshold states wants to see the NPT collapse, for 
fear that the effect would be to uncorck nuclear ambitions among more of its 
neighbours. Unfortunately, that last desire does not yet seem to supply any of 
the three with a strong enough motive to do the difficult things necesssary to 
prop the treaty up.


Nature

Science policy, J. de la Mothe, 16 March, C, Gen

Is science policy in the doldrums?
Science policy is in the doldrums, having run out of intellectual steam and 
strong political commitment. It needs shaking up if it is to support and guide
science and technology in a global, knowledge-based economy.

Policy-makers throughout the Organization for Economic Cooperation and 
Development (OECD) are faced with a serious dilemma. On the one hand, science,
technology and innovation are clearly in the ascendancy, broadly seen as 
essential to the comparative advantage of nations and becoming mainstream 
activities generously supported by governments -- to the tune of nearly $150 
billion a year.  On the other hand, focused science and technology policy -- 
the nest of government programmes and policies designed to support research, 
development and innovation -- seems to be in the doldrums, having run out of 
intellectual steam and strong political commitment. Governments are struggling 
to maximize the return to society from investing in research but have not 
demonstrated an understanding of how best to work with a changing scientific 
community. How did this situation arise and how might it be resolved?
  One reason is that while science and technology have succeeded in moving 
from the periphery to the centre of government and policy, these creative 
enterprises have exposed themselves to an array of evaluative criteria not
germane to science itself. Political economic, distributive, military and 
geopolitical considerations have all become increasingly important in the 
formulation of science policy without regard to the real needs of science. It 
is now a question of "science policy for what?" rather than "science policy for
science's sake". Scientists need to understand this. Whether or not the United
States decides to participate in CERN (European Laboratory for Particle 
Physics), Russia in the Space Station, Japan in the International Thermonuclear
Experimental Reactor or any of the G-7 nations in national information 
infrastructure programmes (such as information highways) often has more to do 
with non-scientific strategic considerations than research priorities. The same
is true for other countries. Moreover, in recent years, deficits and debts have
greatly reduced the freedom that politicians have in public spending generally,
with government budgets -- determining social, military, industrial as well as
science spending -- now being capped, consolidated and reduced. As a result,
political rhetoric designed to elicit funds through international cooperation
in science and technology is increasing.
  This erosion of funding should come as no surprise. The scientific community
-- whose members have come to view research grants as an entitlement rather
than a privilege -- has for decades promised the public and politicians far
more than it could deliver. The "endless frontier" of science has not managed 
to translate itself into an "endless solution". Science has therefore become a
target for mainstream spending cuts along with every other government 
portfolio. The effect on science policy is that control on spending in science 
and technology is passing from the hands of research administrators to
bureaucrats in departments of finance, commerce and industry. With it comes the
trend of setting priorities and developing so-called "performance measures" for
innovative work.
  A second reason for politicians' ennui with science policy is that the rapid
growth of science budgets from the late 1950s until the mid-1980s was an 
anomaly, not the norm. A whole generation of young scientists trained in the
aftermath of the Second World War expected this upward trend to continue. Yet
expansion of science was as much a result of the postwar baby boom and the 
chilled nerves of post-Sputnik policy hysteria as it was of any power inherent
in human technical prowess. Indeed it was no coincidence that public science 
policy arrived on the scene (in the guise of technology assessment, technology 
forecasting and so on) roughly when the public consequences of the fruits of
research were being felt or perceived. It was a growth curve that just had to
flatten. Otherwise, as the sociologist Derek de Solla Price noted in 1969, if
science budgets and science enrolments did indeed keep going up, by 2001
"every man, woman, child and dog would be a scientist". Or, as D. Allan 
Bromely, a former US presidential science adviser, recently argued, the past 40
years has been an unnatural blip, and scientists will have to get used to that.
Science funding has entered a protracted steady state.
  A third reason is that science policy has stopped being about science and is
now principally about economic growth, job creation and industrial development.
The focus on the development of conditions for the knowledge-based economy --
which most OECD governments are desperate to develop (though not to define) --
has brought a flurry of attention by (mostly) governments to the content and 
orientation of trade, investment, industrial and research activities. Thus, 
interest in a country's technological balance of payments, GERD/GDP ratio 
(gross expenditure on research and development as a percentage of gross 
domestic product), foreign direct investment, tax credits for research, the 
creation of clusters of small high-tech companies and so on has mesmerized
policy-makers into a narrow quest for the economic indicator that will reveal
their techno-economic future. This has led to a plethora of new programmes in
which university researchers work with companies, participate in networks and 
collaborate with researchers in other disciplines. And to get funding, 
researchers concoct an array of economic benefits that might flow from their
efforts. These range from new jobs for both researchers and nonresearchers to
the shoring-up of failing companies and industries, as well as the creation of
new businesses.
  Implicit in much of this is a confusion within bureaucracies about the 
extent to which governments should intervene in the workings of the economy.
One type of policy economist sees economic growth as a function of 
company-level activities in which the government has little or no role except
to provide a stable fiscal environment. In this world, grants for research 
distort the market and artificially boost the salaries of researchers. What is
more, science and technology policy -- however defined -- means to these
economists a form of "picking winners" (something that most people agree
governments are ill-equipped to do and which is anathema to the private-sector
free-trade mantra). In this model, governments are often seen as surrogates for
industrial research and development. A second type of policy economist,
however, agrees on the importance of companies but also sees cooperation
between government and business as a key to successful competition in a global
economy. In this particular world, science policy is not just about grants but 
is also about "strategic trade", "strategic technologies" and attracting 
investment into research and development. But for this type of policy view to 
work, a government must know where its science-based strategic interests lie. 
Many countries, however, have no clear idea about what these are. And 
scientists haven't helped. At one level this might be easier for the United 
States, the Netherlands, Canada or Australia. At another level, the whole 
character of "strategic interests" is being utterly re-defined in the context
of the post-Cold War world. Scientists must help in this definition.
  A fourth reason is that some governments do not know how to delineate clearly
their goals for science and technology or how to orchestrate their activities 
to achieve the most effective use of public money. They may also lack the 
political will to reallocate public funds across the science portfolios. 
Although priority-setting is de rigueur, it is rarely accompanied by action
plans and recommendations that can be implemented (eloquent White Papers and
other policy statements notwithstanding). Given that billions of research 
dollars are spent annually by each of the OECD governments, it is hardly
surprising that research administrators act less in the public interest than as
members of a tribe, protecting their own budgets and programme mandates much as
they would if they were protecting territory.
  Science policy is not just about funding and delivering the goods, and
science-policy ideologues should not be involved in managing science budgets.
Effective science policy is possible only when governments organize themselves
in ways that allow them to receive sound advice on their science and technology
activities. Every country has at some time benefited from (and been damaged by)
charismatic advisers who gain access to the corridors of power by virtue of 
their political contacts and experience. Such people will always be available
and consulted. But they are not sufficient. Questions need to be asked about
whether a government can really afford to be without an independent chief
science adviser, an office of science and technology policy, an office of 
technology assessment, an effective national academy of sorts, or an 
independent science-policy research unit. At the moment, only a few OECD
countries are puzzling over these questions. Pluralistic structures for policy 
advice are not only more democratic; they can also help to shape effective 
policy. Governments should be asking whether it would be worth their while
training their own interdisciplinary teams of people competent in the 
evaluation of research. And science communities must ask themselves whether it 
is good enough to act or to be perceived acting as a mere lobby group to
government for physics, biology or engineering, or for a space station or an
information highway -- rather than in the broad public interest.
  Science and technology are serious activities. They must not be diminished by
accountants' pens or by politicians seeking unrealistic goals. Nor should they
be distorted by scientists' lurid claims or promises. Science and technology 
are important to advanced economies, but they clearly cannot replace the
entrepreneur, the shrewd financier or the crafty mechanic. Science and 
technology are about creativity and insight, so all they can do is inspire and
help to provide the tools of capitalist development. They cannot be held
responsible for next year's products or the next quarter's bottom-line. Nor can
the institutions of research be expected to conduct missions for which they
were never designed.
  If science policy is to emerge from its current doldrums, it must focus on
whet it does best. In the 1950s and 1960s, science policy helped to build
impressive science infrastructures throughout the OECD. In the 1970s and 1980s,
science policy helped democracies to come to grips with the public impact of 
technology and assisted in the development of national competitive advantages.
Today, science should not fall prey to industrial of trade policy theorists.
Instead, it should extend its responsibilities to serving the global common
ground and solving social ills, while maintaining its obligation to the
advancement of knowledge.
  The practitioners of science and the managers of science policy have some
major challenges before them. First, they must learn to develop a stronger and
clearer partnership with government in developing the future direction of 
research and science policy. This partnership must recognize the changing 
nature of the scientific enterprise itself, where innovation and scientific 
research are increasingly interrelated. Second, the research enterprise (and 
there is more than one) must become more sophisticated in delivering its 
messages, priorities and needs to the public policy process. Scientific
practitioners can indeed provide solid advice in the shaping of national policy
for research, but this must be done in a way that is realistic to the task at
hand. The need for performance measures, indicators and state of research 
policy/foresight reports will increase as governments seek to demonstrate 
accountability and transparency to the public. The scientific community can
help in these efforts. Finally, the public dimension of the interface between
science and government will be tested with new pressures, as we are seeing with
issues related to ozone depletion, persistent organic pollutants, new 
reproductive technologies and research problems associated with ageing. The
ability to respond to new demands from more knowledge-thirsty societies and
truly global issues will strain the bond between national politicians and 
international scientists. Good science policy is needed as never before. It
must be shaken out of its doldrums.



Genetic information, R. J. Pokorski, 6 July, C, Bio

Genetic information and life insurance
Genetic testing will become the future standard of medical care. Life insurers
will also need access to genetic information if the insurance industry is to
survive intact and if cover is to remain affordable.

Life insurers are beginning to realize that genetic information is relevant to
their assessments of morality risk. If people use genetic tests to decide when
to buy insurance and how much to buy, then insurers will also want to know the 
results of these tests.
  But genetic tests may be difficult to interpret because of complex 
interactions between genes and the environment and variations in the age at 
onset and severity of disease. Doctors fear that patients will postpone genetic
testing and thus miss the benefits of disease prevention and early diagnosis. 
There are also worries that insurers might use genetic information to invade 
privacy, discriminate against people with genetic diseases or deny insurance 
cover if genetic tests give unfavourable results.
  My intention here is to show that genetics will become such an integral part 
of medical care that it will be impossible to draw meaningful distinctions 
between genetic and nongenetic information. Further, I argue that denying life
insurers access to genetic information could lead to a fundamental 
restructuring of the life insurance industry and a drastic reduction in the
number of people who could afford such cover.

Genetic tests
The US Institute of Medicine's Committee on Assessing Genetic Risks predicts
that "multiplex genetic testing" will soon become routine clinical practice,
with many genetic tests done on a sample of blood or other tissue. The
committee foresees a time when the public will be offered genetic screening by
walk-in testing and mail-order and home-test kits.
  The ability to monitor genetic evolution from health to disease will make 
obsolete the traditional pathological distinctions between metaplasia,
dysplasia and cancer. Molecular biological tests will replace standard 
histological techniques. To a life insurance company, it makes no difference if
a person has a pathological diagnosis of moderate-to-severe dysplasia or its
genetic counterpart. In either case, there is a high risk of cancer developing 
in the near future.
  The PCR (polymerase chain  reaction) test has been used to detect exfoliated
neoplastic cells in the stool of patients with colorectal neoplasms, in the
sputum of patients with lung and head-and-neck cancer, in the blood, bile and
stool of patients with pancreatic cancer and in the urine of patients with
bladder cancer. It will soon be common medical practice to use this DNA
technology to diagnose genetic and nongenetic disorders in people who have
developed a disease but are without symptoms. Distinctions between genetic and
nongenetic information become even more blurred for tests that measure gene
products, including tumour makers such as prostate-specific antigen, and many
common blood chemistry and haematological tests.
  It is worth emphasizing here the distinction between predisposition and
presymptomatic. If a genetic test identifies a predisposition to a disease, the
disorder may or may not occur. By contrast a genetic test that detects a
presymptomatic disorder identifies a condition that is already present but
whose symptoms have not yet developed.
  Many new forms of therapy will be based on genetic principles. As Henry
Miller of the Institute for International Studies at Stanford University in
California writes: "Somatic-cell human gene therapy applied to genetic defects,
cancer, and cardiovascular disease may approximate in the first half of the
next century what antibiotics have been in the second half of this century.
Genetic testing will also be important in predicting the outcome of disease.
For example, allelic loss from the long arm of chromosome 18 in patients with
TNM stage II or III colorectal carcinoma has remarkable prognostic value.
  A list of common diseases with a genetic component now includes virtually 
every condition encountered in an average medical practice. Information about
family history, physical examinations and past treatment may also be seen as
genetic. Indeed, a report published in 1993 by the US National Center for Human
Genome Research states that "for policy purposes, it will become increasingly
difficult to distinguish genetic from nongenetic diseases, and genetic 
information from nongenetic information" and goes on to observe that 
"recognizing that our genes affect many common diseases not previously thought
of as genetic will transform the scope and meaning of terms such as genetic
information, genetic test, asymptomatic condition, presymptomatic condition,
and genetic predisposition to disease". Can insurers really be asked to ignore
all this information because it is genetic in nature?
A fundamental principle of private life insurance is equity. Policy-holders
with the same or similar risk of death are charged the same premium. By
contrast, public life-insurance schemes operate on the principle of equality:
everyone at given level of income pays the same amount, subject to laws
governing maximum age for contributions.

Risk classification
When people apply for life insurance, they are classified according to their
expected mortality. The primary basis of risk classification is age. Yet in 
each age group the probability of death is greater for some people than others.
Because of broad variation in life expectancy, all individuals in an age group
cannot be offered private insurance on the same terms, so insurers use 
different mortality classifications. Most applicants worldwide are offered 
insurance at standard (average) rates, 2 to 3 per cent are charged a higher 
premium and 1 to 2 per cent are refused cover. (Despite the introduction of
innumerable screening, diagnostic and therapeutic technologies over the past
century, the percentage of people who have been able to obtain life insurance
has in fact remained stable or increased.)
  A private insurance system must retain the ability to identify the risks it 
is asked to insure (genetic or nongenetic), classify them into groups with
similar expectations of loss and charge a price that reflects the level of risk
posed by the group. This conclusion is supported by theories of private life 
insurance that have evolved over the past century, historical experience of
assessment societies that collapsed because of failure to coordinate risk and
premium, and basic economic principles.
  Life insurance companies use both loss analysis and risk analysis when
calculating premiums and determining the type of information needed to classify
risks. Loss analysis looks at the past, and risk analysis looks at the present 
and into the future. Because the risk portfolios of all insurers already 
contain many policy-holders with genetic diseases, insurers would not be
concerned if future purchasing decisions were based on previous buying
practices, because these claims would be anticipated by loss analysis. This
will not be the case once genetic testing becomes common medical practice, any
more than it was with past medical advances that provided patients and 
insurance applicants with better information about their health and expected 
longevity. People will use genetic information to guide their insurance
purchases. Life insurance companies will need access to the same information to
classify the risks they are asked to accept in order to coordinate risk and 
premium.
  Suggesting that genetic information should not be used to classify risks is
tantamount to advocating a fundamental restructuring of the life insurance
industry. The economic scope of such an undertaking would be immense.
Large-scale cross-subsidies would be required between healthy and unhealthy
policy-holders. Premiums would become intolerably high for most people. To
retain some semblance of today's private life-insurance mechanism, mandatory
participation by all consumers would be required and the government would need
to subsidize life insurance purchases of those unable to afford higher
premiums.
  Consumers would respond to a price increase in several ways. Some will buy
less life insurance than if prices had remained the same, a reflection of the
fact that insurance is a normal good, in an economic sense. Others would decide
to do without any protection because it would have become too expensive.
Lower-income families would probably be the most adversely affected: life
insurance is viewed as one of the few available investment options for
low-asset households. Some people might buy the same amount of life insurance
but have less money for mortgage payments, financial investments or insurance 
for medical expenses, disability income or long-term care. And people in the
group with the higher mortality risk would probably buy much more life
insurance than before. They have a remarkable economic incentive: a high claim
expectation and low premium rate.
  It would be impossible to predict the overall effect. But it seems safe to
say that the percentage of people who could buy life insurance would not
surpass today's high levels; lower-income applicants would be
disproportionately affected; the total amount of life insurance purchased would
decrease; and policy-holders would have less money to spend on other things, as
all economic costs are eventually paid for by individuals.
  Life insurance is inherently a "discriminatory" product because premiums are
based on differences in expected mortality rates. The differences are not
intended to jeopardize basic rights but are necessary for actuaries to
determine how much premium to collect to pay for future claims.
  There is consensus in most countries that life insurers should not use race,
ethnic background or religious preference in risk classification. Insurers have
fully supported this position. But genetic information is different. Most
genetic defects that cause disease in later life are probably not inherited but
are acquired as somatic mutations (mutations acquired later in life through
exposure to environmental carcinogens or errors in DNA replication or repair).
There would be nothing unique about charging a higher insurance premium if
cancer had developed because of genetic injury caused by cigarette smoke,
ultraviolet light or some other environmental toxin. The same could be said for
presymptomatic cancer tests of urine, stool or blood specimens, as most
cancer-causing mutations are somatic.
  Further, in the average patient (or insurance applicant) it will usually be
impossible to determine if genetics or the environment dominated the disease
process. There would also be unresolvable dilemmas in risk classification if
genetic information were treated similarly to race, ethnic background or
religious preference. If genetic tests were used to monitor the development of
cancer, would applicants be treated differently if the cancer-causing gene was
inherited rather than the result of a somatic mutation? When genetic factors
are identified that correlate with increased longevity or a lower risk of
disease, will applicants with these favourable traits be told that this
information cannot be used by insurers? Would a 30-year-old man with a life
expectancy of only 10 years be treated differently if genetic rather
environmental factors were responsible?
  Fault or control is not a concern of life insurers, only relative mortality
risk. For example, many insurance companies use cardiovascular risk factors to
estimate the chance of premature coronary heart disease. If there were several
unfavourable risk factors, the company's medical director would never consider
whether the applicant was at fault or lacked control because of failure to
exercise, maintain a normal weight or follow a healthy diet.
  Practical distinctions between fault and control will use much of their
meaning as more genes are discovered. Genes can influence cardiovascular risk
factors such as obesity, hypertension, hyperlipidaemia, diabetes mellitus and
family history. Because nicotine is clearly addictive, it may be only a matter
of time before a genetic predisposition to the additive effects of tobacco is
discovered. Genes even play a role in alcohol and drug addition, two diseases
often cited as examples of fault or lack of control. Insurers could not use
fault or control in risk classification, nor could they restrict underwriting
to situations where the applicant was thought to be at fault or in control.
Most diseases are not a person's fault or under one's control. Exempting
genetic diseases or information from risk classification would be equivalent to
saying that applicants with all other disorders were responsible for their illness.
  Genetic tests will be used to diagnose disease that have already developed;
most of these disorders (cancer and many chronic disorders for example) are not
traditionally considered to be genetic in nature. In this instance, the tests
can be viewed dispassionately as the latest in a series of new medical
technologies intended to improve patient care. Concern about the use of genetic
tests by life insurers is mostly irrelevant as the disorders -- genetic or
nongenetic, overt or presymptomatic -- are already present.
  Other genetic tests screen for diseases that might develop in the future.
These would be used by life insurers in a similar way to tests already used in
the underwriting process. For example, individuals with normal or moderately
increased serum cholesterol levels are offered insurance at average premium
rates; some applicants with significantly increased levels are charged an extra
premium; and a small percentage are refused cover because of markedly increased
levels. Extra premiums are required only rarely even though abnormal
cholesterol tests are common: underwriting is not based simply on an isolated
cholesterol value but also on factors such as age, type of insurance and other
cardiovascular risk factors.
  Life insurers are genuinely interested in finding ways to use genetic
information without jeopardizing the ability to obtain insurance. The risk
classification system is not perfect but it does allow the vast majority of
applicants to buy cover.



Future for research, J. Maddox, 28 September, C, Gen

What future for research in Europe?
A conference this week in Paris may help to define a strategy for the future of
research in Europe, not simply within the confines of the present European
Union but on a wider canvas.

No scientist who works in Europe can forget that, in the mere five centuries of
modern science, Europe was almost the sole source of the world's innovation for
well over 400 years. A recitation of the names of past heroes beginning with
Copernicus inevitably excites nostalgia, but also envy. What marvellous
prosperity sprang from the discoveries associated with the names of Volta,
Carnot, Faraday, Weber, Pasteur, Hertz, Marconi, Haber and the like! But sadly,
Europe's long period of preeminence in science (which term includes
"technology" in what follows) is exclusively concentrated in the first long
stretch of the five centuries of modern science. Beginning in the 1930's, but
decisively after the Second World War, the United States and, latterly, Japan
have become the powerhouses of innovation. What is to be done?
  Taken at its face value, that can be read as a call to chauvinism. Yet if the
social function of science is to improve the health, enlarge the wealth and
deepen the wisdom of people at large, may it not be sufficient that there
should be innovation and relatively free access to it through mechanisms such
as the international patent laws and the arrangements of the World Trade
Organization (the successor of the General Agreement on Tariffs and Trade,
better known as GATT)?
  Famously, neither technological chauvinism nor contentment that others will
be sufficient sources of innovation, will meet Europe's need. Recreating
Europe's previous dominance in innovation, intellectual and practical, is an
impossible dream, but the profits of European companies and thus the prosperity
of European people increasingly depend on the ability of European companies to
sell their products on global markets. That is the bread-and-butter reason for
worrying about the condition of research in Europe. There is another group of
reasons: the inheritors of the Copernican revolution are the able researchers
on the eastern seaboard of the Atlantic whose career-advancement would seem
best assured by the purchase of a one-way ticket to North America. Although
that calculation now seems much abated, the frustration of these people needs
to be taken account of.
There is of course, more to Europe than to the European Union (EU), although
that state of affairs may not persist for long, given the union's ambition to
extend its membership to the east. There is also more, much more, to European
research than that supported by the research budget of the European Commission,
even though the commission's influence is steadily growing, partly because much
of what it spends requires matching funds by the recipients. But this is
evidently a transitional period in the evolution of research in Europe. That is
one good reason why this may also be a time when the future pattern may be
influenced in important ways by decisions constructively taken now.
These are some of the reasons why Nature is one of the sponsors of a conference
being held this week in Paris with the title "Research policies for Europe's
future". The other sponsors are the Science Policy Research Unit at the
University of Sussex, whose director is Professor Michael Gibbons, and the
Paris-based Observatoire des Science et Technologie whose director, M. Pierre
Papon, was director of the Centre de la Recherche Scientifique (CNRS) in the
early 1980s. Most of the participants are people with experience of both
research and its administration. The conference will be supported financially
by the European Commission, and will be addressed tomorrow afternoon by
Mme Edith Cresson, the research commissioner in Brussels.

Collaboration
Anecdotal evidence abounds that the past few years have seen a resurgence of
successful innovation in Europe. Perhaps the most persuasive evidence is the
frequency with which those who trudge around the continent encounter people who
are enthralled with the research on which they are engaged and confident that
their projects will yield results that command international respect. Why has
this come about?
  Collaborative institutions deserve much of the credit. CERN (the European
Laboratory for Particle Physics) at Geneva is the conspicuous example of
success, not merely for being the place at which the intermediate vector bosons
were discovered but because the Large Hadron Collider to be built in the tunnel
that accommodated LEP (the Large Electron-Positron collider) is the surviving
accelerator project most likely to discover the Higgs boson and, more
important, to throw light on the question whether there is "new physics" beyond
the "standard model" of fundamental particles. But that, of course, is only
part of the story; CERN's influence on the physics community in Europe has been
profoundly stimulating, both in providing experimentalists with hospitality for
their projects and in the development of technique. It is not irrelevant that
CERN also gave the world the World-Wide Web, the most accessible part of the
global superhighway as it is at present.
  In spite of recurring arguments about the large proportion of the budget of
the European Space Agency (ESA) spent on service and technical development
rather than research projects in the simple sense, the excellence of the
science projects carried through has similarly been stimulating for the
research community. The mission to Halley's comet in 1986 broke new ground.
Hipparchus, the astrometry satellite, is generally applauded as a neat idea.
ESA's stake in the Hubble Space Telescope is insufficiently recognized,
especially outside Europe. While the ingenuity of the space research community
everywhere is a perpetual marvel, ESA seems to have weathered the hazards of
the bureaucracy of collaboration after a slow beginning.
  The European Southern Observatory has similarly enabled European astronomers
to walk taller than would otherwise be the case, with access to really large
telescopes. Nature has a particularly soft spot for the comparator station for
comparing very-long-baseline interferometry (VLBI) records established at
Dwingeloo in the Netherlands with partial funding from the European Commission.
  Yet the biggest surprise of the past few years has been the success of formal
collaborative projects in the life sciences. The European Molecular Biology
Laboratory (EMBL) at Heidelberg in Germany has become an outstanding centre of
excellence in just two decades; its people exude confidence and
cosmopolitanism.
  Still more remarkable, though, is the success of the projects (supported by
the European Commission) to determine the nucleotide sequence of model
organisms such as yeast and Arabidopsis. The yeast chromosomes are tumbling out
as quickly as they can be published; from Barcelona to Manchester, one
encounters people contributing to these projects. In retrospect, there could
hardly have been a more effective technique for ensuring the spread of novel
techniques to a host of laboratories. The surprise is that collaboration has
not been the enemy of speed.

Resolve
Another important part of the impetus for the enhanced confidence of the
European research community has been the determination of some national
governments to enhance the standing of national research enterprises. Early in
the 1980s, the (then) West German government was disconsolate that it had won
so little benefit over the years from its generous investment in research, but
then Klaus Von Klitzing won a Nobel prize (for his work on the quantized Hall
effect) and dawn appeared to break.
  Within the EU, both France and Spain have similarly enlivened their domestic
research communities by the deliberately generous spending of funds on
research. Indeed, the first Mitterrand government in France took office in 1980
in a cloud of rhetoric comparable with that of Mr Harold Wilson in advance of
his first election as British prime minister in 1964; the difference is that
Mitterrand's ministers were able to turn the rhetoric into reality, while
Wilson's were defeated by the budget problems that quickly blew them off
course.
  The benefits of the Mitterrand upheaval are still apparent in the steady
stream of contribution by French researchers to international journals of
distinction as well as by the rapid increase over the past decade in research
spending by French industry, which grew by 61 per cent in real terms between
1981 and 1992 to a total of ECU10.54 billion (at 1987 prices). On a much
smaller scale (by a factor of more than 10), spending in Spanish industry
increased during the same period more than threefold, to a total of ECU1.41
billion, comparable with industrial research and development (R&D) spending in
the Netherlands. For what it is worth, research spending has increased even
faster in Italy than in France and Spain. In real terms (1987 prices), total
spending on research and development grew from ECU9.01 billion in 1982.
  Money is not an unambiguous determinant of quality in research, but it will
be surprising if these examples fail eventually to influence other members of
the EU. Indeed, the past decade has seen spending grow fastest in the countries
at the outset spending least, with the result that total R&D spending increased
by 58 per cent between 1981 and 1983. Interestingly, total R&D spending by
members of the European Free Trade Association, EFTA, (all except Switzerland
now members of the EU) increased by 66 per cent during the same period, which
seems to be yet another proof of how investment in research is taken as a
guarantor of competitiveness.

Problems
None of this implies that everything is just fine in Europe. On the contrary,
there are many obvious problems still to be solved. For one thing,
international comparisons are far from flattering. In the old EU, total R&D
spending was 2 per cent of aggregate gross domestic product (GDP), compared
with 2.8 per cent in Japan and 2.4 per cent in North America (Mexico included).
Similarly, the R&D workforce in Europe is estimated at 700,000, compared with
750,000 in the United States (whose population is two-thirds the size) and
520,000 in Japan (whose population is half that of Western Europe). And while
the annual growth rate of the technical labour force is now greater (at 4 per
cent) in the EU than in the United States (2 per cent), it falls short of that
in Japan (at 5 per cent).
  What this implies is that Europe as a whole needs a mechanism for stimulating
the great diversity of European university systems to augment the accumulation
and employment of technically skilled people. While the Maastricht Treaty gave
the commission a competence it previously lacked in higher education, it is not
self-evident that it will be able to accomplish much while university policy
remains as central to national policy as has traditionally been the case.
  There are also serious problems in the infrastructure  of research that merit
attention. the recognition by EMBL last year that Europe needs a facility for
breeding genetically defined mice for use in research was more than a means of
keeping Italy in membership: it was a discovery of how the research enterprise
can be strengthened. The VLBI comparator station is an earlier example. It
would be interesting to know what would happen if the research community in
Europe were invited to declare its needs in such respects.
  There remains the question of the European Commission's role in the
management of research. Traditionally its function has been to support
intra-European collaboration in research, both in academic research (as through
the "Science" programme, closed in 1992) and in applied research, as with the
clutch of programmes called Esprit (in information technology). On paper, the
need for the stimulation of industrial collaboration should have been
attenuated with the arrival of the single market in 1990, but that penny has
not yet dropped.
  On the other hand, there remains a crying need to stimulating higher
education and research in the parts of Europe where it is at present neglected,
but the scope for action there is necessarily restricted by the doctrine of
subsidiarity (that the commission shoulder responsibilities that member states
can perform for themselves) and mutual jealousy. Why, a high-spending member
state will readily ask, should it give the commission funds to spend on
research grants to be spent in laggard countries, when the recipient
governments could raid their own tax revenues if they chose? But that typifies
the nub of the European dilemma that would persist even if the EU were
constitutionally a federation. Even in the United States, affirmative action on
behalf of states such as Arkansas, whose activity in research is below the
average, is an unpopular case.



Implications, J. B. Graham, 11 May, H, Bio

Implications of the late Palaeozoic oxygen pulse for physiology and evolution
The late Palaeozoic was marked by significant changes in atmospheric chemistry
and biotic composition. Geochemical models suggest a marked increase and then
decline of atmospheric oxygen and associated shifts in the concentration of
carbon dioxide. Although the actual magnitude of these changes is uncertain,
the pulse of oxygen concentration may have reached a maximum of 35% and then
dropped to 15% (compared with the present 21%). This oxygen pulse may have
influenced the evolution of major groups of organisms.

Most models of atmospheric evolution indicate that oxygen fluctuated
substantially at various times during the Phanerozoic. The recently developed
oxygen model of Berner and Canfield confirm general features of several earlier
reconstructions, and indicates a rise in oxygen to 35% and then a fall to 15%
over the span of 120 Myr during the late Palaeozoic. Although these atmospheric
oxygen fluctuations are thought to have been largely driven by biotic factors,
there has been little consideration of how oxygen changes affected organismic
physiology or biosphere evolution.
  This essay focuses on the biological effects of hyperoxia and describes
changes in the fossil record correlating with varying atmospheric oxygen
levels. Relative to the present atmospheric level of 21% oxygen the Palaeozoic
oxygen changes are substantial and would have had dramatic biological
consequences. An increased global oxygen supply during the Mid-Devonian to
late carboniferous would have enhanced diffusion-dependent processes such as
respiration. Accordingly, certain organisms could attain a larger body size. An
increased aquatic oxygen concentration and greater oxygen penetration into
benthic substrates would permit greater biomass densities in most habitats.
Elevated oxygen could also increase metabolic rate and turnover and resource
accessibility, thereby promoting the radiation of some taxa. Associated
atmospheric changes in air density and heat capacity would have affected
several biological processes.
  In contrast, a decline in global oxygen during the Permian would require
compensatory respiratory adaptations and might have been restrictive for some
groups. The rate of oxygen decline was too gradual, however, to have been the
primary cause of the end-Permian extinction. A 15% oxygen atmosphere at sea
level, for example, is equivalent to the oxygen partial pressure (15 kPa in dry
air) at an altitude of about 2.5 km, which is well within the adaptation limits
for most extant biota.

Effects of changing O2 concentration
Table 1 compares biologically important physical differences between the
existing atmosphere and the estimated extremes of the late Palaeozoic oxygen
pulse. In plants and most animals, respiration is entirely diffusion limited,
thus tissue composition and metabolic demand as well as body shape and
thickness are determined by the availability of oxygen. Table 1 shows oxygen
effects on diffusion-dependent body thickness. Relative to dimensions dictated
by the present atmospheric level, organisms in 35% oxygen could have a 27%
greater diffusive distance, allowing for thicker constructions, less work
invested in gas exchange, and greater body size. In contrast, the estimated
end-Permian atmosphere containing 15% oxygen would necessitate an 18% reduction
in diffusive distance. Biological effects from changes in ultraviolet radiation
during the oxygen pulse are unlikely given that all of these atmospheric oxygen
levels far exceed the minimal amounts required for a biologically effective
ozone shield.
  Nitrogen partial pressures are thought to have been fairly constant
throughout the Phanerozoic. Thus, relative to present atmospheric levels, a 35%
oxygen atmosphere would have a 21% greater air density and barometric pressure,
whereas a 15% oxygen atmosphere would be 13% less. A denser atmosphere would
favourably influence the evolution of insect flight by offering greater lift
and altering characteristics such as the Reynolds number and boundary layer
thickness. Density changes would also affect biological processes ranging from
ventilatory mechanics to wind-shear resistance. Physical parameters such as
viscosity, specific heat and thermal conductivity would also vary in the low
and high oxygen atmospheres. Calculations of such variation require physical
constants based on standard temperature and pressure, however, which may not
apply to the variable oxygen conditions of the late Palaeozoic atmosphere. 
  Paleoatmospheric models for carbon dioxide show that this gas underwent large
and sometimes inverse fluctuations compared to those for oxygen. The high carbon
dioxide levels occurring in the Ordovician and Silurian resulted from tectonic
activity. During the late Carboniferous oxygen peak, atmospheric carbon dioxide
concentration was similar to PAL (0.036%) but was about three times greater at
the end of the Permian. Carbon dioxide shifts of this magnitude affect a number
of atmospheric properties, aquatic pH, as well as animal and plant physiology
(see below).

Late Palaeozoic biological correlations
The mid- to late Palaeozoic (Devonian-Permian) saw major shifts in the diversity
and complexity of nearly all groups of plants and animals throughout the
biosphere. The Permian saw a dramatic restructuring punctuated by the
end-Permian extinction.
  These biotic changes and shifts in divergence of major groups proceeded in
parallel with the rise and then fall of oxygen availability. However, the late
Palaeozoic was also characterized by changes in physical factors such as
climate, tectonics and geochemicals cycling. Some workers have linked localized
oxygen reductions during the Permian to the decline and disappearance of taxa;
however, the most likely effect of global atmospheric hypoxia was elimination
of species specialized for the permissive hyperoxic atmosphere.
  Terrestrial invertebrates. Insects and other arthropod groups provide
spectacular examples of the expansive and contractive influences associated
with the oxygen pulse. There was a marked Carboniferous diversification of
insects on the basis of feeding specializations and the origin of flight, and
many species exhibited gigantism relative to extant forms. Predatory defence is
the usual explanation offered for insect gigantism. However, by increasing
diffusive permeation, increased oxygen concentration would have permitted
insects to become larger. Insects have a tracheal gas-exchange system, a
branching network of tubes extending from the body surface to the small
tracheoles within respiring cells. This system is primarily, if not
exclusively, diffusion dependent, with 0.5 cm suggested as  an upper limit to
passive diffusion.
  Giantism occurred in the Protodonata (dragonflies; wing span up to 71 cm),
the Ephemeroptera (mayflies, 9-20 cm), the Palaeodictyoptera (1-43 cm) and in
the Diplura and Thysanura. For example, the giant Carboniferous dragonfly
Meganeura monyi had a thoracic diameter of about 2.8 cm. By contrast, thoracic
width of the largest (10-cm wingspan) extant dragonflies (Anax, Aeshna) is
about 1 cm (M. May, personal communication). Most of the various insect taxa
that attained exceptionally large body sizes during the Carboniferous did not
persist after the Permian, when 27-30% of the known insect orders were lost.
Gigantism also characterized some other Carboniferous arthropods (such as
diplopods, arthropleurids and scorpions), extant relatives of which are
diffusion dependent.
  The greater density of the hyperoxic atmosphere facilitated insect flight by
enabling relatively small articulated winglets to provide lift, albeit with
suboptimal aerodynamics. Although the Devonian may have seen the first
apterygote insects, winged forms diversified extensively throughout the
Carboniferous. The earliest proto-wings possibly had a respiratory function and
were used secondarily for locomotion and predator escape. The need to power
flight would have required ever-more sustained and higher levels of oxidative
metabolism, which would be facilitated by higher levels of atmospheric oxygen.
The ability of some Recent flying insects to ventilate the tracheal system
minimizes diffusive limitations; however, mechanical ventilation does not
mitigate diffusion limitation in the terminal tracheoles. Schidlowski argued
that the tracheal system of Meganeura would have been inadequate for sustaining
flight metabolism in oxygen at present atmospheric levels, and predicted a
hyperoxic Carboniferous atmosphere.
  Flight became the decisive agent for insect dispersal and diversification.
The improvement of flight and its association with smaller species may have
ensured its persistence in numerous pterygote taxa through the Permian. But as
already noted, large fliers did not survive beyond the Permian.
  Aquatic invertebrates. The majority of aquatic invertebrates lack
well-developed respiratory and ventilatory systems and depend upon
diffusion-mediated respiration. Increased oxygenation of Devonian and
Carboniferous fresh waters would have increased their utility as nursery
grounds for eggs and larvae, thereby expanding life-history options for the
rapidly diversifying terrestrial arthropods. Beyond this, global atmospheric
hyperoxia would have reduced the limitation of seasonal aquatic hypoxia,
favoured infaunal penetration of marine sediments, allowed denser aggregation,
and increased the body size of some groups.
  Size increases have been recorded for a number of taxa throughout geological
time, and have been proposed to reflect multiple biotic factors. The observed
correlation between size increases in marine invertebrates (bryozoans,
foraminiferans, rugose corals and brachiopods) and atmospheric hyperoxia is
notable because these forms have a primarily diffusion-dependent respiration.
Specific examples include the largest known brachiopod (Gigantoproductus, with
a shell 300 mm wide) of the early Carboniferous, and the radiation of the large
fusilinid Foraminifera (Mid-Carboniferous-Permian).
  The evolutionary transition from a "brachiopod-dominated late Palaeozoic
marine fauna" to the modern, "mollusc-dominated Mesozoic-Cenozoic fauna"
occurred at the end of the Permian. Associated with this was the end-Permian
extinction of several dominant members of the sessile, epifaunal filter-feeding
community of the Palaeozoic, including the rugose and tabulate corals, and
numerous families of brachiopods, bryozoans, crinoids and others. A number of
factors, including changes in sea level and reduced temperature, may have
contributed to this extinction and faunal transition. Whereas we do not regard
Permian reductions in global oxygen as a major causal factor in either of these
events, hypoxia probably contributed to the decline of many of the
diffusion-dependent faunal elements. The bivalves, which had a relatively
greater Permian survival than did the brachiopods, and which became dominant in
the Triassic, had numerous specializations that allowed ventilation, mobility,
and sediment penetration.
  The terrestrial flora. The Carboniferous and early Permian flora included
ferns and primitive gymnosperms, as well as sphenopsids and lycopods, which
were arborescent and grew to considerable height. Increased plant height and
arborescence required thicker support structures, particularly as a greater
atmospheric density elevated wind stress. Heightened atmospheric oxygen
facilitated these morphological changes by allowing diffusion through thicker
support elements and by enhancing the oxygen-dependent biosynthesis of lignin,
a dominant structural material of many carboniferous plants. An increased
oxygen level may not however, have been entirely advantageous for plants,
particularly when it occurred simultaneously with a decline in carbon dioxide.
Changes in the carbon dioxide to oxygen ratio can lessen plant productivity by
photorespiration. However, the rarefaction of atmospheric carbon dioxide over
the Carboniferous may have established a selective advantage for new, more
effective carbon-fixation pathways in some plants.
  Terrestrial vertebrates. The diversification of tetrapods began in the
Devonian and was accelerated during the Carboniferous, when at least eleven of
sixteen known basal lineages (including the three primary classes of amniotes:
Synapsida, Diapsida and Anapsida) all appeared. In the Permian, 75 per cent of
these tetrapod lineages went extinct.
  Global atmospheric hyperoxia possibly aided the vertebrate invasion of land.
Breathing hyperoxic air reduces the ratio of evaporative water loss to oxygen
uptake, and thus increases aerial gas exchange efficiency by lowering
desiccation. Although the early tetrapods had both gills and lungs, gills are
not effective for aerial gas exchange, particularly carbon dioxide discharge,
which is a more acute constraint for aerial respiration than oxygen uptake. As
many early tetrapods were sizeable and had a thick body armour, aerial
cutaneous respiration could not compensate for the reduced respiratory capacity
of gills in air. Tetrapods were thus heavily dependent on lung exchange. The
rise in atmospheric oxygen coupled with the reduction in carbon dioxide would
have elevated primitive lung effectiveness. Subsequent changes in these gas
ratios could also have influenced the transition from pulse pumping to
aspiration breathing and perhaps even the shift from an oxygen to a
carbon dioxide-modulated respiratory control mechanism.
The reduced ratio of water loss to oxygen uptake achievable in hyperoxic air
may have also been critically important in the evolution of the cleidoic egg.
  Hyperoxia would have also enhanced tetrapod metabolic capacity and thus
contributed to the sustained power production required to overcome gravity.
Increased metabolic performance opened new ecological options within the
rapidly expanding terrestrial biosphere. Although tetrapod diversity decreased
in the Permian, the synapsids underwent a pronounced diversification,
proceeding from the pelycosaurs to the therapsids, a diverse assemblage of
herbivores and carnivores known as the mammal-like reptiles. The
diversification of synapsids seems to be partially attributable to the effects
of hyperoxia and a denser atmosphere on activity enhancing specializations such
as metabolic heat retention. The large "sails" of pelycosaurus such as
 Dimetrodon are thought to have functioned in heat transfer. The capacity to
regulate heat gain and loss was an important precursor for endothermy.

Conclusion
Attempts to correlate atmospheric oxygen with biosphere evolution have included
the origin of the ultraviolet-ozone shield and the early radiation of plants,
the evolution of the diffusion-limited Ediacaran fauna, the Cambrian explosion,
the correlation of patterns of extinction and metabolic requirements among
various taxa, and regional or even mass extinction events.
  We propose that a pulse in atmospheric oxygen spanning the mid-Devonian,
Carboniferous and Permian periods influenced the contemporaneous biosphere. The
rise in oxygen during the Devonian and Carboniferous favourably influenced
diffusion-dependent features. Increased oxygen availability may have also
fuelled the diversification and ecological radiation of late Palaeozoic groups
by acting as a substrate for the evolution of behavioural, physiological and
ecological adaptations, permitting greater exploitation of aquatic habitats and
the newly evolving terrestrial biosphere. The fall in global atmospheric oxygen
during the Permian was restrictive and modified or eliminated some taxa that
radiated in hyperoxia, but was not a primary cause of the end-Permian
extinction.



Neural activity, F. Crick, 11 May, A, Bio

Are we aware of neural activity in primary visual cortex?
It is usually assumed that people are visually aware of at least some of the
neuronal activity in the primary visual area, V1, of the neocortex. But the
neuroanatomy of the macaque monkey suggests that, although primates may be
aware of neural activity in other visual cortical areas, they are not directly
aware of that in area V1. There is some psychophysical evidence in humans that
supports this hypothesis.

What is the relationship between "awareness", in particular visual areas,
and neuronal activity in the nervous system? Within a larger context, this
question is sometimes known as the "mind-body" problem, and has been asked
since antiquity. In recent years, we have been trying to understand this
relationship from a neuronal, reductionist point of view by making the
following two basic assumptions.
  (1) For an animal to be aware of some aspect of a visual object, there has to
be a group of similar neurons (to allow coarse coding) located somewhere in the
cortical system (which includes the neocortex and the hippocampus, as well as
directly associated parts, such as the thalamus), the firing of which is
correlated with that feature in the visual scene. We shall not discuss here the
so-called binding problem (that is how different aspects of a single visual
object are bound together), nor the idea that this binding may require
correlate frings (such as the g oscillations) of sets of neurons in different
cortical areas or the detailed nature of the neuronal correlate of
consciousness (NCC).
  (2) Such neurons must project directly to some part of the front of the
cortex, in particular, to those areas in front of the primary motor area (M1,
also called area 4). In general, this large expanse of cortex is called
"frontal" cortex and includes motor cortex, premotor and prefrontal areas.
  We have argued elsewhere that to be aware of an object or event the brain has
to construct a multilevel, explicit, symbolic interpretation of part of the
visual scene. By multilevel we mean, in psychological terms, different levels
such as those that correspond, for example, to lines or to eyes or to faces. In
neurological terms we mean, loosely, the different levels in the visual
hierarchy.
  What exactly do we mean by "explicit"? This is not an easy question to
answer. The pattern of coloured dots on a television screen, for instance, may
contain an implicit representation of a person's, but only the dots and their
location are explicitly represented on the screen. A simple neural network
(such as a one-layer perception) could be constructed to respond whenever a red
dot appeared at one particular place on the screen, or even to a particular
fixed set of dots. Such a simple network cannot be tuned up to respond to a
particular face that might appear anywhere on the TV screen, and varied in size
orientation or facial expression.
  It is tempting to say that an explicit representation of a feature means that
in the brain there is only a single neuron (otherwise called a "grandmother"
cell) that responds to that particular feature, and to that feature alone. We
think this very improbable, if only because the firing of a single neuron in
the cortical system is, by itself, unlikely to activate strongly any of its
postsynaptic targets. Furthermore, "normal" visual input is unlikely to activate
such a cell by itself without also activating its neighbours.
  We believe that any particular visual feature will be coarse coded (that is,
spread out over a set of similar neurons), and indeed there is some evidence
for this in the case of faces. These "face neurons" do not respond to a single
spot of light (as retinal ganglion cells do) but are activated mainly by
face-like objects, each neuron of the set being activated in a somewhat
different way. This type of firing of a set of neurons is considered to be the
neural symbol for a face, and the collective firing of this set makes the
face-like aspect of the face explicit. (This use of the term "symbol" should
not be taken to imply the existence of a disembodied homunculus who is looking
at it.) The meaning of such a symbol depends not only on the receptive fields
of these neurons but also to which other neurons they project (their projective
fields), and exactly how they are connected to them.
  The firing of a single retinal ganglion cell might represent explicitly a
particular spot of light in the visual field, but could not represent a face.
An explicit face representation can only occur at a higher level in the visual
system, after much further processing, especially as the "face-neurons" are
found experimentally to be relatively indifferent to the location and
orientation of the face in the visual field; if they were not there would have
to be too many of them. Note that a person is not necessarily aware of all such
representations. Something more is required for awareness: see (2) above.
  We have not previously discussed in detail the need for projections from the
visual system to the front of the brain, although we have mentioned it in
passing. Our basic argument assumes that, in going from one visual area to
another further up in the visual hierarchy (that is, further away from the
retina), the information is recoded at each step. This is broadly compatible
with the known fact that the "features" to which a neuron responds become more
complex in going from the primary visual cortex, V1 (also called striate
cortex, or area 17) to the higher levels in the visual hierarchy, such as the
inferotemporal cortical areas.
  A neuron that is firing, but whose axon has been inactivated by local
anaesthetics, can contribute little, if anything, to immediate visual
awareness, unless one favours some dualistic position. Thus the main
destination of the axon -- loosely, its projective field -- is clearly
important.

Prefrontal brain areas and planning
Our second assumption is based on the broad idea of the biological usefulness
of visual awareness (or, strictly, of its neural correlate). This is to produce
the best current interpretation of the visual scene, in the light of past
experience either of ourselves or of our ancestors (embodied in our genes), and
to make it available, for a sufficient time, to the parts of the brain that
contemplate, plan and execute voluntary motor outputs (of one sort or another).
  Exactly how these prefrontal and premotor cortical areas operate is currently
unknown, although there is now fragmentary evidence about the behaviour of some
of them. Even in the macaque, the details of neuroanatomical connections
between all these areas have not yet been worked out in as much detail as they
have for most of the visual areas.
  It is probably a general rule that the further, in terms of the number of
stages, a prefrontal area is from the primary motor area M1, the longer is the
timescale of the planning in which it is engaged. Moreover, these cortical
areas are all heavily involved with the basal ganglia (which include the neostriatum,
the globus pallidus and the substantia nigra), the main function of which may
be to provide a bias back to these areas (as well as to the superior colliculus
in the midbrain) to influence the next step in their processing, that is, to
assist some behaviours that involve a sequence of activities. The subject is
additionally complicated for humans because of our highly developed language
system and its usefulness for expressing our "thoughts" (in silent speech, for
example).
  Fortunately, at this stage, the details of the behaviour of these "frontal"
areas need not concern us. All we need to postulate is that, unless a visual
area has a direct projection to at least one of them, the activities in that
particular visual area will not enter visual awareness directly, because the
activity of frontal areas is needed to allow a person to report consciousness.

Primary visual cortex and its connections
This lack of any direct projection to the frontal cortex appears to be true for
area V1 of the macaque monkey, the almost exclusive recipient of the output of
the lateral geniculate nucleus (LGN). V1 has not direct projections to the
frontal eye fields (part of area 8), nor to the broad prefrontal region
surrounding and including the principal sulcus, nor, as far as we know, to any
other "frontal" area. Nor does it project to the caudate nucleus of the basal
ganglia, to the intralaminar nuclei of the thalamus (L. G. Ungerleider,
personal communication), the claustrum or the brainstem, with the exception of
a small projection from peripheral V1 to the pons. V1 does, of course, provide
the dominant visual input to most of the posterior visual cortical areas,
including V2, V3, V4 and MT (also known as V5). Among subcortical targets, V1
projects to the superior colliculus, the LGN and the pulvinar.
  It is unlikely that information sent along the pathway from V1 to the
superior colliculus (partly responsible for initiating and controlling eye
movements) can produce conscious visual awareness. There is a multistage
pathway going from V1 to the colliculus, from there to the pulvinar (a large,
mainly visual, part of the thalamus) and thence to higher visual areas. This
pathway may be involved in visual attention, but, according to our arguments,
it is not sufficiently direct or strong to produce, by itself, vivid visual
awareness of the neural activities in V1.
  The pathway from V1 to the colliculus might possibly be used to produce
involuntary eye movements, so psychophysical tests, using eye movements as the
response, might show a form of blindsight in which subjects respond above
chance while denying that they see anything. It is also possible that this or
other pathways can produce vague feelings of some sort of awareness.
These
Primary visual cortex and awareness
Our hypothesis is too speculative to be convincing as it stands, as we are not
yet confident as to how to think correctly about most of the operations of the
brain, and especially about the detailed function of the so-called "back
pathways". Some readers may find our suggestion counterintuitive, partly
because for many years V1 was the only visual cortical area that was worked on
extensively. We would ask them: do you believe that you are directly aware of
the activity in your retina? (Of course, without your retinae, you cannot see
anything.) If you do not believe this, what is the argument that you are
directly aware of the neural activity in V1?
  To avoid misunderstanding, let us underline what our hypothesis does not say.
We are not suggesting that the neural activity in V1 is unimportant. On the
contrary, we believe the detailed processing in V1 is highly important for
normal vision, although recent work has shown that V1 in at least one patient
is not essential for some limited form of visual awareness related to motion
perception. All we are hypothesizing is that the activity in V1 does not
directly enter awareness. What does enter awareness, we believe, is some form
of the neural activity in certain higher visual areas, because they project
directly to prefrontal areas. This seems well established for cortical areas in
the fifth tier of the visual hierarchy, such as MT and V4. For areas in the
intervening tiers, such as V2, V3, V3A, VP and PIP, we prefer to leave the
matter open for the moment.
  Experiments suggest that only some of the active neurons in a cortical area
are likely to produce direct visual awareness. During binocular rivalry, the
visual input is constant but the visual percept changes. Experiments on neurons
in cortical area MT of the alert macaque monkey show that during binocular
rivalry produced by two gratings moving in opposite directions, only a subset
of the active neurons in cortical area MT follow the percept. It will be of
great importance to discover the layer in which these cells are located, their
type and where they send their axons. The firing of most of the neurons in MT
does not depend on which of the two alternative percepts the monkey reports
seeing.
  Our hypothesis was suggested by neuroanatomical data from the macaque monkey.
For humans, we are less certain because of the present miserable state of human
neuroanatomy, but we surmise that our hypothesis, if true for the macaque
monkey, is also likely to be true for apes and humans. To be established as
correct, it also needs to fit with all the neurophysiological and psychological
data. What kind of evidence would support it?

Physiological and psychophysical evidence
A possible example may make this clearer. It is well known that the colour
perceived at one particular visual location is influenced by the wavelengths of
the light entering the eye from surrounding regions in the visual field. This
mechanism acts partially to compensate for the effects of differently coloured
illumination. A white patch surrounded by patches of many colours still looks
fairly white even when illuminated by pink light. This form of (partial) colour
constancy is often called the Land effect.
  It has been shown in the anaesthetized monkey that neurons in V4, but not in
V1, exhibit the Land effect. As far as we know, the corresponding information
is lacking for alert monkeys. Because people cannot voluntarily turn off the
Land effect, it would follow, if the same results could be obtained in a
behaving monkey, that it would not be directly aware of the "colour" neurons in
V1. Notice that if neurons in both V1 and V4 in the alert monkey did turn out
to show the full Land effect, this would not, by itself, disprove our
hypothesis, as we do believe that people are visually aware of certain neural
activity in V4 that could be triggered by activity in V1.
  Psychophysical experiments would support our hypothesis if they demonstrate
that people are not aware of neuronal activity that is highly likely to occur
in V1. Such experiments have been done recently by D. I. MacLeod and Sheng He
(personal communication). In brief, they have shown that exposure to
high-contrast gratings that are so finely spaced that they cannot be seen (that
is, cannot be distinguished from a uniformly grey surface) can produce an
orientation-selective loss in sensitivity of human subjects to slightly less
finely spaced gratings that can indeed be perceived. Because of neuronal
convergence in higher cortical areas and the associated increase in receptive
field size, neurons sensitive to the very fine conditioning grating appear to
be restricted to V1. (This is one of the reasons why it is frequently assumed
that we are aware of neural activity in V1.) These psychophysical experiments
are therefore compatible with the idea that certain neurons in V1 respond to
very high spatial frequencies of which we are not visually aware. (We did not
know of these results when we first formulated our hypothesis.) The support for
our ideas would be greater if it were shown (by imaging methods such as
positron-emission tomography (PET) or functional magnetic resonance imaging
(MRI)) that these invisible gratings produced significant activity in V1 in
humans. For an alert macaque, it might be possible to show experimentally that
very finely spaced gratings, that activated certain neurons in V1, could not be
reported by the monkey, although the animal could report less finely spaced
ones.
  The firing of some neurons in V1 depends upon which eye the visual signal is
coming through. Neurons higher in the visual hierarchy do not make this
distinction, that is, they are typically binocular. Most people are certainly
not vividly and directly aware of which eye they are seeing with (unless they
close or obstruct one eye), although whether they have some very weak awareness
of the eye of origin is more controversial. These well-known facts suggest that
people are not vividly aware of much of the activity in V1.
  Our ideas would not be disproved if it were shown convincingly that (for some
people) V1 is activated during visual imagery tasks. There is no obvious reason
why such top-down effects should not reach V1. Such V1 activity would not by
itself prove that people are directly aware of it, any more than the V1
activity produced there when their eyes are open proves this.
  Theories are rarely proved or disproved by a single experiment; it needs a
coherent body of interlocking data to establish a theory as correct. It could
be argued, in opposition to our view, that there is indeed a small subset of V1
neurons the firing of which directly expresses some unique aspect of visual
awareness. These special neurons could easily be missed, making it difficult to
disprove our idea. If it turns out that in many other cortical areas only a
particular type of neuron expresses visual awareness (for example, some of those
in the lower cortical layers), and that neurons of a similar type in V1 respond
in ways of which we are not aware, this might be sufficient proof of our
hypothesis.
  Our hypothesis is thus rather subtle; although we believe that if it turns
out to be true it will eventually come to be regarded as completely obvious. We
hope that further neuroanatomical work will make it plausible for humans, and
further neurophysiological studies will show it to be true for most primates.
We have yet to track down the location and nature of the neural correlates of
visual awareness. Our hypothesis, if correct, would narrow the search to areas
of the brain further removed from the sensory periphery.



Origin, P. R. Crane, 2 March, RA, Bio

The origin and early diversification of angiosperms
The major diversification of flowering plants (angiosperms) in the Early
Cretaceous, between about 130 and 90 million years ago, initiated fundamental
changes in terrestrial ecosystems and set in motion processes that generated
most of the extant plant diversity. New palaeobotanical discoveries, combined
with recent phylogenetic analyses of morphological and molecular data, have
clarified the initial phases of this radiation and changed our perspective on
early angiosperm evolution, though important issues remain unresolved.

Angiosperms dominate the vegetation of most terrestrial ecosystems and consist
of roughly 250,000-300,000 extant species, more than all other groups of land
plants combined. For almost 150 years, understanding the origin and early
diversification of these dominant land plants was hindered by what was thought
to be an uninformative fossil record, uncertain relationship among extant
angiosperms, and apparently insuperable morphological "gaps" between
angiosperms and other seed plants (gymnosperms). The 1960s and 1970s saw
notable progress toward breaking this impasse. Syntheses of data from extant
angiosperms demonstrated that the subclass Magnoliidae is a phylogenetically
basal assemblage and studies of fossil pollen, leaves and reproductive
structures documented a major diversification of angiosperms in the Early
Cretaceous between about 130 and 90 million years before present.
  Recently there has been renewed interest in the patterns and processes of
early angiosperms evolution, and explicit phylogenetic analyses, based both on
morphological and on molecular data, have clarified and revitalized many of the
old debates. In the fossil record, bulk sieving techniques have yielded diverse
and exquisitely preserved Cretaceous flowers (both mummified and charcoalified)
that have resolved the systematic relationships of many early angiosperms and
identified the source of important dispersed Cretaceous pollen. New information
on floral form and reproductive biology in putatively basal extant groups has
also catalysed comparative studies and enhanced the interpretation of fossil
material. Here we review how these developments have sharpened arguments over
the relationship among basal angiosperms and their seed plant relatives, and
have changed our perspective on early angiosperm evolution.

Origin of angiosperms and their flowers
Ideas on the origin of angiosperms have sometimes invoked polyphyly, have
treated the pteridosperms (seed ferns) as a natural group, and have implicated
almost all groups of fossil and living gymnosperms as potential angiosperms
ancestors. More recent work has emphasized cladistic discrimination of
relationships among major clades of extant and extinct seed plants. Parsimony
analyses, based on morphological data, have shown that angiosperms are one of
the most strongly supported monophyletic groups in the plant kingdom and that
the pteridosperms (as traditionally defined) are a highly unnatural group.
However, the most striking result from recent phylogenetic analyses in the
support for earlier ideas that identified Bennettitales (extinct) and Gnetales
(extant) as the seed plants that are most closely related to angiosperms. The
resulting group (Bennettitales, Gnetales and angiosperms, plus Pentoxylon in
some analyses) has been termed the "anthophytes", to emphasize their shared
possession of flower-like reproductive structures. Among extant taxa,
angiosperm monophyly and a close relationship between Gnetales and angiosperms
(to the exclusion of Ginkgo, conifers and cycads), is also supported by
parsimony analysis of partial 18S and 26S ribosomal RNA sequence data as well
as analyses of combined morphological and molecular data.
  Although all of the explicit cladistic studies conducted to date broadly
support the anthophyte concept, they differ markedly in their resolution of
relationships within the clade, and in their identification of the closest
relatives of anthophytes. In addition, the long-standing question of whether
angiosperm flowers are derived from a simple branch (euanthial) or derived from
multiple branches (pseudanthial) is still unresolved. The origin of angiosperm
stamens and carpels, and carpels, and their homologies with structures in other
seed plants also remain problematic. Angiosperm stamens are both more regular
and more simple than the pollen organs of the Bennettitales, Gnetales and most
Mesozoic pteridosperms. In contrast, angiosperm carpels and bitegmic ovules are
relatively complex compared to the ovulate structures of Gnetales and
Bennettitales. Particular concerns are how the carpel and the outer layer of
the typical angiosperm ovule should be compared to structures in related
groups. Current explanations are all inadequate, and lean heavily on critical
interpretations of fossil specimens that are not well understood (such as
Caytonia, corystosperms).
  Such uncertainties highlight the need to clarify structural and developmental
homologies among the reproductive structures of angiosperms and related groups;
in many respects, the morphological gap between angiosperms and other seed
plants remains as wide as ever. This gap can be closed most directly through
renewed morphological documentation and phylogenetic analysis of well preserved
fossil gymnosperm material from Mesozoic fossil floras. Progress may also be
possible by understanding how the genes controlling angiosperm stamen and
carpel differentiation are expressed in the reproductive organs of extant
gymnosperms (such as cycads, conifers, Gnetales).

Angiosperm phylogeny and floral evolution
Current hypotheses of angiosperm evolution recognize two large clades
(monocotyledons and eudicots) embedded within a poorly defined basal assemblage
(grade) of magnoliid dicots (Magnoliidae). The monocotyledons are defined as
monophyletic by their single cotyledon and other features. Eudicots are
circumscribed by the production of triaperturate or triaperturate-derived
pollen (convergent in Illiciaceae, Schisandraceae).
  Generalizations developed around the turn of the century emphasized large
multiparted flowers like those of extant Magnolia as a starting point for
angiosperm floral evolution. Recent ideas accept that plants in the subclass
Magnoliidae retain a broad array of probable unspecialized angiosperm features
(such as parts generally free, lack of differentiation within the perianth),
and have also documented several previously unrecognized floral features that
are general among extant magnoliids (such as valvate anther dehiscence,
ascidiate carpel development). Modern studies have emphasized the great
diversity of floral form, biology and structure among magnoliids. Variation in
the number and arrangement of floral parts is extreme, and both large,
multiparted bisexual flowers and small, simple, frequently unisexual flowers
are widespread.
  Identifying the likely basic condition of angiosperm flowers is therefore
intimately connected with the recognition of phylogenetic patterns among
magnoliids. Unfortunately, the substantial morphological differences that
separate angiosperms from all other seed plants, combined with uncertain
relationships among the anthophytes, makes it difficult to "polarize" many of
the crucial characters. This, in turn, complicates attempts to "root" the
angiosperm tree and contributes to the instability of current phylogenetic
results. Among the studies currently available, there is emerging agreement
that undue attention has focused on extant Magnoliaceae and its allies, and
combined morphological and molecular results tend to favour phylogenetic models
in which taxa with small, trimerous or even more simple flowers are basal in
angiosperms.
  Fossil evidence of early magnoliid diversity. The variety of leaves and
pollen in the Early Cretaceous implies that magnoliids were diverse early in
angiosperm evolution, and this is supported by the surprisingly reach, emerging
record of fossil flowers. In the extensive assemblages of Early Cretaceous
angiosperm reproductive structures from Portugal and eastern North America, all
of the fossils described so far are small (frequently less than 2mm in length),
and because they occur with larger fossils of other plants (such as conifers)
it seems unlikely that this is a result of depositional bias. The flowers are
generally few-parted and often with an undifferentiated perianth. Stamens
generally have small pollen sacs with valvate dehiscence, and a relatively
large apically expanded connective. Carpels generally have a poorly
differentiated stigmatic surface. Among magnoliids, the fossil record indicates
that several extant families were already differentiated by about the Turonian.
  Fossil evidence of early monocot diversity. Among extant plants
monocotyledons comprise only about 22% of angiosperm species. Over half of this
diversity is accounted for by four families (Orchidaceae, orchids; Poaceae,
grasses; Cyperaceae, sedges; Arecaceae, palms). The Cretaceous fossil record of
monocotyledons is depauperate compared to that of contemporary magnoliids and
eudicots, and perhaps reflects a variety of biases working against both their
preservation and recognition. During the mid-Cretaceous, monocots are probably
represented by several leaves, and more equivocally by dispersed pollen.
However, evidence of rapid monocot diversification is provided later in the
Cretaceous by fruits of Zingiberales (gingers and their allies), and leaves and
stems of palms. By the Early Tertiary many monocot groups had differentiated.
  Fossil evidence of early eudicot diversity. Eudicots comprise about 75% of
extant angiosperm species, and are recognized as a monophyletic group in
phylogenetic analyses of combined morphological and molecular data.
Triaperturate pollen, which is diagnostic of the group, is first recorded in
dispersed palynological assemblages around the Barremian-Aptian boundary (about
125 Myr) or perhaps slightly earlier.
  Basal groups in the eudicot clade include the ranunculids (Ranunculidae,
sometimes included in the Magnoliidae) and also the "lower" hamamelidids.
Embedded with this basal eudicot assemblage are the diverse groups that
comprise the bulk of extant angiosperms, including the "higher" hamamelidids
along with the subclasses Rosidae, Caryophyllidae, Dilleniidae and Asteridae,
which contain many highly diverse extant families (such as Asteraceae, the
sunflower family). Among the eudicots that can be recognized during the
mid-Cretaceous diversification are several "lower" hammamelidid lineages and a
diversity of generalized rosiid types. By around the Campanian, eudicots were
very diverse.

Pre-Cretaceous angiosperms?
Current studies of the fossil record show an orderly sequence of appearance of
angiosperm remains beginning with putative magnoliid pollen in the Valanginian,
and triaperturate pollen of eudicots around the Barremian-Aptian boundary (or
slightly earlier). By the earlier Cenomanian there are diverse magnoliids, a
variety of hamamelidids and a few rosiids, which show clearly that several
angiosperm families had already differentiated. By the Campanian, Maastrichtian
and Palaeocene, many extant angiosperm families in all subclasses had already
differentiated.
  Claims of pre-Cretaceous angiosperms need to confront this orderly sequence,
and although the literature on angiosperm origins is replete with putative
pre-Cretaceous angiosperms, most have been shown to be stratigraphically
misplaced, unrelated to angiosperms, lacking in diagnostic characters, or too
poorly preserved for reliable determination. Recently, however, there has been
renewed discussion of the possibility of pre-Cretaceous angiosperms stimulated
by the anthophyte hypothesis (which implies that the lineage leading to
angiosperms diverged from other known groups of seed plants before the Late
Triassic), the description of new and potentially relevant fossils from
Jurassic and Triassic rocks; and calculations of divergence times based on
molecular clock assumptions. These developments have highlighted the importance
of distinguishing clearly between the timing of angiosperm divergence
(splitting of the stem lineage from its sister groups) and the timing of
angiosperm diversification (splitting of the crown group into extant clades).
  In our view, the absence of distinctive triaperturate pollen grains in
numerous, rich, Triassic and Jurassic palynofloras from both hemispheres
precludes the long cryptic period of evolution implied by some estimates of
rates of molecular evolution implied by some estimates of rates of molecular
evolution, at least for eudicots. The systematic affinities of recently
described putative Jurassic and Triassic angiosperms are either clearly with
other groups (such as dipteridaceous ferns) or remain equivocal (such as
Triassic Sanmiguelia-like plants and Crinopolles-type pollen grains). Although
some of these fossils may be attributable to the anthophyte clade, they
highlight the difficulties of accurate systematic determination based on
inadequate material. Most defining features of angiosperms are unlikely to be
preserved in the fossil record. The most useful diagnostic morphological
features of angiosperms are stamens with two pairs of pollen sacs, and a carpel
enclosing the ovule. Both can only be observed in well preserved fossil
material.

Coevolutionary interactions
Angiosperm diversification has often been linked to the diversification of
pollen and nectar-collecting insects on the supposition that insect pollination
provides new possibilities for reproductive isolation and therefor elevation of
speciation rates. Compared to wind pollination, which is widespread in other
seed plants, insect pollination may also permit effective outcrossing at lower
population densities and in a greater range of environments, perhaps thereby
reducing extinction rates. These factors may explain the diversity of some of
the most speciose angiosperm families (such as orchids), but the extent to
which they provide a general explanation for the massive mid-Cretaceous
increase in angiosperm diversity is uncertain. In the Bennettitales,
flower-like reproductive structures provide indirect evidence of insect
pollination from the Late Triassic to Late Cretaceous, and insect pollination
may also have been established in some Mesozoic pteridosperms. The size and
morphology of diverse mid-Cretaceous gnetalean pollen, combined with evidence
from extant taxa, is also suggestive of insect pollination in the Gnetales.
Early angiosperms may therefore have coopted pollinators from previously
established relationships with other groups of seed plants.
  Whatever the effect of insect pollination on the initial angiosperm
diversification, discoveries of Early and mid-Cretaceous flowers leave no doubt
that early members of the group were insect pollinated. Stamens in fossil
flowers have small anthers with low pollen production, anther dehiscence is
valvate, pollen grains are often covered with a pollenkitt-like material,
stigmatic surfaces are generally unelaborated and pollen grains are often
smaller than the most effective size for wind dispersal. Comparison with modern
relatives suggests that these flowers were probably pollinated by
pollen-collecting or pollen eating insects. Flowers pollinated by nectar
collecting Hymenoptera and Lepidoptera occur in more derived groups of
angiosperms and appear later in the fossil record.
  Although Early Cretaceous angiosperms may have been very similar to their
living relatives in pollination syndrome, modes of fruit and seed dispersal
were probably substantially different. Cretaceous angiosperms fruits and seeds
are generally very small compared to their modern relatives, and there is no
evidence of specialized mammal or bird dispersal. Among basal groups, as in
angiosperms as a whole, the evolution of fleshy fruits, arillate seeds and
other apparent adaptations for animal dispersal, seems to be correlated with
the evolution of frugivorous/granivorous birds and mammals, perhaps during the
latest Cretaceous, but most strikingly during the Early Tertiary. The only
suggestion of animal dispersal of angiosperms in the Early Cretaceous is
provided by a single species of small fruits covered with hooked spines.

Rise to ecological dominance
The mid-Cretaceous diversification of angiosperms marks the transition from
Mesozoic ecosystems dominated by ferns, conifers, cycads and Bennettitales to
more modern Late Cretaceous and Tertiary ecosystems dominated by angiosperms.
In the palaeobotanical record this change is much more profound than that
occurring at the Cretaceous-Tertiary boundary and is also reflected in the
transition from sauropod-dominated to ornithopod-dominated dinosaur faunas.
Data on the floristic composition of both macrofloras and microfloras show that
angiosperms had attained diversity levels of about 50-80% by the end of the
Cretaceous, but there is evidence that angiosperm abundance still remained
subordinate to gymnosperms and ferns in some habitats, perhaps over large
geographical areas. The extent to which this is a general phenomenon is
currently uncertain but it is supported in part by the palynological data and
by the prominence of certain open-habitat ferns in some Late Cretaceous floras.
These considerations emphasize the value of complementary macrofossil-based,
and pollen/spore-based, assessments of diversity and abundance in evaluating
the rate and magnitude of the angiosperm radiation. They also raise the
possibility that disturbance by herbivorous dinosaurs may have been a
significant ecological factor in the first half of angiosperm history.
  Problems of large-scale stratigraphic resolution make it difficult to resolve
geographic patterns in the angiosperm radiation but compilations of
palynological data, especially from the Northern Hemisphere, show that the
initial increase in angiosperm diversity occurred in low palaeolatitude areas.
This result is of biogeographic interest for understanding the current
distribution of relictual magnoliid families, and also as significant
ecological implications. During the Early Cretaceous, low latitude areas
experienced semiarid or seasonally arid conditions that may have promoted a 
weedy life history with precocious reproduction (progenesis). Associated
effects may have included simplification and aggregation of sporophylls to
form a flower, enclosure of ovules in a carpel, truncation of the gametophyte
phase of the life cycle, and major reorganization of leaf and stem anatomy. In
turn, several of these features may have contributed to angiosperm "success"
through elevated speciation rates and/or more rapid and more flexible
vegetative growth. It remains uncertain, however, whether such effects were
manifested at the base of the angiosperm clade or within one or more angiosperm
subgroups.
  Interestingly, the Gnetales, which show many remarkable structural and
biological convergences to angiosperms, also diversified during the
mid-Cretaceous in low palaeolatitude areas, although they never became
significant at middle and high palaeolatitudes and experienced rapid decline
during the earliest Late Cretaceous. The temporal, palaeogeographic, and
perhaps ecological, parallels between this radiation of Gnetales and
angiosperms implies a common response to changing environmental conditions.
Explanations of angiosperm diversification may therefore have underemphasized
the effects of environmental changes during the Early and mid-Cretaceous, which
included high rates of sea-floor spreading, sea-level stands and probably high
global temperatures.

Future directions
The five living groups of seed plants are a poor sample of the total historical
diversity of the seed plant clade and thus additional molecular phylogenetic
analyses are likely to lead only to limited progress in evaluating the
phylogenetic relationships of angiosperms. However, reducing the gap between
angiosperms and their gymnosperms relatives is important because of its
implications for rooting the angiosperm tree. Renewed palaeobotanical efforts
with the early members of the Gnetales, Bennettitales, angiosperms and other
potentially closely related groups are therefore a high priority. Recognition
of "stem group" taxa, with some but not all of the defining features of the
"crown group" has been possible in other studies of land plant phylogeny, and
would help to resolve several outstanding problems.
  Current data from the fossil record and combined morphological/molecular
phylogenies of extant taxa challenge the view that earliest members of the
angiosperm clade were large, woody plants with Magnolia-like flowers. Instead
they suggest a very different concept of early angiosperms as perhaps
herbaceous plants of small stature. Flowers would have been small, simple
(perhaps unisexual) and probably lacking clear differentiation into sepals and
petals (suggesting that molecular genetic models of floral morphogenesis
developed for the eudicots Arabidopsis and Antirrhinum may require modification
among magnoliids). Stamens would have had a poorly developed filament and a
well developed anther with valvate dehiscence. Pollen would have been small,
monocolpate, tectate with a weakly developed endexine in non-aperturate areas.
The gynoecium would have been composed of one or more unilocular carpels
containing one or two ovules. The stigmatic surface would have been
unelaborated.
  Ecologically, the early diversity of angiosperms at low palaeolatitudes, and
the parallel Aptian-Cenomanian radiation of angiosperm and gnetalean pollen in
these areas, suggests that both groups responded in similar ways to the same
environmental cues. Possible linkages between mid-Cretaceous climatic, tectonic
 and other environmental changes, and their possible impact on terrestrial
ecosystems need to be explored. Early Cretaceous fossil plants from low
palaeolatitudes are likely to be particularly informative.



Hidden science, D.B.Paul, 23 March, C, Bio

The hidden science of eugenics
The early eugenicists were not stupid, but they did not share our social 
values. The rise and fall of the eugenics movement is a history that modern 
medical genetists would do well to heed.

Many textbooks suggest that eugenicists were guilty of an astoundingly simple 
mistake. According to conventional accounts, enthusiasts about eugenics thought 
they could eliminate mental deficiency by segregating or sterilizing affected 
individuals. But a basic understanding of the Hardy-Weinberg principle suffices to destroy that illusion.
  Eugenicists in the 1910s and 1920s attributed most mental defect to a 
recessive Mendelian factor (or in today's parlance, allele ). But it is clear 
that if a trait is rare, most deleterious genes will be hidden in apparently 
normal carriers. Selection against those actually affected will thus be 
ineffective. Tables and formulas in many general biology and genetics textbooks 
serve to make the point that hundreds of generations are required before a rare 
deleterious trait would disappear.
  It is true that many eugenicists were muddled about genetics. But what about 
the host of respected eugenicists, such as R. A. Fisher in the United Kingdom,
Erwin Bauer in Germany, Herman Nilsson-Ehle in Sweden and Edward Murray East in the United States, who championed eugenics long after the implications of the 
Hardy-Weinberg principle were understood? The insight that selection is slow 
when genes are rare originated in 1917 and was popularized in the 1920s by 
J. B. S. Haldane in the United Kingdom and H. S. Jennings in the United States. 
Yet in the 1920s and 1930s, nearly all geneticists, including those traditionally characterized as opponents of eugenics, took it for granted that 'mental 
defectives' should be prevented from breeding. Moreover, the geneticists who 
first discussed the social implications of the Hardy-Weinberg principle did so 
in an effort to expand the scope of eugenics, not demonstrate its futility.

Invisible danger
In his 1917 essay ''Hidden Feeblemindedness'', the Harvard geneticist East 
lauded efforts to cut off the stream of ''defective germplasm'' through 
segregation or sterilization of the affected. But East thought that the primary 
danger lay elsewhere, in the vast mass of invisible heterozygotes.
  East had been strongly influenced by the psychologist Henry H. Goddard, author 
of The Kallikak Family: A study in the Heredity of Feeble-Mindedness, a 
chronicle based on data collected by a fieldworker who traced family members and assessed their mental and moral state. Many family members were of course dead 
or could not be located. Their mentality and character were assessed on the 
basis of heasay. Judgements of both the living and dead were swift and 
subjective.
  Two years later, in 1914, Goddard published Feeble-Mindedness: Its Causes 
and Consequences, in which he discussed the meaning of the Kallikak data for 
theories of inheritance. He argued that '' normalmindedness'' is a dominant 
trait inherited in a Mendelian fashion; an individual lacking the factor for 
normal mentality would be feebleminded -- ''incapable of performing his duties 
as a member of society in the position of life to which he is born''.
  The recessive theory of mental defect was widely accepted by Mendelians. The Cambridge geneticist R. C. Punnett spoke for many when he wrote that no one 
''who has studied the numerous pedigrees collected by Goddard and others 
[could] fail to draw the conclusion that this mental state [that is, 
feeblemindedness] behaves as a simple recessive to the normal''. Charles 
Davenport did note the illogicality of expecting a socially defined trait to be 
inherited as a simple Mendelian recessive; he though there were many different 
(and separately inherited ) mental deficiencies. A few geneticists also 
contested Goddard's claim that feeblemindedness was caused by a single 
Mendelian factor. But these were minor quarrels. With the exception of Thomas 
Hunt Morgan, who argued that much of the behaviour associated with 
feeblemindedness arose from ''demoralizing social conditions'', no Mendelian 
geneticist before the 1930s rejected Goddard's claim that social deviance was 
largely due to bad--recessive--heredity.
  In 1912 Davenport offered the following advice: Prevent the feebleminded, 
drunkards, paupers, sex offenders, and criminalistic from marrying their like 
or cousins or any person belonging to a neuropathic strain. Practically it 
might be well to segregate such persons during the reproductive period for one generation. Then the crop of defectives will be reduced to practically nothing.
At the time, such predictions were common. Even without the benefit of Hardy-Weinberg, East realized that they were wrong. The ''real menace'' 
of the feebleminded, he argued, lay in the huge heterozygotic reserve, constituting 
about seven per cent of the US population, or one in every fourteen individuals. 
East sounded an alarm: ''Our modern Red Cross Knights have glimpsed but the 
face of the dragon.''

A question of time
His point was echoed by Punnett. For his influential Mimicry in Butterflies 
(1915 ), Punnett needed to know how fast a Mendelian factor would spread 
through a population. His Cambridge mathematics colleague, H. T. J. Norton, 
prepared a table displaying the number of generations required to change the 
frequency of completely dominant or recessive factors at different selection 
intensities. Punnett called attention to the table's implications for eugenics. 
Policies aimed at the affected, he argued, would take a distressingly long time 
to work. The Hardy-Weinberg formula indicated that more than ten per cent of 
the population carried the gene for feeblemindedness. With G. H. Hardy's help, 
he also estimated the rate  at which a population could be freed from mental 
defects by segregating or sterilizing the affected. Even under the unrealistic 
assumption that all the feebleminded could be prevented from breeding, it would 
take more than 8,000 years before their numbers were reduced to 1 in 100,000, 
given Goddar's estimate that about 3 in 1,000 Americans were genetically 
feebleminded. Punnett concluded that eugenic segregation did not, contrary to 
his initial belief, seem hopeful.
  Punnett, who served with Fisher on the Council of the Cambridge University 
Eugenics Society, did not intend to provide an argument against eugenics. In 
fact, he explicitly endorsed both East's scientific point and his policy 
proposals. Like East, he aimed to convince his readers of the need to identify 
the carriers of defective genes. ''Clearly if that most desirable goal of a 
world rid of the feebleminded is to be reached in a reasonable time,'' he 
asserted, ''some method other than that of the elimination of the feebleminded 
themselves must eventually be found.''
  According to Fisher, Punnett's goal was subverted by opponents of eugenics, 
who seized on his table to argue that segregation and sterilization worked too 
slowly to justify the effort. In a 1924 article, ''The Elimination of Mental 
Defect'', Fisher argued that Punnett's calculations obscured the fact that 
selection would initially be rapid. And for all practical purposes, he noted, 
that is what matters. Even under Punnett's unrealistic assumptions of a single 
gene for mental defect and of random mating, he argued, substantial progress 
could be achieved in the first few generations if affected individuals were 
prevented from breeding. In the first generation alone, the reduction would be 
more than 11 per cent.
  Notwithstanding some technical differences, Jennings, Punnett and Fisher 
agreed that mental defectives should be prevented from breeding. Jennings, who
 is sometimes portrayed as an opponent of eugenics, asserted that a gene that 
produces feeblemindedness ''is the embodiment, the material realization of a 
demon of evil; a living self-perpetuating creature, invisible, impalpable, that
blasts the human being in bud of leaf. Such a thing must be stopped wherever it 
is recognized.'' Fisher diverged from Punnett and Jennings (who asserted as 
late as 1930 that feeblemindedness was ''the clearest case'' of a recessive 
single gene defect ) only in claiming that the affected tended to mate with 
each other and that the trait was multifactorial. All agreed that the incidence 
of mental defect could be reduced by at least 10 per cent in the first 
generation and by 19 and 26 per cent by the second and third generations 
respectively. Even Haldane, who regarded compulsory sterilization ''as a piece 
of crude Americanism'', thought it ''would probably cut down the supply of 
mental defectives in the next generation by something of the order of 10 per 
cent''.
  Why were the estimates so high? It is often said that eugenics was based on a 
mistake about the efficacy of selection against rare genes. But few geneticists 
made this error, at least after 1917. Feeblemindedness was not considered to be 
rare. Indeed, the raison d'�tre of the eugenics movement was the perceived 
threat of swamping by large and rapidly growing class of mental defectives.
  According to eugenicists, the number of physical and mental defectives would 
once have been kept in check by natural selection. But in civilized societies, 
it seemed that the process had practically ceased. Medicine and public charity 
now kept the 'unfit' alive. Worse, these failures were now reproducing faster 
than their betters. In the first three decades of this century, a raft of 
studies seemed to demonstrate the high fertility of the feebleminded. For 
example, the British Royal Commission on the Care and Control of the Feeble-
Minded reported in 1908 that defectives averaged seven children, normal couples 
only four. In the United States it was commonly believed that between 300,000 
and 1,000,000 people were feebleminded as a result of genetic defects, the 
figures tended to increase as mental tests came into wider use. In 1912, 
Goddard tested New York City schoolchildren and estimated that two per cent 
were probably feebleminded. The results of tests given to army recruits during 
the First World War were even more dramatic: they indicated that 47.3 per cent 
of the white draft and 89 per cent of the black draft were feebleminded.
  Contemporary textbook examples of the futility of eugenics often mention 
Tay-Sachs disease, phenylketonuria and albinism. Selection against such 
conditions is certainly futile. But they are usually rare and their effects 
either lethal or minor. Victims of Tay-Sachs and (with a few exceptions ) 
untreated phenylketonuria do not leave offspring. Albinos and treated 
phenylketonuriacs reproduce, but these conditions are not disabling. And with 
regard to the potential increase in genes for treatable genetic diseases, the 
Hardy-Weinberg argument is relevant. Most single-gene disorders are extremely 
rare and would spread slowly in the population. Moreover, some disease genes 
may be maintained at high frequency because of heterozygote advantage; although
 this is difficult to demonstrate, some geneticists believe it explains not 
only the well-confirmed case of sickle-cell anaemia, but also cystic fibrosis 
and Tay-Sachs (in Ashkenazi Jews), among other diseases. Selection here is 
futile.
  But these arguments do not apply to the early eugenics movements. Given 
widely shared assumptions about the incidence and causes of feeblemindedness, 
eugenic policies could be expected to reduce substantially the number of 
affected individuals. In the famous case of Buck v. Bell (1927 ), decided in 
the wake of US Army mental tests, Justice Holmes upheld the Virginia 
sterilization law ''to prevent our being swamped with incompetence''. 
Commenting on this passage, one author remarks that '' what such reasoning 
fails to take into account is that... sterilization will have almost no effect 
on the frequency of the disease''. The point is illustrated by a recessive 
disorder affecting 1 in 40,000 individuals.
In any case, geneticists in the 1920s would generally have favoured eugenic 
policies whatever their exact effect. Most would have assented to Jennings's 
claim that ''to stop the propagation of the feebleminded, by thoroughly 
effective measures, is a procedure for the welfare of future generations that 
should be supported by all enlightened persons. Even though it may get rid of 
but a small proportion of the defective genes, every case saved is a gain, is 
worthwhile in itself''.
Like Jennings, Lancelot Hogben is often portrayed as an opponent of eugenics. 
Hogben did criticize some advocates of sterilization for exaggerating both the 
extent of the problem and efficacy of their solution. He also argued that the 
fact that we cannot do everything ''is not a valid reason for neglecting to do 
what little can be done''. His point was echoed by E. G. Conklin, who, like 
Jennings and Hogben, criticized some eugenic proposals. But Conklin approved of 
the segregation or sterilization of the feebleminded. He asked of the American 
Eugenics Society's proposed sterilization policy: '' Can any serious objection 
be urged to such a law ?''.
  Punnett, East, Fisher, Jennings and even Haldane made roughly the same 
estimates of the speed and scope of eugenic selection. But the facts did not 
speak for themselves. They required interpretation in the light of other 
assumptions and goals. Thus Haldane opposed sterilization, arguing that ''with 
mental defects as with physical defects, if you once deem it desirable to 
sterilize I think it is a little difficult to know where you are to stop''. 
This is a powerful argument. But it is a social, not a scientific one. Lionel 
Penrose was the most vehement geneticist critic of eugenics. An expert in the 
genetics of mental deficiency, he stressed the heterogeneity of its causes and 
the modest influence of eugenic measures in reducing its incidence. But his 
main argument was ethical. Penrose maintained that the best index of a society's 
health is its willingness to provide adequate care for those unable to care for 
themselves. 
  The Hardy-Weinberg theorem meant different things to different people. For 
example, Curt Stern once remarked: ''To state that reproductive selection 
against severe physical and mental abnormalities will reduce the number of 
affected from one generation to the next by only a few per cent does not alter 
the fact that these few per cent may mean tens of thousands of unfortunate 
individuals who, if never born, will be saved untold sorrow''. It may not even 
matter if the reduction in absolute number is minuscule; the rate of selection 
is immaterial if one assumes with Jennings that the ''prevention of propagation 
of even one congenitally defective individual puts a period to at least one 
line of operation of this devil. To fail to do at least so much would be a crime''.
  Many advocates of sterilization employed a loose definition of 
'feeblemindedness', accepted Goddard's defective data and logic or assumed 
that ''it would be possible at one fell stroke [to] cut off practically all of 
the cacogenic varieties of the race. But it was possible to recognize all these 
flaws and still remain a eugenicist. After 1920, it was well understood that 
most genes for mental defects would be hidden in apparently normal carries. 
For most geneticists this seemed to be a good reason to widen eugenic efforts 
rather than abandon them.
  This implication makes sense in the light of social values. In 1918 Popenoe 
and Johnson wrote that ''so few people would now contend that two feeble-minded 
or epileptic persons have any 'right' to marry and perpetuate their kind, that 
it is hardly worth while to argue the point''.
  Politics changed these assumptions. During the 1940s and 1950s, many 
geneticists tried to distinguish the race- and class-biased eugenics of the 
past from a new eugenics that focused on disease. But attempts to distinguish 
good from bad eugenics were ultimately unsuccessful. Nazi atrocities gave 
eugenics of any kind a bad name and produced a backlash against the view that 
the state had a legitimate interest in who reproduced.
  That reproduction should be a private matter was strongly reinforced by a 
trend towards respect for patients' medical rights, the development of a broad 
jurisprudence of privacy and the rise of feminism. By the 1960s, reproductive 
autonomy had become a dominant cultural value. This was a far cry from the 
assertion of a 1914 committee of the American Breeders Association: ''Society 
must look upon germ-plasm as belonging to society and not solely to the 
individual who carries it''. A change in values, and not the progress of 
science, explains why few Swedens would now agree with the 1936 commission that 
criticized  as ''extremely individualistic'' the notion that individuals have 
their right to control their own bodies.
  It is often said that support for eugenics declined in the 1930s as its 
scientific errors were exposed. But the eugenics movement grew stronger during 
the Depression. In the United States, the number of sterilizations increased. 
Sterilization was legalized in Germany ( 1933 ), British Columbia, Canada 
( 1933 ), Norway ( 1934 ), Sweden ( 1934 ), Finland ( 1935 ), Estonia (1936 ) 
and Iceland (1938 ), Denmark, which in 1929 had legalized 'voluntary' 
sterilization, permitted its coercive use on mental defectives in 1934. These 
laws were generally applauded by geneticists.
  How then do we account for the popularity of the claim that eugenics was 
based on technical error? We suggest two related reasons. In the first quarter 
of this century, nearly all geneticists were enthusiastic supporters of a 
movement that is now generally held in contempt. In Germany, not one geneticist 
criticized the interwar eugenics movements. After the Nazi came to power, 
genetics was invoked on behalf of even more extreme measures of racial 
purification. Nevertheless, most of Germany's leading geneticists, including 
those who before 1933 had criticized antisemitism, actively helped to build the 
racial state. They served on important commissions, provided opinions on racial 
ancestry and participated in the drafting of racial laws. More than half of all 
academic biologists joined the Nazi Party, the highest membership rate of any 
professional group. In other countries, too, eugenicists promoted policies such 
as immigration restriction that reflected strong class and racial biases. So 
the history of the field is the source of some embarrassment (and 
defensiveness ). It is far more comforting to think that eugenics' decline was 
also due to geneticists. The myth rights the historical balance.

Backdoor eugenics
The claim also enables textbook writers and college teachers to avoid 
controversial issues. If eugenics is assumed to have rested on a technical 
error, it no longer raises thorny ethical questions. Geneticists can therefore 
condemn eugenics without questioning any of the aims of genetic testing. As 
Arthur Caplan points out, when the state of California ruled that screening for 
maternal serum a-fetoprotein should be offered to all pregnant women, it did so 
''in the hope that some of those who are found to have children with neural 
tube defects will choose not to bring them to term... thereby preventing the 
state from having to bear the burden of their care''. There is a similar cost-
benefit reasoning in the 1990 guidelines of the International Huntington 
Association and the World Federation of Neurology, which deem it acceptable to 
refuse to test women who ''do not give complete assurance that they will 
terminate a pregnancy where there is an increased risk'' of Huntington's 
disease. Those who made this recommendation certainly did not think they were 
promoting eugenics. Assuming that eugenics is dead is one way to dispose of 
deep social, political and ethical questions. But it may not be the best one.



Importance, P. Swinnnerton-Dyer, 19 January, C, Gen

The importance of academic freedom
The principle of academic freedom is an inalienable right in any civilized 
country. How should the state support academic research without interfering 
with it?

One of the problems in writing about academic freedom is that the phrase is 
much misused, sometimes out of sheer woolly-mindedness but more often by those 
who wish to conceal dubious actions under a high-minded disguise. This article 
will be based on the following definition: Academic freedom is the right of any 
scholar, within his field of expertise, to think about and to research on any 
problem that interests him, to make public his conclusions, and to teach what 
he sincerely believes to be true. A more authoritative definition is that put 
forward by Lord Jenkins, the Chancellor of the University of Oxford, and now 
enshrined in British law; it claims for academics The freedom within the law to 
question and test received wisdom, and to put forward new ideas and 
controversial or unpopular opinions without placing themselves in jeopardy of 
losing their jobs or privileges they may have at their institutions.
  These definitions have much in common, but one of the significant differences 
between them needs to be emphasised because it relates to the basis on which 
academic freedom is claimed. On my definition, which is more restrictive of the 
two, academic freedom is confined to experts addressing matters within their 
own expertise; it exists because experience has shown that to allow experts to 
challenge received wisdom is a major engine of progress. On the broader 
definition, academic freedom extends to any academic addressing any topic; and 
the claim is that it is a privilege that universities have had since their 
origin in mediaeval times. Historically, this is a doubtful claim; and in any 
case, a privilege that is claimed as being entrenched in history is asking to 
be challenged.

Defence
If one has to defend academic freedom, it does matter which definition one 
takes and which justification one uses. It is easy to show by examples that 
academic freedom in the narrower sense does promote material progress and is 
therefore desirable, other things being equal; but this does not mean that it 
is entitled to override all constraints. And indeed, academic freedom is in 
practice not absolute, though it does allow academics to do things that 
ordinary people could not lawfully do. One example will suffice. In most 
industralized countries, biologists are allowed to conduct experiments on 
animals that would lead to criminal charges for cruelty if anyone else did 
them; but a biologist is allowed to do so only if he or she can convince a 
mixed panel of other biologists and laymen that the problem being addressed is 
important and that there is no other way to make progress on it. More 
generally, there are in most advanced countries restrictions on experiments that 
are morally offensive or might pose environmental dangers. To my mind these are 
acceptable constraints on academic freedom; but I find it hard to think of any 
other constraints that are justifiable. And if a constraint on academic freedom 
is undesirable, it is not rendered any the less undesirable by being 
imposed by legislation. In particular an academic should be free to express 
views on a matter on which he or she is expert, regardless of any offence they 
may cause.
  But questions relating to academic freedom are not easy, and it is a mistake 
to think about them in terms of black and white. Academic freedom is a good 
thing, and so therefore are the structures needed to ensure it; but there are 
other good things that are incompatible with extreme forms of academic freedom,
 and so there need to be constraints on it. As the poet Tennyson said: ''The 
old changeth, yielding place to new, and God fulfils himself in many ways. Lest 
one good custom should corrupt the world''. But however extreme the claims of 
some of the defenders of academic freedom, there is at present little danger 
that academic freedom will be taken to excess; the risk is rather that it may 
be dangerously weakened.
  Academic freedom has powerful enemies, and its defence involves constant 
vigilance. These enemies fall into three groups: those who think they represent 
an authority superior to reason; those who find some truths emotionally 
unacceptable; and what might be called the enemy within. Let me give some 
examples.
  Those who work in Roman Catholic universities have to be very cautious about 
challenging doctrines of that church. This is no longer a curb on science, as 
it was in the days of Galileo; but it is still a considerable constraint on 
theologians and philosophers. Again, only a generation ago, any taint of 
Marxism put one at risk of dismissal from many US universities, and the fate of 
a Russian biologist who disagreeded with Lysenko -- and therefore implicitly 
with Hegel, whose views were sacrosanct because they provided the philosophical 
basis of Marxism -- was likely to be even more unpleasant.
  The witch-hunt against Marxism is over, but in the United States there is a 
new generation of witch-hunters: the advocates of political correctness. Their 
position is that there are many things that may be factually true but which 
ought to be suppressed because they induce traumas in the underprivileged. Most 
of these uncomfortable facts relate to differences between races or between 
sexes. But such an attitude goes well beyond the lunatic fringe. I once heard 
Keith Joseph, one of the two best ministers of education Britain has had since 
the Second World War, accuse a certain economist of ''digging up left-wing 
facts''. Again, many countries have episodes in their history of which the 
popular view, though wrong, is so tenaciously held that a scholar who tries to 
revise it is at considerable risk. One such example is the history of France 
under the German Occupation.

Academic duty
By the 'enemy within' I mean those who say that academic freedom is an 
unchallengeable right and deduce from this that they are not answerable to anyone 
for the way in which they carry out their duties -- or even for their failure 
to carry them out at all. Let me give an example, and by no means the most 
extreme that I could have chosen. When I was a young academic at Cambridge, a 
senior colleague in another faculty pinned a notice to his lecture-room door 
reading: ''I have lost interest in the subject of this course, and shall not be 
delivering any more lectures on it.'' Needless to say, no action was taken, 
except that the examiners for that course were advised to be more lenient than 
usual; indeed, the whole episode was simply seen as an example of that 
eccentricity on which the older universities used to pride themselves. I think 
the reaction would be very different today.
  More generally, academic freedom does not diminish the duties of an academic.
 In this context, it is natural to ignore administration and divide the duties 
into those connected with teaching and those connected with research. So far as 
teaching is concerned, an academic's duty is to his or her pupils. One part of 
it is to present the truth as he or she sees it, but it goes much further than 
that. The academic has a duty to keep up with the latest research in his or her 
subject, so that teaching is up to date; when controversial issues are 
addressed, the academic has a duty to acknowledge clearly views other than his 
or her own; lectures must be interesting and suited to the students who 
actually attend them, rather than an idealized audience; and an academic must 
give a balanced account of the subject.

Research
There are important respects in which research is different. It is commonly 
said that academics have a duty both to teach and to do research. But even if 
that appears as a duty in their contracts of employment, it is a duty that 
cannot be enforced, for one good reason; not everyone can be a charismatic 
teacher, but anyone can teach well if he or she puts in enough effort, whereas 
the same is not true of research. Most scientists, in particular, reach a 
moment at which they know they will never have another worthwhile idea, and 
the only result of their continuing to do research after that point is to clog 
up the learned journals with papers that no one should be expected to read. 
That may be the moment to write a book pulling one's subject together in a lucid 
way -- and certainly the need for such books greatly exceeds the supply -- but 
only a minority are capable of that. Thus simply to give up research at that 
point is wholly honourable.
Academic freedom is closely linked to tenure, which is the right to retain 
one's job unless dismissed for ''good cause'' as defined in the university's 
statutes and assessed by a judicial process. Roughly speaking, ''good cause'' 
is either academic misconduct or failure to carry out the duties of one's 
office. Tenure is of course a nuisance for management, both because it can make 
it financially impossible to refresh a tired faculty through new appointments 
and because it means that a subject in which student demand falls may be 
overstaffed for a generation. In Britain in the 1980s, tenure was vigorously 
attacked by the Thatcher government, because it looked like a version of the 
classic trades' union feather-bedding their training had taught them to 
detest, and because it seemed to exempt academics from the discipline of the 
market. Moreover, tenure did give rise to abuses, because academics were 
virtually never dismissed for good cause, however clear the case against them. 
In the event, tenure was abolished by legislation for all new appointments, 
though it was preserved for those who already had it; but universities continue 
to behave as if tenure still exists.
  The reason why tenure is so important is that it is the only way yet 
discovered of assuring academics that they can conduct their research and 
teaching without fear. Most universities, private as well as public, are 
crucially dependent on money from the state. If professor X is a voluble 
exponent of unpopular views, he will worry that behind the scenes the 
government will say to the university administration that it would be easier to 
increase the university's grant if professor X would keep his mouth shut. The 
government may have no intention of saying any such thing, but it will not be 
possible to convince professor X of that. If he is to continue to express his 
views without feeling under threat, it can be only because he knows that 
because he has tenure, the university has no way of silencing him. To quote 
from Conrad Russell's book on academic freedom: '' The point is not that 
academics may not be dismissed for their opinions; it is that they need freedom 
from fear that they might be so dismissed. The temptation to trim unpopular 
conclusions, to cut out the extra sentence which unambiguously spells out the 
provocative finding, is one to which most academics are not immune. The certain 
knowledge that they can only be dismissed for misconduct which in effect 
amounts to breach of contract is a necessary defence against this''. 
  If anything, the situation of an academic in a private university is 
potentially worse than in a public one; for a private university is more 
dependent on benefactors. It is not merely that a major private benefactor is 
likely to feel less inhibited about putting pressure on a university than the 
government would be; but the university itself is likely to believe that if it 
is known to, be a hotbed of unpopular views that will discourage potential 
benefactors.
  So far, I have been considering the academic freedom of academic individuals.
But the academic freedom of complete institutions raises equally important 
issues, and is at the moment the more active battleground. By the academic 
freedom of an institution I mean the right to decide what subjects to teach and 
how to teach and examine them; the right to appoint its own staff and to choose 
its own students; and ( within the limits of prudence ) the right to decide 
within the institution how its income shall be spent. This last right cannot be 
absolute, for some income will always be earmarked for particular purposes by 
the person or body that provided it. But it is crucial that not too large a 
proportion of income should be earmarked in this way.
  There is an old English saying that he who pays the piper calls the tune. 
Most universities are vitally dependent on state funding. This enables the 
government, if it chooses, to impose its will on the university in considerable 
detail: to what extent is it entitled to do so? In this matter, different parts 
of Western Europe have very different traditions. In Germany, for example, most 
universities were funded by the state to provide well-trained graduates for the 
service of the state; and in France the higher-education system was refashioned 
by Napoleon for the same reason. In both cases, this carried with it extremely 
detailed external control, though this has been somewhat relaxed in recent 
years. By contrast, in England, Oxford and Cambridge had from the beginning a
large degree of independence; and most of the nineteenth-century universities 
were founded by groups of businessmen to provide trained middle management for 
local industries. So there was no tradition of state authority over 
universities; and the authority of businessmen diminished as their contribution 
to funding became less important. Moreover, at the time when the state took 
over the major responsibility for university funding, universities still had a 
prestige that they were not to lose until the student troubles of the late 
1960s; so state funding of universities was arranged through an intermediary body, 
to make it as difficult as possible for the state to interfere with the way universities were run.
  Restraint in exercising its powers was never a characteristic of the Thatcher 
government, and, among British institutions, universities seemed more ripe for 
reform than most. So the past 10 years have seen fierce fighting over how much 
autonomy the universities should retain, the decisive factor being whether they 
could be persuaded to reform themselves before they were reformed from outside. 
In the event, the outcome seems to me largely acceptable, though the 
preservation of a reasonable degree of autonomy has been a close-run thing and 
has needed a new approach to the relationship between universities and 
government.
  Thirty years ago, universities were seen as a national treasure; it was the 
government's duty to fund them adequately but not to enquire too closely what 
they did with the money. That was an acceptable ethos in the days when 
government finances were expenditure-led -- that is, the government listed all 
the good things it would like to do, added up the cost and then asked itself 
what taxes would be needed to pay for them. Nowadays the government decides 
what level of expenditure it can afford, taking political consideration into 
account, and then divides this sum among the many deserving claimants. Among 
the crucial concepts underlying decisions on expenditure are 'accountability' 
and 'value for money'; if higher education is to get a reasonable share of 
public money, it must pass these tests.
  This requirement does diminish academic freedom; the task is to reconcile the 
two as far as possible. In Britain, the key to this is the current system of 
funding universities. A crucial factor in making this system work is the 
existence of an intermediary body -- the Higher Education Funding Council ( HEFC ), which has inherited the role formerly played by the University Grants Committee.
 In consequence, the government itself takes decisions only about the system as 
a whole; splitting them up into decisions about individual universities is the 
major part of the work of the HEFC. Equally, recommendations to the government 
on what resources need to be provided come from the HEFC, which acts as an 
honest broker; it receives the bids of individual universities, but need not 
entirely believe them. Because money is so tight, any apparent unfairness in 
its division between individual universities would be much more bitterly 
resented than in earlier days. So the HEFC has had to develop a funding 
methodology that is both transparent and objective. That methodology is what I 
shall next describe.

Funding Methodology
The underlying philosophy is that the government grant to universities is no 
longer seen as a subsidy to preserve valuable institutions that could not 
otherwise survive. Instead, the government buys certain services from 
universities, essentially teaching and research. So far as teaching is 
concerned, the government determines a price per student, which varies from one 
subject to another but, within a given subject, is the same for all 
universities. In real terms, this price is reduced a little every year, to 
allow for the increased efficiency that the government assumes ( without much 
justification ) can be obtained without any loss of standards. The government 
also decides the total number of students it is prepared to fund, and their 
division between very expensive subjects ( human and veterinary medicine ), 
fairly expensive subjects ( other laboratory-based subjects ) and cheap 
subjects. In this it takes some account of the advice of the HEFC, but the 
decisions about how many student places to fund and what resources to provide 
for teaching are politically sensitive. The decisions in the past 2 years to 
reduce the number of funded places have produced a lot of disappointed parents, 
and must have contributed to the present unpopularity of the government; and of 
course it would be politically catastrophic if a university were forced into 
bankruptcy.
  In teaching, there is therefore a straightforward implied contract, which can 
be regarded either as being between the government and the university system or 
between the HEFC and individual universities. The government, acting in the 
national interest, buys so much teaching and the universities deliver it. It 
is easy to check, through the statistical returns every university has to make, 
that the universities are delivering at least the amount of teaching that they 
are being paid for. There remains the question of whether the teaching is of 
good enough quality. Originally the government took the view, in keeping with 
its 'market forces' philosophy, that if a university or a particular department 
taught badly then the news would travel back to schools and there would soon 
not be enough applicants to fill its places. More recently, it recognized the 
flaws in this argument and has set up a teaching assessment system. It is too 
soon to tell how effective this will be; but one must recognize that operating 
such a system is neither cheap nor easy, and there are problems here that still 
await a satisfactory solution.
  Teaching assessment is here to stay, at least until the government asks 
itself whether what it achieves is worth the considerable sums that it costs
 -- not to mention the distraction from more constructive activities. However, 
since it does not assess what it taught, but only how well it is taught, it 
can hardly be said to infringe the academic freedom of individual universities.
By contrast, in professional subjects there are learned societies that monitor 
the content of courses as well as the quality of teaching; and on the basis of 
this monitoring they could refuse to recognize particular university courses as 
professional qualifications. In practice they very seldom go as far as this, 
because to do so would render the course so unattractive to potential students 
that it would have to be closed down; but the threat enables them to enforce 
whatever changes they see fit. This is a real restriction on the academic 
freedom of universities; but it is generally accepted as being a proper one because learned societies are themselves expert academic bodies.
  The funding of research raises very different issues from that of teaching.
 Government funding of research in universities comes through two main 
channels: the research component of the block grant, which comes through the 
HEFC; and funding for specific projects, through the research councils. It is 
for government to decide how much money it can provide for the support of 
research across the system, and perhaps to give some indication of how it 
should be split between subjects -- though even this is dangerous ground. 
Certainly any decisions beyond these must be for the HEFC. It is reasonable to 
expect that the quality of teaching in a given subject will not vary enormously 
from one university to another; but there can be not doubt that the quality of 
research will vary enormously. Moreover, the amount of research of respectable 
quality that academics would like to do is far greater than any government can 
hope to fund. Within each subject, it must therefore be right to concentrate 
most of the research funding in those departments that are likely to provide 
the best value for money -- and the only plausible way of assessing this is to 
ask what their track record in research has been in the recent past. Thus it is 
essential for the HEFC to conduct regular assessments of the research quality 
of every department in every university -- in contrast with the teaching 
assessment, which is a peripheral part of the system. Fortunately, the 
assessment of research, which can be done from published work, is far easier 
than that of teaching. Any senior academic has a good idea of the quality of the 
departments in his or her subject across the country, the only real difficulty 
being to distinguish between those that are 'awful' and those that are merely 
'poor'.
  Within a given subject, the research resources attributed to a department 
depend on its research rating and its size. How one should measure size in this 
context is a complicated issue and not a very important one. There is no way 
of ensuring that, for example, 'excellent' in physics is comparable with 
'excellent' in history even if that statement is meaningful -- which I doubt. It 
is important that a subject should not attract more resources nationally as a 
result of its reviewing panel being kind-hearted. This is prevented by the HEFC 
deciding in advance of the review what amount shall be shared out between the 
departments in each subject. That decision involves a complicated feedback 
mechanism, based on estimated current research expenditure across the system 
in each subject.
  I have left to the end the crucial feature of this allocation formula, and 
the one that preserves for individual universities a reasonable degree of 
academic freedom. As I have described the funding methodology, the grant to 
each university is made up of lots of individual packets -- in effect, one for 
teaching and one for research for each department, though there are further 
complications that I have not mentioned. But the next stage is to add up all 
these amounts, and send the university a single cheque that it can spend as it 
chooses. In particular, it does not have to give each department the sum of 
money demanded by the formula; it may give some departments less to give others 
more. This freedom is likely to remain, because it is also a protection to the 
HEFC. No funding formula can ever be perfect without also being unacceptably 
complicated; but the potential unfairness in the individual packets should 
cancel out when added together.



Punishment, T.H. Clutton-brock, 19 January, RA, Bio

Punishment in animal societies
Although positive reciprocity ( reciprocal altruism ) has been a focus of 
interest in evolutionary biology, negative reciprocity ( retaliatory 
infliction of fitness reduction ) has been largely ignored. In social 
animals, retaliatory aggression is common, individuals often punish other 
group members that infringe their interests, and punishment can cause 
subordinates to desist from behaviour likely to reduce the fitness of dominant 
animals. Punishing strategies are used to establish and maintain dominance 
relationships, to discourage parasites and cheats, to discipline offspring or 
prospective sexual partners and to maintain cooperative behaviour.

'' Upon this a question arises: whether it is better to be loved than feared 
or feared than loved? It may be answered that one should wish to be both, but, 
because it is difficult to unite them in one person, it is much safer to be 
feared than loved, when of the two, either must be dispensed with... for love 
is preserved by the link of obligation which, owing to the baseness of men, is 
broken at every opportunity for their advantage; but fear preserves you by a 
dread of punishment which never fails.''
                                                      Machiavelli, The Prince.
Reciprocal altruism or positive reciprocity has been a major focus of 
theoretical and empirical research in evolutionary biology since Trivers first 
drew attention to its importance. In contrast, negative reciprocity 
( retaliatory infliction of fitness reduction ) has attracted little attention: 
theoretical research on aggression has mostly focused on questions concerning 
expected levels of expenditure conflicts, whereas empirical research has 
investigated the motivation, causation, development and distribution of 
aggressive behaviour. Yet individuals ( or groups ) commonly respond to 
actions likely to lower their fitness with behaviour that reduces the fitness 
of the instigator and discourages or prevents him or her from repeating the 
initial action. We refer to behavioural tactics of this kind as punishment, 
without implying either a conscious decision or a moral sense on the part of 
the punisher. Although ( human ) social scientists now commonly restrict the 
term to the imposition of penalties by those in authority, punishment is still 
widely used to refer to interactions between individuals in humans and our 
definition is similar to Grotius' ''the infliction of an ill suffered for an 
ill done''.
  As all forms of animal punishment presumably involve some cost to the 
punisher and benefits may be deferred, punishment is temporarily spiteful, in 
the same way that reciprocal altruism is temporary altruistic. However, in the 
longer run, punishment is a form of selfish behaviour which benefits the 
punisher because it reduces the probability that the victim will repeat a 
damaging action or will refuse to perform a beneficial one. In most cases, 
punishers benefit because victims are eliminated or forced to emigrate. 
Punishing tactics may also be learned: as punishers will often receive 
reinforcement from their opponents' responses, they will likely to repeat 
their behaviour.
  Evolutionary game theory models developed foe asymmetric animal contests 
confirm that punishing tactics will often be evolutionarily stable. Box 1 
considers the simple situation where two individuals differ markedly in 
fighting ability so that one can punish the other without fear or retaliation, 
whereas Boxes 2, 3 and 4 consider the more complex situation where individuals 
can retaliate to punishment, incorporating retaliation as a variant of the 
simple hawks-doves game. We initially focus on situations where a subordinate 
can choose whether or not to transgress against a dominant while the dominant 
can choose whether or not to punish them, subsequently considering cases where 
the roles are reversed and subordinates can choose whether or not to punish 
the transgressions of dominants. Box 3 shows that 'commonsense' evolutionarily 
stable strategies ( ESSs ), where dominant individuals punish subordinates who 
subsequently avoid transgressing, and where dominants transgress against 
subordinates without fear of punishment or retaliation, are the most likely 
states to develop.

Do animals punish each other?
Most theoretical investigations of animal conflict have focused on contests in 
which two individuals compete for a resource, but many aggressive interactions 
among social animals occur where resources are not immediately at stake. 
Individuals frequently respond to threats or attacks by threatening or 
attacking the initiator of the attack and quantitative analyses of aggressive 
interactions confirm that reciprocal aggression is common ( though individuals 
will also attack each other without obvious initial provocation ). In 
addition, dominant animals often respond aggressively to individuals that have 
failed to react in an appropriate fashion. For example, rhesus macaques that 
find sources of preferred food and do not give food calls announcing their 
find are more likely to be the target of aggression than individuals that give 
food calls. Subordinates that gain priority of access to preferred food 
sources are also likely to be attacked by dominants: in small groups of 
domestic horses, subordinates that are separated and fed within the sight of 
dominants are likely to be attacked on their return to the group. Though 
experimental evidence is lacking, many studies suggest that individuals gain 
from retaliatory aggression because it reduces the probability of recipients 
repeating potentially injurious actions. Sequences of reciprocal aggression 
may span hours, days, or even weeks, and are commonly associated with displays 
likely to attract the attention of other group members to the interaction. 
Punishers are typically dominant to the individuals they punish, though 
subordinates sometimes punish dominants at times when they cannot retaliate. 
The intensity of attacks appears to be related to the extent to which 
individuals have infringed each other's interests and to be modified by 
kinship, age and sex.
  In social primates, aggressive exchanges often involve kin of the principal 
protagonists. In vervet monkeys, adult females who have been displaced from 
food sources may seek out and attack their displacer's relatives. In macaques, 
members of different matrilineal groups ally with each other and individuals 
that have been displaced or attacked by members of another matriline commonly 
respond by attacking a vulnerable member of their aggressor's matriline. 
Attacks on a member of one matriline are commonly followed by retaliation 
against members of the aggressor's matriline by relatives of the victim.
  Retaliatory aggression may also involve unrelated allies. In savannah 
baboons, males commonly develop close 'friendships' with particular females, 
and individuals that have been involved in fights may seek out and threaten or 
chase their attacker's 'friend'; and vice versa, males may threaten or attack 
other males or females that attack or harass their ?friends' or may attack the 
friends of their aggressors. Where individuals respond to attacks on their 
allies with retaliatory aggression, aggressors sometimes show affiliative 
behaviour to their victim's kin immediately after an attack. For example, in 
pigtail macaques, aggressors are more likely to show affiliative behaviour to 
their victim's kin in the minutes following an attack than at other times. 
Similarly, in vervet monkeys, aggressive interactions are commonly followed 
by affiliative interactions with members of the opponent's matriline. 
  Punishment appears to be particularly common in five social contexts where 
pronounced conflicts of interest occur:
  The establishment and maintenance of dominance relationships. During the 
establishment of dyadic dominance relationships, two individuals frequently 
trade attacks or threads until one of them establishes a superior position. 
This is often followed by a period of consolidation when most attacks or 
threads are directed by a dominant at the subordinate. Subsequently, 
subordinates usually avoid contests, and mild threats by dominant individuals 
are sufficient to constrain their behaviour. However, from time to time, 
subordinates test dominants, probably because this allows them to check for 
changes in the dominant's fighting ability, which would permit them to reverse 
the relationship. Unsuccessful challenges commonly elicit attacks by the 
dominant, followed by a period of heightened sensitivity to any failure by 
subordinates to respond to mild threats.
  An example of the role of punishment in the establishment of dominance 
relations comes from studies of the development of aggressive interactions 
between chicks of the blue-footed booby, a species in which females are larger 
than males. Parents typically lay two eggs which hatch asynchronously. Older 
chicks establish their dominance by pecking and jostling younger chicks. Any 
serious challenge by the younger chick elicits fierce and determined 
escalation of these attacks by the older chick. Dominance relationships 
developed immediately after hatching commonly persist, even if relative sizes 
change: for example, though female chicks are substantially larger than males, 
first hatched males retain dominance rank ( and feeding priority ) over 
later-hatched sisters. 
  In social primates where a male's competitive ability depends partly on 
supportive coalitions with other group members, individuals are most likely to 
join coalitions against individuals that frequently join coalitions against 
them. In some primates, dominant males not only punish rivals for challenging 
their position but also punish their rival's supporters as well. For example, 
in chimpanzees, dominant males depend partly on the support of mature females 
to maintain their position. In a captive group of chimpanzees, de Waal 
describes how a younger male challenging an established dominant regularly 
threatened or attacked his rival's female supporters. His declining rival was 
unable to intervene and, eventually, the females transferred their support to 
the challenger. The frequency with which the new alpha male directed 
aggression at females then declined rapidly.
  Theft, parasitism and predation. In many animals, individuals 
surreptitiously remove resources from their neighbours or parasitize their 
investment in their progeny. In birds, neighbours commonly attempt to steal 
nesting material or food from each other or to dump eggs in their neighbours' 
nests. In some mammals, juveniles will try to sneak milk from lactating 
females other than the mother, whereas young males may persistently attempt 
to sneak copulations with females that are being guarded by mature males.
  With few exceptions, individuals respond aggressively to detected cheats or 
parasites. In birds, long chases occur though rapid escape is often possible. 
In mammals, attacks on detected parasites may be damaging or fatal: for 
example elephant seal pups that are caught attempting to steal milk from 
unrelated females are often badly bitten and sometimes killed. Similarly, 
harem-holding fallow bucks repeatedly chase persistent sneaks over long 
distances and attack them with their antlers if they can catch them. Here, 
too, sneaks are sometimes killed by harem holders.
  Similar attacks or threats are often directed at heterospecific parasites or 
even at predators. Both brood parasites, such as cuckoos, and predators are 
commonly the target of persistent mobbling; for example, side-striped jackals 
will persistently mob leopards that hunt in their territory, preventing them 
from successfully stalking ungulates ( personal observation ). Lions not 
uncommonly attack jackals and spotted hyenas that parasitize their kills, 
sometimes killing individuals. In many of these species, hosts and their 
predators or parasites are territorial, so that the same individuals encounter 
each other repeatedly. Although aggression directed at parasites or predators 
may have other benefits, one likely advantage is that it teaches individual 
predators and parasites to avoid repeating attempts on the same individual or 
in the same location.
  The establishment of mating bonds. In polygynous or promiscuous species, 
males sometimes attack females that refuse to associate with them. Territorial 
male red and fallow deer will prod straying females with their antlers. Male 
hamadryas baboons initially threaten females that stray with an eyebrow flash 
but if they fail to return immediately, will bite them on the neck. In rhesus 
macaques, males are most likely to attack oestrus females when the female's 
nearest adult neighbour is either subordinate to the attacker or of a 
different social group from the female. In chimpanzees, Goodall describes how 
males repeatedly attack females in the early stages of consort formation until 
they become more cooperative and follow the male closely. In all these cases, 
it seems likely that one of the principal benefits of aggression to males is 
that it reduces the risk of losing potential mating partners. Dominant males 
will also punish members of either sex that interfere with female consorts. 
Smuts describes two cases in which male baboons punished females dominant to 
their consort partner who attempted to interfere with their partners.
  Males also punish females that refuse their mating attempts. In social 
primates living in multi-male groups ( including savannah baboons, rhesus 
macaques and chimpanzees ), males sometimes attack females that reject their 
attempt to mate, hitting or biting them. Dominant male primates may also 
punish females for approaching, courting or mating with subordinates or males 
from other groups. Females are clearly aware of these risks: for example, 
female hamadryas baboons will hide from dominant males while being groomed by
subordinates or non-group males. In a captive group of chimpanzees, de Waal 
describes how females sometimes consistently refused to mate with subordinate 
males when the dominant male was present, but immediately mated with them when 
the dominant male was confined for the night. Attacks by males appear to 
increase the chance that females will subsequently mate with their aggressors. 
For example, male Japanese macaques that show aggression to females during the 
mating season were significantly more likely to mate than males that did not 
show aggression towards females. In captive lowland gorillas, females learn to 
present more frequently to more aggressive males whom they otherwise avoid.
  Several studies show that attacks by males can have substantial costs to 
females. Serious wounding is not uncommon: for example, anestrous female 
olive baboons are seriously wounded by males around once a year. Wounds may
subsequently lead to death, abortion or loss of subsequent reproductive 
performance. Males have been seen to kill females belonging to their harem or 
social group in several mammals, including elephant seals, red deer and grey 
langurs.
  Parent / offspring conflict. Conflicts of interest between parents and their 
offspring often occur over the duration or level of parental investment and 
parents often adopt punishing tactics adapted to modify the behaviour of their 
offspring. In vertebrates, females will bite, peck, kick or slap juveniles 
that persistently attempt to suck of solicit other forms of investment. In 
some cases, punishment can have considerable costs to offspring. In European 
coots and moorhens, parents usually feed whichever offspring is closest to 
them but discourage 'geedy' young that persistently follow them closely after 
they have been fed by picking them up and shaking them. On occasion, persistent
young are even killed. Although killing offspring is presumably non-adaptive, 
it may generally be to the parents' advantage to teach its progeny not to 
attempt to monopolize resources by lowering the benefits of persistent 
attempts to feed.
  The enforcement of cooperative behaviour. Compared to other explanations of 
cooperative behaviour, relatively little attention has been paid to the 
possibility that cooperative behaviour occurs because dominant breeders coerce 
subordinates to assist them. Previous model of the punishment of defectors in 
human groups have assumed that punishment and its benefits are shared equally 
among group members, an assumption that weakens the benefits of punishing 
because it favours individuals that cooperate but do not punish. In practice, 
dominant animals are usually more likely than other group members to punish 
non-cooperators and commonly gain a disproportionate share of reproduction. 
These inequalities substantially strengthen the benefits of punishing 
defectors and cooperation can often be maintained by punishment, especially if 
individuals learn responses for specific dyads.
  Enforced cooperation is probably common in animal societies. Punishment 
frequently occurs in dyadic interactions between cooperating individuals: 
chimpanzees, for example, form supportive coalitions to gain access to 
resources and will attack allies that fail to support them in competitive 
interactions with third parties. In some primitively eusocial Polistes wasps, 
dominant workers may inherit the queen's role, and conflicts of interest 
consequently arise between queens and dominant workers over how much time and 
energy the latter should invest in rearing sibs. Queens are regularly 
aggressive to inactive workers, chasing, biting, grappling or bumbing them, 
and experimental removal or cooling of queens rapidly lowers the level of 
worker activity. Similar behaviour occurs in the primitively eusocial naked 
mole rat where queens direct repeated aggression at lazy workers. And in 
superb fairy wrens, helpers that are experimentally removed from the group at 
times when the group is raising young ( when they play a major role in 
providing food ) are usually attacked and harassed by the dominant male on 
their return, whereas helpers removed during non-breeding season are never 
attacked.
  In a few cases, subordinate members of cooperatively breeding groups punish 
dominant individuals that infringe their fitness. For example, in primitively 
eusocial paper wasps Polistes fuscatus, colonies are founded by small groups 
of sisters that initially cooperate to build and defend the nest and feed the 
young. Queens are ranked in a dominance hierarchy and the dominant queen 
regulates the reproduction of subordinates by eating around a third of their 
eggs. Excessive egg-eating by the dominant queen ( mimicked by experimental 
removal of subordinates' eggs ) elicits an increase in the frequency of 
aggression directed at the dominant by her subordinate sisters.

Conclusions
Theoretical studies confirm that where individuals live in stable social 
groups, conflicts of interest between group members are likely to lead to the 
evolution of punishing tactics and negative reciprocity. Empirical research on 
social vertebrates shows that a high proportion of aggressive interactions 
involve reciprocal threats or attacks between individuals and their kin or 
allies. By punishing actions that infringe their interests, dominant animals 
teach subordinates to behave in a fashion that increases ( or avoids 
reducing ) the dominant's fitness. As selection is likely to favour 
subordinates who test dominance relationships from time to time, punishment is 
likely to be a continuing feature of dominance relationships. Punishment is 
commonly used to constraint the demands of offspring; to establish dominance 
relationships and mating bonds; to persuade reluctant helpers to cooperate; 
and to discourage thieves, cheats, parasites and even predators. Punishers are 
generally dominant individuals who can punish at little cost -- although 
subordinates may sometimes wait for favourable opportunities to punish 
dominant individuals at times when they cannot retaliate effectively.
  Punishing tactics have important consequences for other aspects of social 
behaviour. Both punishers and their victims may minimize the risk of 
retaliatory aggression by ritualized reconciliative behaviour, involving 
affiliative behaviour or close physical contact shortly after an aggressive 
interaction. For example, pairs of long-tailed macaques that were given the 
opportunity to show reconciliative behaviour after induced conflicts, 
subsequently showed closer proximity to each other than pairs that had been 
experimentally prevented from reconciling. Following conflicts, both 
contestants may seek reconciliation with members of each others' kin groups, 
while their relatives may also seek reconciliation with each other.
  In many social vertebrates, dominant individuals will also intervene to end 
aggressive exchanges, possibly because repeated aggression between other group 
members can reduce their fitness. For example, in cercopithecine monkeys, both 
dominant males and dominant females will intervene in quarrels between other 
group members. In captive chimpanzees, intervention on the side of weaker 
individuals may help high ranking males to consolidate and maintain their 
social position: rising males initially support winners in quarrels between 
other group members but switch to supporting losers as soon as they reach the 
alpha position. Subordinate individuals may also intervene to encourage 
reconciliation: for example, females will apparently help to bring rival males 
together after a conflict by grooming or presenting them. They probably have a 
direct interest in doing so, for females are commonly attacked during 
prolonged aggressive interactions between males.
  It is clear that analogies exist between punishing tactics in social 
animals and retaliatory aggression among humans. Retaliation is a common 
feature of aggressive interactions between individuals as well as between 
groups: obvious examples include retaliatory expulsions of diplomats and tit 
for tat sectarian killings. In some tribal societies, individuals are 
permitted to punish minor infringements of their interests themselves, while 
the punishment of more severe offences is either carried out by community 
action or is delegated to particular individuals, an arrangement likely to 
limit the extent of negative reciprocity and minimize disruption within the 
group. Even here, there are parallels with the aggressive behaviour in animal 
societies: in some social Hymenoptera, group members may cooperate to punish 
or prevent defection, whereas in some social primates, subordinate individuals 
will draw the attention of dominant animals to actions by other group members 
that are likely to infringe their interests.
  Although negative reciprocity provides a theoretical framework for research 
on the evolution of aggression in social animals, there are many questions 
that have yet to be explored. How does the involvement of other individuals 
affect punishing strategies? What is the optimal magnitude of punishment? 
What tactics should individuals adopt? Should they exact retribution 
proportional to the damage to their interests? Should they escalate -- like 
opposing regiments during the First World War, who punished breaches of 
informal peace pacts on the basis of two or three for one. Or should they 
de-escalate, thereby signalling a readiness to terminate the conflict?
  Firm experimental evidence that individual animals punish other group 
members whose behaviour is likely to depress their fitness and that 
punishment reduces the risk of repetition is now badly needed. The factors 
affecting the magnitude and frequency of punishments and the effects of 
asymmetries between contestants need to be explored. As reciprocal attacks are 
often widely spaced in time and may be modified by intervening interactions 
between contestants and other individuals, these questions pose methodological 
as well as conceptual challenges for behavioural scientists. Their solution 
will require a novel mixture of quantitative ethology, cognitive psychology 
and evolutionary biology.



Fifty years, McG. Bundy, 3 August, C, Gen

Fifty years in the shadow of the bombing
The world is still sharply divided over Hiroshima and Nagasaki. But we have 
since created a tradition of non-use of nuclear weapons that is a much better 
guide of the future than were those fateful bombings 50 years ago.

This summer we are remembering the bombings of Hiroshima and Nagasaki. These 
events fully deserve remembrance, but their fiftieth anniversary next week 
should also lead us to reflect on the extraordinary fact that what happened 
then has not happened since. The bombings themselves have a double meaning, 
of deliverance and destruction, that can still sharply divide people. But 
there is not such division over the subsequent 50-year world-wide record of 
non-use of the nuclear weapons. We may attribute the result to different 
causes: this or that state has had effective deterrent strength; leaders with 
the power to fire weapons have shown statesman-like restraint; public opinion, 
at home or abroad or both, has held them back; or mankind has been lucky. 
These varied interpretations have claims on our attention, but it is right to 
begin with the recognition that the division over the attacks on Hiroshima and 
Nagasaki has no-parallel when it comes to the record of non-use in the 
succeeding half-century; we are all glad about that achievement.
  What makes for hot debate about the two bombs dropped in August 1945 is that 
people give differently weighted answers to the question of what would have 
happened without them. One side -- among Americans the majority side -- still 
emphasises that the use of the new weapon ended an appalling war that 
desperately needed ending. It can cite the detailed record of the rapid 
political interaction set off by the Hiroshima bomb. Within three days, three 
interlocking realities became apparent that led to surrender and peace; first 
the bomb existed and could be used again and again; second, the Russians would 
not be mediators but a new enemy; and third, the Americans would allow the 
institution of the Japanese emperor to be continued after surrender. No 
historian can tell us definitively how to weigh the impact of each of these 
three realities on the decision-making of the Japanese government, but there 
can be no doubt of their combined effect in producing quick surrender. It is 
also an open question how much this process shortened the war, but certainly a 
wholly genuine fear of heavy and sustained battlefield losses in the minds of 
US decision-makers. The more optimistic postwar estimates of some analysts do 
not tell us much. The broken Japan that they examined was not the one that US 
leaders or last-ditch Japanese generals had in mind in July and early August 
1945. At that climatic moment, the new bomb was seen in Washington as an 
enormously promising instrument of early victory over a ferociously determined 
enemy.
  On the other hand, modern defenders of the decision to use the new weapon 
cannot usually defend the choice of large cities as targets. The best they can 
do is point out that the bombing of cities -- in particular the use of 
deliberately massive incendiary attack -- was already an established, accepted 
and highly destructive way of war before Hiroshima. The largest 'conventional' 
raid, on Tokyo in March 1945, was about equal to Hiroshima in its human cost, 
and such bombing of major cities continued throughout the spring and early 
summer of that year.
  The actual course from nuclear decision to Japanese surrender has been 
largely neglected in this year's retrospective debate. A narrow but intensely 
felt disagreement did however erupt over the proposal to display Enola Gay, 
the plane that dropped the Hiroshima bomb, in a Smithsonian exhibition 
intended to commemorate the end of the war. Those who celebrate the bomber's 
mission and crew remember a great war swiftly ended by an extraordinary 
combination of nuclear technology with aircrew skill and courage. Those who 
focus on a city demolished and a hundred thousand people indiscriminately 
killed denounce the deed and those who ordered it. That the war thus ended had 
itself already become that kind of war -- as bad in its overall inhumanity as 
Hiroshima -- is largely ignored on both sides. Yet the inescapable reality is 
that this kind of war was what had to be ended. 
  By this test, the bombings of Hiroshima on 6 August and Nagasaki on 9 
August were a success. The Hiroshima bomb shocked the Russians into an 
immediate entry into the war, and these two events together shocked the 
Japanese government into a public offer of surrender on the single condition 
that the institution of the emperor be preserved. When it became clear that 
this condition was the only obstacle to victory and peace, the reaction of US 
troops in the Pacific was decisive. The headline in the New York Times on 11 
August said it all: ''GI's in the Pacific Go Wild With Joy: 'Let' em keep 
Emperor,' they say''. The political doubts in Washington disappeared, and 
surrender terms were promptly agreed. As this history became known, it was 
recognized on all sides that the emperor himself played a decisive role in 
ending the war, deciding for peace when his ministers remained divided, and 
accepting the US condition that his own continued role must have democratic 
support. The emperor, more than any other one man, ended the war. But the 
situation in which he could become decisive was precipitated by the bomb on 
Hiroshima. The second bomb, on Nagasaki, strengthened the forces for surrender 
in the final Japanese debate. The bomb alone surely did not end the war, but 
just as surely it did precipitate the actions, Russian, Japanese and American, 
that together brought about surrender.
  Later critics neglect the reality that by the summer of 1945, the earliest 
possible ending of the war was a totally commanding purpose for Americans. If 
President Harry Truman had held back from any available means of rapid victory,
he would have turned his back on his own strong convictions, abandoned the 
repeatedly proclaimed policy of his great predecessor, Franklin D. Roosevelt, 
and gone against the deeply held convictions of his countrymen, especially 
those in the armed forces still at risk. It is equally natural that millions 
of others -- and especially the citizens of Hiroshima and Nagasaki -- remember 
most what happened to those two cities. These contrasting memories are 
reflected in the divided US sentiment on what to emphasise in anniversary 
commemorations of the end of the war, and the division is both understandable 
and honourable. ( I should declare my own interest -- I learned of Hiroshima 
when the infantry regiment of which I was a member was reaching the US West 
Coast on its way from Europe to Japan. To us it was wonderful news. )
  One other decision deserves to be remembered. The target-selectors had a 
preferred target, a great industrial centre named Kyoto. But Henry Stimson, 
the secretary of war, knew that Kyoto was also the ancient capital of Japan; 
he had visited its monuments and seen its historic and artistic treasures, and 
he knew that to the Japanese it was a combination of Athens and Jerusalem. He 
struck it off the list, and with Truman's support he overruled appeals and 
blocked end-runs from General Leslie Groves. This great decision does not of 
itself excuse the bombing of other cities, but it does remind us that it is 
good to think hard about what we should not do with this weapon. 
  What is most remarkable about Hiroshima and Nagasaki is that they are the 
only times that nuclear weapons have so far been used in the world. Moreover, 
there is a worldwide consensus that this 50-year record of non-use is an 
achievement that should be extended. We have had sharp debates about ways and 
means, and we have had crises with nuclear danger in them, but the cumulative 
lesson of these debates and crises is that it has been right, for all 
concerned, to stay clear of the nuclear cliff. That we have managed to do so 
owes much to the sanity of statesmen, but something also to good luck, because 
there were dangers such as tactical bombs that we did not know about or take 
account of. Fortunately, we did not trigger them, but we might have -- as in 
the Cuban missile crisis of 1962, a third of the way along those 50 years.
  The Cuban missile crisis is probably the most serious moment of nuclear 
danger since Nagasaki. It sharply engaged the two states with the largest 
nuclear forces, and the encounter took place on an island where local Soviet 
commanders could have used tactical nuclear weapons. As inflammatory 
conventional hostilities on the island became real, and with it the 
possibility of escalation completely unintended by the two heads of government.
One may believe, as I do, that even if there had been a few local explosions, 
general war would have been avoided, but the legacy of anger and hatred would 
have been enormous. Fortunately, the general caution that governed both John 
F. Kennedy and Nikita Khrushchev was sufficient to prevent such an unintended 
escalation. The escape was too narrow to leave either side with any appetite 
for additional encounters of this kind, and that is the lesson both sides 
learned. After Cuba, there would be no further US-Soviet nuclear crisis.
  Still the hardest test of all for Americans may Have come later, in the one 
large war the United States ever clearly lost: Vietnam. The cruel frustrations 
of the Vietnam years are still vivid in the memories of most adults Americans. 
But what is often forgotten about that war is that in all its long trials, only 
an occasional aberrant voice was raised in favour of a nuclear solution. Far 
over on the right, Senator Barry Goldwater once or twice sounded that way, but 
when he did he lost ground politically and shocked his own advisers. In 1967, 
the joint chiefs of staff strongly pressed in Congress for an increase in the 
bombing of North Vietnam, but they limited their appeal to conventional 
weapons. Their supporters in the Senate were careful not to press them further.
As a real political choice the use of nuclear weapons, and even threats of 
nuclear weapons, remained off limits.
  President Richard Nixon believed in nuclear threats -- he thought , probably 
wrongly, that in 1953, as vice-president, he had seen them used effectively by 
President Dwight Eisenhower over Korea. But what was most important about 
Nixon's own inclination toward nuclear threat over Vietnam is that he could 
not act on it; he kept his warnings vague, and the contrary mindset of the 
American people was as plain to Hanoi as to him. Vietnam offers many painful 
lessons in what is not sensible in trying to help weak friends, and all sorts 
of alternative  lines of policy were urged along the way. I know of no strong 
public argument, made at the time or later, that it would have been better to 
use nuclear weapons. Indeed, the dominant nuclear lesson for US military 
planners from the experience has been that if they could not use nuclear 
weapons in Vietnam, they had better not count on using them anywhere, unless 
of course some one should use them on the United States first.
  The first 50 years show many other crises with a nuclear thread in them, on 
matters as varied as the control of islands off China and border disputes in 
the Persian Gulf. What is striking as one looks back at such crises is that 
although their specific histories are varied, so that students can draw their 
own lessons about the importance of firmness or flexibility, there does seem 
always to have been a double awareness: that this situation could in fact turn 
nuclear, and that any such result would be a disaster for all concerned. My 
own impression -- it is not a subject that lends itself to precision -- is 
that the restraining power of this awareness has been great.
  The end of the Cold War made possible -- even easy -- a dramatic reduction 
in the intensity of the nuclear competition between Moscow and Washington. At 
the end of the 1980s, under the leadership of Mikhail Gorbachev, the Russians 
quite simply abandoned the Cold War. Most conspicuously, they reserved their 
policy toward eastern Europe, where that war had begun, and the 40-year 
confrontation in the centre of Europe came to an end. Nuclear d�tente was not 
far behind.
  The first consequence, as rapid as unexpected, was a shared recognition that 
the enormous nuclear armament of the superpowers made no sense for either of 
them. The change that began with Gorbachev got its most dramatic exposition 
from President Ronald Reagan. While the Cold War lasted, Reagan had been a 
partisan of nuclear strength, offensive and defensive. In particular he had 
supported a wildly unrealistic commitment to strategic defence, going so far 
as to say that such defences would protect the United States in the same way 
that a roof keeps rain out of one' house. In the new political atmosphere of 
the Gorbachev era, Reagan reversed his policy and turned against the nuclear 
arms race. Recognizing that neither of the two nuclear superpowers had a 
prospect of achieving any decisive superiority and that any large-scale 
nuclear exchange between them would be a shared catastrophe beyond all human
experience, he pronounced in 1985 that '' a nuclear war cannot be won and must 
never be fought''. For the rest of his time in office, repeating this 
statement at frequent intervals, he joined with Gorbachev not only in 
large-scale agreements for nuclear arms reduction but also in dramatic 
political demonstration of a new trust between Moscow and Washington Today 
there is still unfinished work to be done on lowering the nuclear danger from 
superpower stockpiles, much of it interconnected with the political 
disintegration of the Soviet Union.
  The meaning of the Reagan dictum goes far beyond the old competition between
Washington and Moscow. That competition was the largest, the most wasteful and 
politically the most productive of fear and mistrust, but the Reagan 
proposition is valid for much smaller forces too. If two countries with small 
arsenals should use even five or ten nuclear weapons against each other, they
would both be enormous losers. Exchanges of nuclear warheads are inevitable 
catastrophes. The choice of nuclear war, when both sides have survivable 
weapons, is an act of suicidal folly.
  What is astonishing in retrospect is that political recognition of this 
proposition, and acts of arms control consistent with it, were so long in 
coming. There were many who understood the hazard from the beginning, but for 
some 40 years they did not prevail in either country. There was too much 
strength in the interlocking influence of traditional military thought and 
intense political competition. 'Their' weapons are bad and dangerous because 
'they' are bad and dangerous, and so 'we' must keep 'ahead', offensively or 
defensively or both. The notion that enough can be enough, even if a rival may 
have more, does not have a large place in the inherited military thought of 
either Americans or Russians. The Cold War nuclear competition was predestined 
by the strategic dispositions of the two superpowers, and its peaceful 
reversal may be the most surprising thing about it.

Restraint
When we look more broadly at the overall record of all the nuclear-weapons 
states, we find a similar record of restraint. None of these states has used a 
warhead in anger, and no third state has come close to the enormous 
over-arming of the United States or the Soviet Union. None has sought nuclear
confrontation with even a non- nuclear opponent. In the main, the available 
evidence supports the conclusion that the dominant purpose behind the nuclear 
forces so far deployed in all countries is defensive -- that it is mainly fear 
that has been at work. This does not allow us to ascribe only defensive 
motives to all possible proliferators, but it does argue that there can be 
political ways and means -- not unlike those used by Reagan and Gorbachev
 -- to lower the level of nuclear competition region by region around the 
world. That has already happened in South America, where Argentina and Brazil 
have turned away from competition in nuclear-weapons development. Even in 
South Asia, where there is a tangled interconnection of nuclear-weapons 
programmes among Pakistan, India and China, restraint is more remarkable than
deployment, and fear more powerful than ambition.
  an important lesson from lesser Cold War crises is that when a situation is 
still one of limited political commitment, it is highly unlikely that nuclear 
weapons will be heavily engaged. In Korea in 1950-53, the political judgement 
that the war must be limited -- that General Douglas MacArthur must not be 
allowed to spread it to China -- itself made plain that this war did not 
justify a resort to nuclear weapons. The United States could hold its ground 
without such weapons, and it did not want to run higher risks for larger 
stakes. In the same case, Stalin was already limiting himself still further, 
to a posture of ''lets you and him fight''. In Cuba, the crisis itself was 
nuclear, but that only made it more important that it be resolved by 
conventional strength; the obvious danger of nuclear confrontation was a 
powerful pressure for moderation on both sides. It is not wise to make a 
general law out of particular examples, each with its own peculiarities, but 
it remains a reasonable proposition that in a crisis over issues lesser 
important than national survival, the danger inherent in escalation will tend 
to keep conflict below the nuclear level.
  There is continuing debate among specialists about the wisdom of 
recommending to all nuclear states a firm public policy that they will never 
be the first to use nuclear weapons, reserving them only for retaliation, and 
so deterrence of nuclear attack. Such a doctrine has the obvious advantage 
that if generally adopted and abeyed, it prevents nuclear war. Its 
disadvantage is that it does not meet real and deep-seated fears of 
conventional aggression. Such fears can be the main justification for a 
nuclear force. That was the situation of the North Atlantic Treaty 
Organization through most of the Cold War; it is the situation of Israel and 
Pakistan today. Moreover, some careful and moderate students believe that 
fear of what could happen in extremis has a value for peace-keeping that 
should not be weakened by a blanket pledge against first use. There are also 
careful and moderate commanders who would never say never to any choice that 
might one day become justified. I have been on both sides of this debate, 
which gets very narrow at its edge. I am content here to remark that there is 
an apparent rough justice in firing back that is absent when one fires first
 -- so that any first use of nuclear weapons must have extraordinarily strong 
justification.
  A broad generalization from the first 50 years is that openness is generally 
helpful to the moderation of nuclear competition. Any given nuclear capability 
will be more disturbing as an undiscussed but imperfect 'secret' than as an 
open deployment. When forces are known with reasonable accuracy -- and our 
capacity to have public knowledge about worldwide nuclear capabilities has 
never been higher than it is now -- unreasonable fears are lowered. In 
particular, it is today possible for the five declared nuclear powers to know 
with assurance that none of them is in danger of having its ability to strike 
back knocked out by surprise attack. That knowledge gives justified confidence 
in Reagan's first principle of nuclear reality. The understanding of this 
proposition, along with a worldwide tradition of non-use unbroken since 
Nagasaki, is a good send-off for the second 50 years.

Clean-up
The clean-up of the warheads left over from the Soviet break-up will be 
costly, and sustained US help, along with parallel US reduction, is a good 
investment in the lessening of nuclear danger. The Nunn-Lugar Amendment of 
1992 provided just such help for the post-Soviet nuclear clean-up; it deserves 
reinforcement, not abandonment, over the rest of the century.
  This new phase will require hard work. This year has brought the welcome 
renewal of the Nuclear Non -Proliferation Treaty, but it was essentially a 
defensive effort, and it took a lot of effort not to lose ground. The agenda 
that needs action by many nations together is now long; we need much more than 
the mere survival of a valuable but imperfect instrument. But that is another 
subject, and the most that can be said here is that in the work of all the 
nations to limit proliferation, one essential requirement is the moderation 
and openness of the nuclear-weapon states. The renewal process created an 
appearance that the nuclear and non-nuclear states are opponents, whereas in 
reality they are partners who have a common enemy in the bomb itself. That 
indeed is the overriding lesson of Hiroshima, Nagasaki and the first 50 years. 
There is hard work ahead for every affected government, and all will be helped 
by remembering that whatever one may think of the way the bomb was used at the 
beginning, one cannot but give thanks that it has not been used since. Prudent 
men and women of all sorts, and especially those in authority, will do well to 
keep that history in mind as they continue to live with the possibility of 
nuclear catastrophe. The record of non-use that has been kept since Nagasaki 
is well worth handing on intact. It belongs to us all.



Evolutionary transitions, E. Szathm�ry, 16 March, RA, Bio

The major evolutionary transitions
There is no theoretical reason to expect evolutionary lineages to increase in 
complexity with time, and no empirical evidence that they do so. Nevertheless, 
eukaryotic cells are more complex than prokaryotic ones, animals and plants are
more complex than protists, and so on. This increase in complexity may have 
been achieved as a result of a series of major evolutionary transitions. These 
involve changes in the way information is stored and transmitted.

The major evolutionary transitions are listed in table 1. There are common 
features that recur in many of the transitions: Entities that were capable of 
independent replication before the transition can only replicate as parts of a 
larger unity after it. For example, free-living bacteria evolved into 
organelles. The division of labour: as Smith pointed out, increased efficiency 
can result from task specialization ( for a comprehensive review of this 
subject in the classical literature, see reference 4 ). For example, in 
ribo-organisms nucleic acids played two roles, as genetic material and 
enzymes, whereas today most enzymes are proteins. There have been changes in 
language, information storage and transmission. Examples include the origin of 
the genetic code, of sexual reproduction, of epigenetic inheritance and of 
human language.

Complexity
There is no generally accepted measure of biological complexity. Two possible 
candidates are the number of protein-coding genes, and the richness and 
variety of morphology and behaviour. Table 2 shows the sizes of the coding 
regions of various organisms. The trend is fairly robust: eukaryotes have a 
larger coding genome than prokariotes, higher plants and invertebrates have a 
larger genome than invertebrates. The last observation is puzzling: perhaps 
the nervous system of vertebrates requires the extra genetic information. 
Unfortunately, The data do not tell us much about structural or functional 
complexity, because we do not know the mapping between genotype and phenotype.
  Bonner measures complexity in terms of the variety of behaviour. For 
example, the emergence of humans depended on a greater behavioural variety. 
The point need not be confined to ethology: complexity increases with the 
diversity of actions an organism can carry out. For example, phagocytosis is a 
complex behaviour that depends on the eukaryotic cytoskeleton: prokaryotes 
cannot do it. The number of cell types in an organism can be taken as a 
measure of its complexity. Unfortunately, it is hard to quantify this aspect 
of complexity, or to get beyond the common-sense, but rather boring, 
conclusion that complexity has indeed increased in some lineages.
  It is more interesting to list the mechanisms whereby the quintet of 
genetic information can increase. The three main possibilities -- duplication 
and divergence, symbiosis and epigenesis -- are shown in Fig. 1.

Transition from independent replicators
In many of the transitions listed in Table 1 we find the common phenomenon 
that entities capable of independent replication before transition can only 
replicate as parts of a larger whole afterwards. Examples include the origin 
of chromosomes; the origin of eukaryotes with symbiotically derived organelles;
the origin of sex; the origin of multicellular organisms ( the cells of 
animals, plants and fungi are descended from unicellular protists, each of 
which could survive on its own: today, they exist only as parts of larger 
organisms); and the origin of social groups. Note that the last two examples 
differ from the previous ones: the cells of multicellular animals did not form 
the organism through a symbiosis of independent entities, but they consist of 
entities ( cells ), the analogues of which do exist as independent forms. 
Thus, units of evolution at the higher level may either be analogous 
( multicellular organisms ) or homologous ( eukaryotes ) to an 'ecosystem' of 
lower-level units.
  Given this common feature of the major transitions, there is a common 
question we can ask of them. Why did natural selection, acting on entities at 
lower level ( replicating molecules, free-living prokaryotes, asexual 
protists, single cells, individual organisms ), not disrupt integration at the 
higher level ( chromosomes, eukaryotic cells, sexual species, multicellular 
organisms, societies )? The problem is not an imaginary one: there is a real 
danger that selection at the lower level will disrupt integration at the 
higher. Some examples are: If Mendel's laws are rigorously obeyed, a gene can 
only increase its representation in future generations by ensuring the success 
of the cell in which it finds itself, and of the other genes in the cell. 
Hence Mendel's laws ensure the evolution of cooperative, or 'coadapted', genes.
But the laws are broken, in meiotic drive, and by transposable elements. These 
are examples of the more general phenomenon of intragenomic conflict. A sexual 
population has an advantage, in rate of evolution, and in the elimination of 
harmful mutations, over an asexual one. But a parthenogenetic female has, in 
short run, a twofold advantage over sexual one, and parthenogens are not 
uncommon. A gene in a somatic cell of a plant might best ensure the 
transmission of replicas of itself by giving rise to a flower bud, even if 
this reduced the success of the whole plant. A bee colony produces more 
reproductives if the workers raise the queen's offspring. But workers do lay 
eggs ( which are unfertilized, and hence male ).
  We cannot explain these transitions in terms of the ultimate benefits they 
conferred. For example, it may be that, in the long run, the most important 
difference between prokaryotes and eukaryotes is that the latter evolved a 
mechanism for chromosome segregation at cell division that permits DNA 
replication to start simultaneously at many origins, whereas prokaryotes have 
only a single origin of replication. At the very least, this was a necessary 
precondition for the subsequent increase in DNA content, without which 
complexity could not increase. But this is not the reason why the change 
occurred in the first place: the new segregation mechanism was forced on the 
early eukaryotes by the loss of a rigid cell wall, which plays a crucial role 
in the segregation of eubacterial chromosomes. Or, to take a second example, 
meiotic sex was an important preadaptation for the subsequent evolutionary 
radiation of the eukaryotes, but it could not have originated for that reason.
  The transitions must be explained in terms of immediate selective advantage 
to individual replicators. We are committed to the gene-centred approach 
outlined by Williams and made still more explicit by Dawkins. There is, in 
fact, one feature of the transitions listed in Table 1 that leads to this 
conclusion. At some point in the life cycle, there is only one copy, or very 
few copies, of the genetic material: consequently, there is a high degree of 
genetic relatedness between the units that combine in higher organism. The 
importance of this general principle was first emphasized by Hamilton in his 
explanation of the evolution of social behaviour, but we believe it to be 
quite general. To give two other examples: multicellular organisms develop 
from a single fertilized egg, so that their cells are genetically identical, 
except for somatic mutation; most eukaryotes inherit their organelles from one 
parent only, so that the organelles in a single individual are almost always 
genetically identical. We think that a similar principle operated in the 
origin of the earliest cells: this example is discussed further in Box 1.
  In several of the listed transitions, one is effectively dealing with a 
group of replicators: when does such a group qualify as an organism, or
 -- viewed from the level of the component replicators -- a 'superorganism'? 
Wilson and Sober define a superorganism as a ''collection of single creatures 
that together possess the functional organization implicit in the formal 
definition of organism''. They suggest that groups are superorganisms if they 
satisfy the following criteria: the population consists of several groups; 
there is a difference between the groups in their contribution of progeny to 
the next generation ( differential group fitness ); variation of group fitness 
is due to heritable genetic variation; individuals within the group have the 
same fitness.
  Obviously, the last criterion cannot hold at the time of origin itself; it 
is precisely this absence of between-individual, within-group selection that 
has to be explained. The real question is whether a mechanism, suppressing 
internal competition, will invade when rare. The answer is known to be yes for 
certain cases. For such a mechanism to spread, selection between groups must 
be effective. It is most efficient if the following criteria are met: the 
number of groups must be much larger than that of the units within each group; 
there is no migration ( horizontal transfer ) between groups; each group has 
no more than one parental group. 
  The effect of these conditions is that there will be genetic differences 
between groups, but individuals in a single group will be similar. If so, the 
groups will be units of evolution and will evolve by natural selection.
  The principle of the small number of founders is important at the time of 
the transition. Two other processes -- contingent irreversibility and central 
control -- help to explain the maintenance of higher-level entities once they 
have arisen, although they are lesser relevant to the origin of such entities.

Contingent irreversibility. If an entity has replicated as part of a larger 
whole for a long time, it may have lost the capacity for independent 
replication that it once had, for accidental reasons that have little to do 
with the selective forces that led to the evolution of the higher-level entity 
in the first place. For example, mitochondria cannot resume independent 
existence, if only because most of their genes have been transferred to the 
nucleus; cancer cells may escape growth control, but have no independent 
future as protists; workers bees may lay eggs, but cannot establish a new 
colony on their own.
  The contingent nature of irreversibility is perhaps best illustrated by the 
reversion from sex to parthenogenesis. Mammals are never parthenogens, 
probably because, at some loci in some tissues, only the allele inherited from 
the father is active; hence, in an embryo with no father, some essential gene 
activities are missing. Gymnosperms are also never parthenogens, always 
require sperm from the males of another species to initiate development, 
perhaps because the sperm provides a centriole. The relevance of these sexual 
hangups -- and there are many others -- is to show how various and accidental 
are the reasons why reversal is difficult or impossible.
Central control. If a 'selfish' mutation occurs in a chromosomal gene, a 
suppressor mutation at any other locus in the genome would be favoured by 
selection. Hence the rest of the genome may win the contest, not because of 
any analogue of majority voting, but because of the large number of loci, and 
hence of possible suppressor mutations, that are available for each selfish 
mutation. It may be relevant that attempts to use driving chromosomes in 
biological control have so far failed because of the rapid evolution of 
suppression. It is in this sense that Leigh's idea of a 'parliament of genes' 
should be understood.

The division of labour
The most familiar examples of the advantages arising from a division of labour 
concern caste differentiation in the social insects. Bell applied Adam Smith's 
ideas to a less familiar example, cell differentiation in the Volvocales. The 
specific name Volvox weismannia is a reminder that these algae are an excellent
example of the segregation of germ line and soma. Most members of the order 
possess only a single cell type, which fulfils all vegetative and reproductive 
functions. In Pleodorina, there is a partial division of labour: some cells 
start with the vegetative functions, but later differentiate into gonidia 
( asexual propagules ). The genus Volvox has a bona fide segregation of germ 
line and soma: germ cells are immotile in the centre of the spheroid, and 
somatic cells bear cilia but cannot divide. The benefit of differentiation has 
been demonstrated: colonies produce a larger bulk of smaller offspring than 
do single cells of a similar size.
  One precondition for the division of labour in the Volvocales is that motile 
cells cannot divide, and mitosing cells cannot move, because the same 
organelles are used either as basal bodies or as centrioles: A similar 
argument was used by Buss for the flagellated blastulae of the lower metazoa.
  Some other cases in which a division of labour is evident are: The evolution 
of many specific enzymes from a set of multifunctional low-efficiency enzymes. 
If the first protocells were equipped with a few multifunctional enzymes, more 
efficient enzymes could evolve only by duplication and divergence. In the RNA 
world, RNA served both as genetic material and catalyst: today, DNA is the 
genetic material, and most enzymes are proteins. In prokaryotes there is a 
single cell compartment, whereas in eukaryotes the genetic nucleus and 
metabolic cytoplasm are separated, and additional organelles have evolved, 
some recruited symbiotically. In sexual populations, isogamy has repeatedly 
evolved to anisogamy, with differentiated sperm and ova. Hermaphrodites are 
replaced by separate sexes: the most convincing explanation for why some 
organisms are hermaphrodite and some dioecious is in term of the advantages 
of a division of labour.
  If cooperation is to evolve, non-additive, or synergistic, fitness 
interactions are needed. If two or more cooperating individuals can achieve 
something that a similar number of isolated individuals cannot, the 
preconditions exist: the image to bear in mind is that two men, each with one 
oar, can propel a boat, but one man with one oar will row in circles. But the 
dangers of intragenomic conflict remain: both relatedness and synergistic 
fitness interactions are likely to be needed.

The evolution of heredity
Heredity means that like begets like: it requires some means whereby 
information can be transmitted. A crucial distinction is between systems of 
'limited heredity', in which only a few distinct states can be transmitted, 
and systems of 'unlimited heredity', capable of transmitting an indefinitely 
large number of messages. We suggest the following stages.
  The origin of simple autocatalytic systems with limited heredity. 
Autocatalysis, whereby a single molecule gives rise to two molecules of the 
same kind, is essential for growth, but does not by itself imply heredity, 
which requires that, if the nature of the initial molecule is changed, two 
molecules of the new kind are reproduced. Several authors have suggested that 
autocatalytic networks could have this property, but at best they could 
display limited heredity, with a few molecular types able to reproduce 
themselves.
  The origin of polynucleotide-like molecules, providing unlimited heredity. 
This transition has proven surprisingly difficult to explain. Even if one 
assumes the presence of all the necessary chemical constituents, there are 
severe obstacles to continued replication, such as enantiometric 
cross-inhibition ( mirror-image building blocks pair but do not form a 
covalent link with the growing chain ) and non-separation of template and 
replica due to the many hydrogen bonds formed between them. Oligonucleotide 
replication is a possible intermediate stage leading to that of 
polynucleotides. Most such experiments use chemical analogues of 
oligonucleotides ( such as the first successful attempt by von Kiedrowski ). 
The short length allows for the spontaneous dissociation ( 'melting' ) of 
template and copy, so ongoing replication is possible, although an increased 
concentration is unfavourable for the latter because two complete strands find 
each other more readily. This results in a parabolic ( subexponential ) growth 
of such replicators, which in a competitive situation leads to a stable 
''survival of everybody''. ''Survival of the fittest'' needs an exponential 
growth tendency and therefore more efficient strand separation. The latter 
could have been accomplished by RNAs with replicase function.
  The origin of the genetic code in the context of the RNA world, before 
translation. The essence of the code is that specific amino acids should be 
attached to specific oligonucleotides: today, it depends on the attachment of 
amino acids to transfer RNA molecules. Several workers in the field have 
realize that translation and coding are difficult to evolve simultaneously. 
One way to avoid such an evolutionary trap is preadaptation: rudiments of a 
complex adaptation may have evolved by selection for some other function. 
Concrete versions of this idea suggest that aminoacylation helped the 
replication of RNA, or that peptide-specific ribosomes with an internal 
message antedate general protein synthesis using external templates 
( messenger RNA ). The idea that we favour is that amino acids were used as 
coenzymes of ribozymes, and were equipped with unambiguous trinucleotide 
handles, enabling them to bind to a ribozyme by base pairing. Such handles 
would enable the same amino acid to be used by several ribozymes. Each new 
amino acid that acquired a specific handle would increase the enzymatic 
versatility of the organism, so that the difficulty of a complete adaptation 
being acquired in a single step largely disappears.
  The origin of translation and encoded protein synthesis. The details of this 
transition are discussed in reference 1.
  The replacement of RNA by DNA as the genetic material could well have 
happened before the origin of genetic code, because it is chemically a much 
less complicated transition. The primary selective force for this may have 
been the increased chemical stability of thymine ( as opposed to uracil ) and 
deoxyribose ( as opposed to ribose ). The usual argument that there is no 
mismatch repair or repair of damage in RNA misses the point that, chemically, 
all these processes would be feasible in double-stranded RNA.
  The emergence of hereditary regulative states in prokaryotes and simple 
eukaryotes. Already in prokaryotes, patterns of methylation are transmitted 
through cell division and can be responsible for states of phenotypic 
differentiation. Thus there is a dual inheritance system, in which heredity 
depends either on differences in DNA sequence or on transmissible states of 
gene activation. Such a system is crucial for the development of animals and 
plants, but what selective forces were responsible for its evolution in 
single-celled organisms? Jablonka suggests that it was the need for protists 
to adapt to regular changes in the environment, the timescale of which was 
too large in comparison with generation times, and too small relative to the 
time required for typical evolutionary changes. In this case, a heritable mark 
M 1 on some gene could have been beneficial in environment E 1, and an 
alternative mark M 2 ( maybe simply the lack of M 1 ) could have been 
beneficial in environment E 2. An alternative suggestion is that morphological 
and physiological adaptations of sexual protists could have been preadaptations 
for simple forms of multicellularity, as alternative phenotypes, specific cell 
adhesion, cell-to-cell signalling and cell-division arrest play a crucial role 
in both.
  The evolution of epigenetic inheritance with unlimited heredity: the 
emergence of animals, plants and fungi. The transition to multicellular 
organisms with many kinds of differentiated cells occurred on three occasions, 
suggesting that it may not have been particularly difficult. This would be 
explained if the main cellular novelty required was an epigenetic inheritance 
system, as this existed already in protists. If so, the emergence and 
radiation of the metazoa had to wait only for suitable environmental 
conditions.
  The emergence of proto-language in Homo erectus -- a cultural inheritance 
system with limited potential in which, because of the absence of grammar, 
only certain types of statement can be made.
  The emergence of human language with a universal grammar and unlimited 
semantic representation. Grammar enables a speaker with a finite vocabulary to 
convey an indefinitely large number of meanings, just as the genetic code 
enables DNA to specify an indefinitely large number of proteins. We accept 
Chomsky's argument that grammatical competence is unique, both in the sense of 
being peculiar to humans, and of being special to language, and not merely an 
aspect of general learning ability. But we are puzzled by the reluctance of 
many linguists, including Chomsky himself, to think about the evolution of 
this competence. The objection takes the form of asserting, not only that human
 language is different in kind from animal communication, but that no 
intermediate is possible between the two.
  It is argued that any rudimentary form of grammar would not allow one to 
generate some types of sentence. This is true but irrelevant: by analogy, it 
is better to have some light-sensitive cells than none at all; a perfect eye 
is not the only useful solution to the problem.
  It is in fact rather easy to think of intermediates between protolanguage 
and true language. There remains the question of the evolutionary origin of 
grammatical novelties. It is reasonable to assume that this happened by genetic
 assimilation, new rules being made up by individuals as non-genetic 
innovations, then learnt by members of the community, then hard-wired into the 
'language organ' subsequently. It has been demonstrated that learning and 
selection can lead to such an assimilation in extreme cases when the latter 
alone could not get anywhere: learning can transform an initially flat fitness 
landscape with a needle-like peak into a well-behaved Fujiyama-like one.
  Perhaps the most convincing evidence both for the belief that grammatical 
competence is to some degree independent of general learning ability, and for 
the possibility of functional intermediates between no grammar and perfect 
grammar, comes from studies of heredity variation in linguistic competence. 
One remarkable case involves a family in which a so-called feature-blind 
dysphasia seems to be inherited in a mendelian fashion, a single dominant 
gene being responsible. Members cannot automatically generate plurals and past 
tense. Although they understand the meaning of plural and past perfectly well, 
they have to learn each new case anew: paint and painted, book and books must 
be learned separately ( in the case of exceptions such as go and went, we must 
do the same ). To be sure, this is not a genetical violation of one of 
Chomsky's rules, but it demonstrates that there can be something useful 
between perfect grammar and protolanguage: it also hods out the hope that we 
will in future be able to dissect language genetically, as we are today 
dissecting development.

Constructive evolution
Although some of the key intermediate stages of evolution seem to have 
vanished, their experimental recreation could teach us a lot. Examples include:
  The de novo synthesis of a living chemical system, such as the chemoton 
( see Box 2 for a summary of this idea, and how several of our discussed 
points integrate into a unified picture at a certain level of organization ).
  In vitro construction of a truly self-replicating RNA.
  In vitro generation of ribozymes, using amplification and selection by 
affinity chromatography. The first such example has been given. In a similar 
vein, the generation of RNA molecules of importance for primordial coding and 
translation, essentially by the same protocol, should provide us with useful 
information about feasible scenarios of the code's origin. Recently Famulok 
reported the in vitro selection of an RNA binding ornithine and citrulline. 
Similar tests using protein-ogenic amino acids would be welcome.
  The establishment of artificial symbioses should help to clarify several 
aspects of some of the transitions. A first example is Jeon's bacteria, 
originally parasitizing an amoeba, which became obligatorily dependent on 
these bacteria later.
  Finally, recreation of extant species or forms may be in certain cases 
possible. The recreation of a fossil fern species from genomes of extant 
polyploids is a remarkable example.

Conclusions
A central idea in contemporary biology is that of information. Developmental 
biology can be seen as the study of how information in the genome is 
translated into adult structure, and evolutionary biology of how the 
information came to be there in the first place. Our excuse for writing an 
article concerning topics as diverse as the origin of genes, of cells and of 
language is that all are concerned with the storage and transmission of 
information. The article is more an agenda for future research than a summary 
of what is known. But there is sufficient formal similarity between the 
various transitions to hold out the hope that progress in understanding any 
one of them will help to illuminate others.


New Statesman & Society

Rising, J. Pilger, 16 June, P

The rising of Indonesia
Although the power of Suharto's military dictatorship pervades Indonesian
society, the world's fourth largest nation is not as "stable" as it might seem,
thanks largely to the example of East Timor's epic resistance.

There are signs that the dictatorship in Indonesia, the world's fourth-largest
nation, is in trouble. Since it massacred some 270 young people in a cemetery
in East Timor three and a half years ago, the military regime of General
Suharto has seen its contrived international respectability vanish; and now its
domestic power may be dissipating. This is not to say that a coup is likely;
the military's repressive force pervades Indonesian society; Yet seldom a week
passes when the limits of its control is not tested by an increasingly
confident popular resistance, suggesting a reawakening of the great mass
movements that distinguished Indonesian pluralism and nationalism until
Suharto's bloody rise in 1965.
  East Timor has made a significant difference. Before the 1991 Dili massacre,
most Indonesians had little idea of what had happened there; and the regime
ensured that its own high military losses, the thousands of maimed Indonesian
veterans and villages of widows, especially on Java, were never common
knowledge. But as East Timor has emerged from the diplomatic and media oblivion
to which it was assigned soon after the Indonesian invasion in 1975, it has
become an international issue, developing at a pace reminiscent of the
successful anti-apartheid movement. The Indonesian press, the middle class,
trade unions and the growing "pro-democracy movement" are now well aware that
Suharto and his generals are gradually becoming pariahs, with pressure coming
from the United Nations Human Rights Commission, the US Congress, the European
Parliament, Amnesty and numerous other bodies acting in response to  tenacious
solidarity campaigns and to public opinion.
  For example, Suharto's recent, disastrous trip to Germany has left him
virtually friendless in Europe, with of course the exception of Britain.
Governments do business with him, but they don't want his presence, a
remarkable situation for the head of the Non-Aligned Movement. In Weimar, the
city council voted to call off his visit; in Hanover, the Lord Mayor attacked
him in a speech given in his presence; in Dresden, angry crowds virtually held
him hostage, encircling his bus and plastering it with "Free East Timor!
Suharto Murderer!" signs.
  During this siege, Indonesian foreign minister Ali Alatas became apoplectic
and entertained the crowd with obscene gestures. A photograph of Alatas giving
the finger, reproduced on the next page, says much about the regime's deepening
frustration and humiliation. In the past, Alatas has cultivated an image of
urbanity and reasonableness, even turning up at the World Conference on
Human Rights at Vienna in 1993 with his very own "collected speeches on human
rights" in a glossy folder. Then, no delegate raised the question of his
regime's well-documented genocide in East Timor. Today, he would not get away
with it.
  Compounding the debacle in Germany, Suharto ordered the arrest of the most
popular member of Indonesia's mostly stooge parliament, Sri Bintang Pamungkas,
accusing him of "insulting the dignity of the president", a criminal offence
that carries a six-year prison term. Bintang's crime was that he happened to be
giving an economics lecture in Germany when Suharto arrived, and was a passive
onlooker at one of the demonstrations. His arrest has greatly enhanced his
popularity, which stems from his courageous demands that the military give up
its civilian role. This has divided the dictatorship, with the so-called
technocrats -- such as those who make fortunes doing deals with the British
arms industry -- wanting the affair hushed up, afraid that it will embarass
their "business partners" in the west and Japan.
  The stamina and flair of the East Timorese resistance has had a powerful,
inspirational influence on the pro-democracy movement. Xanana Gusmao, the
imprisoned East Timorese leader, has become something of a hero in Indonesia.
And like the young Timorese who occupied the American embassy in the centre of
Jakarta last November -- pushing East Timor into the headlines during the Apec
conference and forcing President Clinton to criticise the regime -- young
Indonesians now routinely perform similar acts of bravery and principle. Last
month, Prasetyadi Pancaputra, a 25-year-old student from Surabaya, went on a
hunger strike in the foyer of the parliament building, demanding an end to the
army's tyranny. Within a week, galvanised by his action, a new political
coalition of all of Indonesia's resistance and pro-democracy groups was formed
-- the Indonesian People's Movement, or KPRI.
  This includes the workers' movements, SMID and PPBI, the students of Nahdatul
Ulama, the biggest Islamic organisation, several popular pro-democracy
alliances and a broad free-press coalition of 37 groups. They join other
movements, such as SBSI, the independent trade union formed three years ago and
which offers perhaps the single greatest threat to the regime. Last year, the
chairman of SBSI, Muchtar Pakpahan, was imprisoned for organising strikes and
criticising the government. In January, his sentence was increased from three
to four years. Then, on 20 May, he was suddenly released: an indication of the
power that striking workers hold in Indonesia, one of the world's great
foreign-owned sweatshops.
  Strikes now obsess the dictatorship. Last year, the number of strikes
doubled. Like a Chinese firecracker, they explode across the country, one after
the other: in transport, the textile industry and the shoe industry, makers of
the world's fashionable trainers. The factories stand behind walls topped with
barbed wire, and it is common to find one or two toilets for 100 workers and no
clear running water. In one factory making household goods, 30 workers have
lost limbs. Last year, there were almost 18,000 reported accidents, and
hundreds of deaths.
  The universal story of an Indonesian worker is a young person who leaves his
or her village as a teenager, often after the army has forced the family off
its land. In the cities, they are likely to live in hovels beside open sewers
with an income of as little as 75 pence a day. Many work overtime for months
simply to be able to afford to go home for the Muslim holiday. And, as more are
forced into the cities, more are politicised.
  Last month, for the first time in 30 years, workers and students defied the
ban on May Day celebrations and held demonstrations in Jakarta and Semarang.
One who would have been among them, had she lived, was a young female factory
worker, Marsinah, aged 25, who was tortured, raped and murdered in East Java
because of her trade-union activity. According to Amnesty, all the evidence
points to the regime as her killer.
  "The Indonesian army", says Amnesty in its report Power and Inpunity, "has
always been organised to deal with domestic rather than international threats."
This fact alone makes liars of British ministers who justify the sales of
weapons as "in accordance with a nation's right of self defence". The regime
uses the euphemism "territorial" to describe its internal security forces. In
every village there is meant to be a babinsa, an army spy, whose job is social
controller, ensuring "stability" and that the fascist strictures and petty
ceremonials of Suharto's "New Order" are carried out. However, as the
population has grown, a babinsa might have to contend with a dozen villages, an
impossible task. Asimilar system is already at breaking point in the cities,
such as Jakarta, which is well on the way to doubling its population.
  Perhaps Suharto's most serious misjudgment has been his campaign against
journalists. In banning the weeklies Tempo, Editor and De Tik, which offered
relatively mild criticism of government policies, he dismantled the facade of
"openness" that was central to Indonesia's propaganda of respectability. The
journalists on these and other papers were incensed and have since faced police
truncheons, harassment and prison. At the same time, an extraordinary group,
the Alliance of Independent Journalists, or AJI, published a newspaper,
Independent, whose investigations of venality became required reading among the
elite. It has also published in English a collection of eloquent essays by
journalists, entitled banning 1994. Testimonies such as "A Letter from a
Reporter" and "The Ironies of Openness" bear striking and shaming comparison
with the supine, comfortable work of those journalists in Britain who appear to
give hardly a thought to their relationship with the agents and arbiters of
power.
  These publications and AJI have since been banned. Last October I met Ahmad
Taufik, a founder of AJI, in London. He had just come from the Foreign Office,
where he had listened, incredulous, to the deputy head of the South East Asia
Department, Carol Robson, "assure" him that the "human-rights situation" was
improving in his country -- an encounter capturing the sheer absurdity of
institutional propaganda. I asked him if British and other western support for
the regime was important. "Extremely," he replied. "It means that the regime
can go on ignoring human rights. At the very least, Britain should not sell
them weapons. This reinforces their will to stop the movement against
democracy. Is this what the British want?"
  Last month, overseas development minister Lynda Chalker flew to Jakarta to
give the regime an �80 million "soft" loan. This is an annual ritual undertaken
by a senior representative of Indonesia's biggest arm supplier -- arms that
are, as the World Development Movement has just revealed, substantially
subsidised by public money. One such loan was made shortly after the Dili
massacre in 1991; another preceded the sale, in 1993, of 24 British Aerospace
Hawk ground attack aircraft, the type used in East Timor. This year's "gift"
follows the announcement in March that the Alvis company of Coventry has been
licensed to sell to Jakarta hundreds of armoured vehicles -- which are ideal
for "crowd control". Part of this ritual is to say something fawning about the
regime to its face and the world. Like a roving Lord Haw Haw, Douglas Hurd has
done a lot of this, publicly congratulating Suharto on recognising human rights
as an important element in man's freedom". What the regime had achieved, Hurd
said, was "proof of its recognition of the freedom of union, freedom to express
opinion and press freedom as a fundamental right". All of these "elements" are,
of course, non-existent or banned.
  A few weeks ago, Lynda Chalker told the world from Jakarta that "human rights
have improved in Indonesia". The opposite is true, of course. Chalker is aware
that Ahmad Taufik and many other journalists have been arrested and are to
stand trial in one of the regime's ridiculous Kangaroo courts. She is aware
that all the works of Indonesia's greatest novelist, Pramoedya Ananta Toer, are
banned, including his just completed memoirs. She is aware that the regime's
repression and violence is worse than at any time since the Dili  massacre, as
Amnesty has confirmed. She is aware that ten young East Timorese have been
sentenced to up to three years for peacefully displaying banners in Dili on 9
January. She is aware that an Australian nurse has just described scores of
cases of Indonesian atrocities, including "common" torture and rape, that he
treated secretly in East Timor. She is aware that, in West Papua, 37 people
were killed by the Indonesian army as it forcibly removed tribal people from
the area of the Freeport copper mine, which is partly owned by the British
company RTZ. The list of recent human-rights outrages alone would fill this
journal. The "untrue" ministerial statements, to which Sir Richard Scott's
draft inquiry report refer and which seems to have shocked some commentators,
is standard procedure in covering for the British arm industry, in Iraq or
Indonesia or anywhere.
  The resistance in both Indonesia and East Timor (and West Papua) has far to
go. But it will surely accelerate on 30 June when the International Court of
Justice seems likely to declare illegal the Timor gap treaty between Australia
and Indonesia. (The treaty was signed by Australian foreign minister Gareth
Evans and Ali Alatas flying over East Timor, toasting each other in champagne.)
It will accelerate as more and more East Timorese who previously accepted
integration with Indonesia continue to confound their patrons in Jakarta by
calling for selfdetermination.
  And the resistance will accelerate if people in the west, of whom many
thousands have written to their governments protesting about East Timor, look
carefully at the cheapest bags, jeans, trainers, toys and other consumer goods
in the high streets and do not buy them if the labels say "Made in Indonesia".
The effectiveness of this sanction is considerably more than symbolic, as the
majority of South Africans can verify.



Secret history, J. Pilger, 22 September, P

The secret history of Suharto's bloody rise
On the 30th anniversary of the coup that claimed up to a million lives in
Indonesia, declassified documents reveal how Washington, having tried to
overthrow the nationalist government, backed its favourite general and his
assault in East Timor.

When I met Ahmad Taufik in London last year he reminded me of a kind of brave,
non-conformist journalism that is threatened with extinction in this country.
Taufik, in his early thirties, was a journalist on the Indonesian weekly Tempo,
a publication that General Suharto banned because of its modest attempts to
tell the truth about his regime. Taufik and others formed the Alliance of
Independent Journalists and founded their own magazine, an extraordinarily
daring act in a country where the law is bent to criminalise free speech.
  When we met, he was travelling in Europe alerting anybody who wanted to
listen to the crushing of free expression in Indonesia. That evening he had
seen Carol Robson, the deputy head of the South East Asia Department at the
Foreign office. She assured him that "the human rights situation is improving
in your country". Astonished, he replied that the opposite was true. She said
that the "lighter punishment" handed out to journalists protesting in Jakarta
(who had been beaten by the police) was proof that "conditions have improved".
  Three weeks ago, Ahmad Taufik and another journalist, Eko Maryadi, were the
recipients of this "lighter punishment". They were each sent to prison for two
years and eight months for writing articles that critically analysed the
regime's policies. A third "light" sentence of 20 months was handed out to
Danang Kukuh Wardoyo who, at 18, became Indonesia's youngest political prisoner
and whose "crime" was to have been employed as an office worker at the
magazine. Since then the editor of another dissident journal, Tri Agus Susanto
Siswowihardjo, has been sentenced to two years in prison for writing an article
that "insulted" Suharto.
  None of this is unusual in a country run by a dictatorship which, as Amnesty
documents month after month, is itself truly criminal, guilty of "casual mass
murder" and of imprisoning and torturing thousands of innocent people. Such is
its barbarity that among the political prisoners are old and sick men who have
been on death row for 30 years. On 17 August, the "minister of justice" announced
that two of them, Bungkus and Marsudi, almost certainly former soldiers, were
to be executed. Next week marks an anniversary which, in this year of
anniversaries, will not get the attention it deserves. It will be 30 years
since General Suharto began his bloody progress to power. One of the regime's
achievements has been its domestic brainwashing about Suharto's mysterious
rise. The grand lie is that Suharto and his generals came to the rescue of
their country as it was about to fall victim to the Indonesian Communist Party,
the PKI. This "communist coup" had unique features. None of its leaders, who
were middle ranking army officers, was a communist; moreover they insisted they
were acting to prevent a CIA-backed coup against President Sukarno, whose
nationalist policies had long made him one of Washington's cold war demons.
Merely to utter this version in Indonesia today is to risk your life.
  The intrigues that surrounded the events of 1965 had one clear result.
Suharto and his faction within the army instigated one of the greatest
bloodbaths of the 20th century, murdering up to a million people, most of them
peasants associated with the great, popular movements that had flourished from
independence from the Dutch. It is also clear, from recently declassified US
documents, that the US embassy in Jakarta, while dismissing any reason for a
"communist coup", supported the slaughter and helped the pro-Suharto generals
to plan and execute it.
  The CIA gave them a "hit list" of 5,000 Communist Party members including
heads of trade unions and women's and youth groups, who were hunted down and
killed. In 1990, a former US embassy official disclosed that he had spent two
years drawing up the hit list, which, he said, "was a big help to the army". "I
probably have a lot of blood on my hands," he said, "but that's not all that
bad. There's a time when you have to strike hard at a decisive moment." The
list was approved by the US ambassador. As people on it were murdered, their
names were crossed off by American officials.
  President Johnson's secretary of state, Dean Rusk, cabled his encouragement
to the Jakarta embassy. The "campaign against the communists", he wrote, must
continue as the military "are the only force capable of creating order in
Indonesia". The US ambassador replied that he had assured Suharto and his
generals "that the US government is generally sympathetic with, and admiring
of, what the army is doing".
  When the pro-Suharto group requested more US weapons to sustain the
slaughter, they were assured that "carefully placed assistance" -- covert
deliveries -- would "help the army cope". "No single American action...after
1945," wrote the historian Gabriel Kolko, "was as blood-thirsty as its role in
Indonesia."
  The American press welcomed the bloody events as the triumph of "moderates"
bringing "stability" and a "gleam of light in Asia". It was "the west's best
news for years in Asia" that gave "hope where there was once none". The Wall
Street Journal congratulated Suharto for using "strength and finesse". The Los
Angeles Times turned the truth inside out and said the Indonesian communists
"had subjected the country to a bloodbath". The US land invasion of Vietnam in
March of that year, 1965, was now justified as providing a "shield" behind
which Suharto was encouraged to carry out his important anti-communist work in
the region.
  In Britain the Labour government was almost as supportive. Foreign secretary
Michael Stewart flew to Jakarta in the wake of the extermination campaign and
spoke of "reaching a good understanding" with the regime. The Economist
described Suharto as "at heart benign". The Confederation of British Industries
said that Indonesia now presented enormous opportunities for the foreign
investor". In neighbouring Australia, the leading Indonesia specialist,
Professor J. A. C. Mackie, eulogised the new "moderate" regime, which was
"clearly anti-communist and committed to a low-key, unassertive foreign policy
with a new stress on regionalism and good neighbourly relations with nearby
countries". He made no mention of the massacres that were the precursor to an
"unassertive foreign policy" that resulted in the invasion of East Timor and
the extermination of a third of its population. "The stage was set", wrote
Mackie, "for the working out of a new and more constructive, enduring set of
links."
  This "new set of links" was vividly demonstrated by the Australian and
Indonesian foreign ministers, Gareth Evans and Ali Alatas, as they toasted each
other in champagne flying over the Timor Sea in 1989. They had just signed an
agreement to divide East Timor's oil and gas reserves, which, boasted Evans,
could be worth "zillions".
  This year, the Australian army exercised with and trained the same Indonesian
Strategic Command units responsible for the worst atrocities in East Timor. The
Indonesian general who directed the 1991 massacre of several hundred unarmed
demonstrators in the Santa Cruz cemetery in Dili was welcomed warmly in
Canberra; and while journalists have been imprisoned and trade unionists
tortured and executed, Prime Minister Keating has praised "Suharto's tolerant
society".
  For the British, the "enormous opportunities" have long been taken up;
British companies are now Indonesia's bigger arms suppliers. This "investment",
said Lynda Chalker, overseas development minister, "is helping Indonesia's
poor".
  Shortly after Suharto seized power the west and Japan set up the Consultative
Group on Indonesia, which would ensure the "stability" of the regime with a
steady flow of "aid" and low-interest loans. Chaired by the World Bank, the CGI
has just announced this year's handout of more than $5 billion. In years when
even the Foreign Office has been unable to lie that "the human rights situation
is improving", the aid has not stopped. On the contrary, shortly after the 1991
Santa Cruz massacre, the British government increased its aid to Jakarta by 250
per cent to 81 million, the large percentage rise of any donor country.
  Richard Nixon called Indonesia "the region's hoard of natural resources, the
greatest prize in South East Asia". During the late 1950s, the US mounted an
astonishing campaign to "gain the prize" by replacing the nationalist Sukarno
government with one that would follow its prescriptions. The details of this
episode have remained secret until the recent publication of a book by the
distinguished Asian scholar George McT Kahin and the historian Audrein Kahin.
Working mainly from declassified documents, they disclose how the CIA financed
and armed a dissident army in Indonesia.
  Unknown to Congress and the American public, ships of the US Seventh Fleet
and a camouflaged American airforce were secretly dispatched to support the
rebels, with help from British intelligence and the Royal Navy in Singapore. In
the course of a series of disasters much of Indonesia's airforce and navy were
destroyed and a civil war broke out, causing the deaths of thousands of people.
The Sukarno regime survived the debacle -- and the CIA went on to a similar
triumph with its abortive invasion of Cuba at the Bay of Pigs.
  However, it was only when the genocide in East Timor was revealed that the
scale of the west's complicity with Suharto's fascist "New Order", gained
belated international recognition. The role played by Henry Kissinger, who
illegally bombed neutral Cambodia in the early 1970s, is striking. Last July
Kissinger was speaking in a hotel in New York about his new book, Diplomacy,
when he was challenged from the audience by an East Timorese, Constancio Pinto.
I know this young man; he has endured arrest and torture by the Indonesian army
for most of his adult life. He asked Kissinger what he was doing in Jakarta on
the day before the invasion, on 7 December 1975. Kissinger was then secretary
of state. Kissinger replied that he had so much "global policy" to deal with
that "a little speck" like East Timor "was not a big thing on our radar
screen". He said the Indonesians told him they were about to invade as he was
leaving, "literally at the airport".
  In 1993 Philip Liechty, a former senior CIA operations officer based in
Jakarta at the time of the invasion told me: "Suharto was given the green light
[by Kissinger] to do what he did. There was discussion in the [US] embassy and
in traffic with the state department about the problems that would be created
if the public and Congress became aware of the type of [American] military
assistance that was going to Indonesia at that time. Rifles, ammunition,
mortars grenades, helicopters... you name it... was going straight into Timor.
It was covered under the justification that it was for "training purposes only"
[but]without heavy US logistical support the Indonesian might not have been
able to pull it off".
  Leaked minutes of a meeting between Kissinger and his senior staff in
Washington clearly show Kissinger recommending the illegal export of American
arms to the Indonesian military in East Timor. At his recent book event,
Kissinger was asked about this by Alan Narn, a journalist who was one of the
foreign witnesses to the Santa Cruz massacre. Nairn also asked if Kissinger
would support the convening of an international war crimes tribunal on the
subject of East Timor "and agree to abide by its verdict in regard to your own
conduct".
  Kissinger replied: "I mean, uh really, this sort of comment is one of the
reasons why the conduct of foreign policy is becoming nearly impossible."
(Kissinger has just received an honorary knighthood from the British government
for his services to "peace".)
  In August, the Jakarta regime revoked a three-month cut in the 20-year
sentence of the East Timorese resistance leader, Xanana Gus Mao. This was his
punishment for sending a letter. What maddens the regime is that it has been
unable to stop Xanana's clandestine links with his people and the rest of the
world, just as the apartheid regime in South Africa could never silence Nelson
Mandela in prison. The extraordinary spirit of Xanana, and the epic struggle of
the East Timorese, is now the touchstone of a resistance growing throughout
Indonesia, led by an alliance of striking workers and young people like Ahmad
Taufik -- who makes me proud to be a journalist.



Triumph, J. Pilger, 21 July, P

Another UN triumph
While Bosnia bleeds to death with the cameras standing by, Cambodia's
unfashionable demise is not news. But, as an exercise in western power, there
are similarities between the disaster of the one and the "triumph" of the
other.

At the Cambodiana Hotel, on the banks of the Mekong, Friday night is "Khmer
night". This concession to the people of Cambodia, who are not permitted near
the "luxury" hotel unless they are rich and important, is announced beneath a
poster of Gone with the Wind. The rooms cost up to $200 a night, more than the
annual income of most rural families. As you enter you are decompressed from
the rest of Cambodia with arctic air-conditioning and the sickly smell of cheap
air-freshener. On the wall is a photograph of the hotel manager greeting Lord
Caithness, the former foreign office minister whose job was to deny that
British soldiers were secretly teaching Cambodians how to blow the limbs off
each other with land mines. He is here in the sickly cold, not forgotten.
  The French colons are back around the pool. They are served by
white-uniformed attendants, supervised by a young Frenchman. The alacrity of
deference is not quite up to former colonial standards, but considering the
nature of the intermission provided by Henry Kissinger and Pol Pot, things
could be worse. The talk is whether or not la gl�ce is from bottled water. The
knowing have retreated to their rooms by dusk when malaria-carrying mosquitoes
swarm in from the casino boat.
  Where the lawns fall into the river, out of sight to all but the most
curious, the purest and most frightened people on earth can be viewed in their
rusted iron shanties, the size of cupboards, their children coughing in the
usual way. They are mostly fishermen and former government soldiers who have
come to Phnom Penh to feel safe. They include those disfigured from fighting
the Khmer Rouge and from stepping on mines, and they bring with them a sense
of menace that is like a presence in a Cambodia left to drift back, perhaps to
Year Zero.
  Such is the legacy of what President Bush called the "international
community's greatest triumph": a triumph that heralded Cambodia's "golden age
for human rights" and "a new lease of life". In its 50th anniversary
propaganda, the United Nations calls the "peace" it claims to have brokered in
Cambodia "a brilliantly successful model of international cooperation". With
the UN's self-congratulation currently waylaid by the disaster in Bosnia, the
"triumph" in Cambodia is especially important. That the same senior UN
official, Yasushi Akashi, has led both disaster and "triumph" may be
coincidental; but I doubt it. While Bosnia bleeds to death with cameras and
phones standing by, Cambodia's unfashionable demise is not news; clearly, the
end must come before the headlines return. Then the powers responsible for its
"new lease of life", principally the United States and China, with Europe,
Japan and Australia in tow, will feign shock, and blame what Akashi called "the
Cambodian tradition of deep-rooted violence".
  Devaluing the truth of the past -- that no society has been as brutalised by
foreigners as Cambodia -- has been the real triumph. While an 81-year-old man
is charged in an English court with war crimes related to the second world war,
Henry Kissinger, whose secret, illegal bombing of Cambodia killed up to
three-quarters of a million civilians, is nominated for an honorary knighthood
by the British government -- the same government that worked assiduously to
include the Khmer Rouge in the current "peace process". This connection and
culpability have been so successful consigned to historical oblivion that the
Institute of International Affairs can declare, in all seriousness, that
Cambodia's "self-inflicted destruction" has left it "virtually on its own". Six
years ago, when the United States and its friends were constructing the "peace
process" and Margaret Tatcher was demanding the inclusion of Khmer Rouge in the
Phnom Penh government, a British diplomat told Eva Mysliwiec of Oxfam:
"Cambodia is of no strategic value... it's expendable."
  My own investigations in Cambodia recently suggest that the American imposed
"peace" has left Cambodia more divided, more ethnically volatile, more
politically unstable and at greater risk to Khmer Rouge takeover than at any
time since Pol Pot began his final push for power in the early 1970s.
  Most of the goals declared in the 1991 Paris "peace accords", overseen by the
UN, were never met or were abandoned. The Khmer Rouge were never disarmed; the
mines were never cleared; a secure and "neutral political environment" was
never sought. Instead, the Khmer Rouge were appeased, courted and restored in
the name of a "reality" that they were "too powerful to be left out". This was
entirely specious; in 1990, the Vietnamese and the then Phnom Penh army had
them on the run. One year later, with the UN in charge, the Khmer Rouge were
allowed to regroup and mount a major offensive. The UN commander Australian
General John Sanderson Boasted that UN forces had "prevented the Phnom Penh
army from significantly building up the counter-offensive".
  Under the "peace process" the Khmer Rouge were awarded "the same rights,
freedoms and opportunities" as other "factions", with their distinctions as
genocidists not considered relevant. They were permitted unrestricted access to
supply lines from Thailand, on roads built with American "aid". They were
welcomed back to Phnom Penh by fawning UN officials and given a headquarters
next to the Royal Palace. General Sanderson refused publicly to use the word
"genocide" in any reference to the Khmer Rouge even when it was pointed out to
him that the UN itself had previously described the Khmer Rouge terror as
"genocide in the most restricted meaning of that term".
  Such contortions of intellect and morality became routine. On "United Nations
Day", in Phnom Penh, I watched Pol Pot's senior henchmen Khieu Samphan feted at
a parade honouring the UN troops. This was the man who said that the only Khmer
Rouge "error" had been not to kill more people. Australian foreign minister
Gareth Evans, one of the most outspoken "realists", flew into a Khmer Rouge
"liberated zone" to pin gold Kangaroos on Pol Pot's men. The UN spokesman,
Eric Falt, a Frenchman, said: "You must understand the peace process was aimed
at allowing [the Khmer Rouge] to gain respectability."
  A senior minister serving in the present Cambodian government, an honest man
I have known for a long time, told me: "We could have worked out a deal with
Sihanouk in November 1989 that kept the Khmer Rouge out. We offered him his
throne and he accepted it. But the Perm-5 (of the UN) would have none of it.
The Americans, with the Australians running the messages, said, "No Khmer
Rouge, no deal". We weren't even allowed to outlaw them. Last year, Evans was
still demanding a role for the Khmer Rouge in the government. If we'd been
allowed to get on and oppose Pol Pot, hundreds they subsequently killed,
including the western hostages, would be alive now."
  The UN-sponsored elections used a system of proportional representation
guaranteed to produce a weak coalition. This is now bankrupt and indebted to
international institutions. As a result of the UN intervention, says a secret
report by the UN agency on social development, "Cambodians have lost control
over their own development process." The army, in which a generalship can be
bought for $2,000, is consistently thrashed by the numerically inferior Khmer
Rouge.
  Last week, the town of Treng, near Battambang, was captured by the Khmer
Rouge. More than 1,000 government troops, backed by helicopter gunships and
tanks could not shift them and were forced to retreat. Pol Pot's army is
presently attacking Preah Vihear, unopposed, and has once again surrounded the
"safe haven" of Siem Reap and the Angkor Wat temples. Little of this has been
reported.
  Helen long, a British researcher for the UN Landmines Survey and one of the
few foreigners to have travelled through the Khmer Rouge hinterland in the past
two years (and survived), says Pol Pot's policy of encirclement is proceeding
"village by village, commune by commune, town by town". She confirms the
veracity of UN military maps, which were secretly photographed by the American
Cambodia specialist Craig Etcheson shortly before the 1993 elections. These
show the Khmer Rouge as a pincer movement operating with varying degrees of
impunity in 25 per cent of the country; in another 25 per cent, they operate
freely by day and are in control by night. That means in half of Cambodia, Pol
Pot now has a military advantage he did not have before the UN arrived in
October 1991.
  Across the border in Thailand -- which the Thai government claims it has
"sealed" -- the impunity is almost total. Last week, a British photographer
with long experience in the region and his Thai guide discovered four huge
Khmer Rouge bases inside Thailand. Travelling on the Buddhist new year holiday
and pretending to be lost tourists, they found two bases less than 100 metres
off a busy highway within a short drive of the town of Trat. He described one
of them as "vast and well-established, with concrete buildings and a hospital".
It was flying the Khmer Rouge blood-red flag.
  On an earlier trip to another base, known as "Camp 93", the photographer was
told by Thai soldiers guarding it that if he went beyond their roadblock "the
Khmer Rouge will kill you or take you into Cambodia". He found Thai troops
doing what they have been doing since 1979: providing security and logistics
support for Pol Pot. "There are stretches where the Thais have actually moved
the border," he said, "so that the KR bases have the security of being on Thai
soil. All them are connected by a network of well-made military roads."
  The Americans are, of course, aware of this. "Bringing back to life, then
using the Khmer Rouge" said a former American official two years ago, "was
always high risk." Since then the theme of western propaganda, dispensed to
journalists via the usual "diplomatic sources", has concentrated on a
mysteriously "demoralised" and "almost finished" Kmer Rouge. "And what of the
Khmer Rouge?" wrote William Shawcross in the New York Times. "For them, the
election was like holding a crucifix to Dracula." In 1991, when the UN arrived
Australian and Thai officials estimated Khmer Rouge strength at 40,000. Since
then, if you follow the crucifix/Dracula line, all but a few thousand of these
have "defected" or simply vanished. Some crucifix.
  This nonsense aside, what concerns the Americans now is what will happen when
Sihanouk dies: whether the government headed by Prince Ranariddh can cope with
Pol Pot and can resist the organisational strength of his coalition partner,
the CPP, which is loathed by Washington for its tutelage by the Vietnamese. For
the past year, US Special Forces have been arriving in Cambodia, ostensibly as
part of the US's Humanitarian De-mining Assistance Program. The funding for
this, US$20 million, is being drawn from the Operations and Maintenance Budget,
the most restrictive and secretive of the US defence budgets. At the same time,
the CIA in Phnom Penh has increased its strength to 30 personnel, an
exceptional number for a relatively small embassy.
  One of the most experienced European diplomats in Indo-China says it is clear
the US is using de-mining as a front for its covert operations. "They are
almost certainly planning a coup," he said. "They've lost patience with
Ranariddh, who's no longer regarded as having a grip on events."
  The UN has produced report after report showing that the Thais hold the key
to peace in Cambodia. One suppressed study makes clear that, without the
support of the Thai military, the Khmer Rouge would wither on the vine. But
neither the UN secretary-general nor any western or regional leader has
condemned the Bangkok government and demanded that it close the border and the
Khmer Rouge bases on its soil.
  The Thai military is effectively stealing much of Cambodia's forests and
gemstones. According to Global Witness, the environmental investigation group,
the Khmer Rouge share of just one deal with Thai loggers is �57,000 a day.
  The parallels with the Balkans are enticing. Just as Europe wilfully
recognised and backed Croatia, and America was determined to break up
Yugoslavia, so the US punished Vietnam using proxies. And just as the west,
through the UN, continues to impose a one-sided arms embargo on Bosnia, so the
UN was the instrument of America's blockade of Cambodia for ten years from
1979, providing legitimacy for an American-invented "resistance" dominated by
the Khmer Rouge.
  The former UN chief in Cambodia, Yasushi Akashi, who now runs the UN
operation in the Balkans, once ordered an armed UN convoy in Cambodia to turn
back when it encountered a single Khmer Rouge guerrilla. The ethnic cleansers
in Bosnia would no doubt appreciate the similarities of this appeasement. When
the causes of the destruction both of former Yugoslavia and Indo-China are
finally accounted for, the indelible imprint will be that of western power,
"destroying in order to save".



Truths, T.Eagleton, 2 June, P

Indigestible truths
The changing political scene in Northern Ireland finally seems to be creating
the climate for an Irish review of the traumatic Great Famine.

The Great Irish Famine of the late 1840s was nobody's fault. The Irish had
subdivided the land into hopelessly unviable units, bred like rabbits and were
reaping the whirlwind of their own improvidence.
  As for the relief operation, the British government did what it could --
within its ideological lights. And if those lights -- the sanctity of private
enterprise, for example -- seem a bit dim to us today, we should remember to
measure the Victorians by the moral standards of their own time, rather than
foist anachronistic modern judgments upon them.
 The Anglo-Irish landlords, far from being the feckless, brawling, gluttonous
crew they have been painted, were on the whole a responsible, socially minded
ruling group, many of whom did what they could for their starving tenants,
bankrupting themselves in the process.
  Until recently, this was the version of the Famine that came from
professional historians in Ireland. Perhaps the most that can be said for this
scandalously sanitised account is that it marks an advance on Victorian
Britain's own view of the catastrophe, where the prevailing ideology was
providentialist.
  Distasteful though it appeared, the Great Hunger signified to them a divine
displeasure with the potato and an effort on the part of the Almighty to
catapult a barbarous and desperately backward Ireland into the modern epoch. If
"greed, sloth, dirt, idleness and perversion" should fail to accomplish the
work of depopulation, so the economist Thomas Malthus wrote, then Nature "in
her wisdom is at hand to step in with pestilence, epidemic and plague".
  It was unfortunate for the Irish that this was a doctrine devoutly espoused
by the chief under-secretary of the Treasury, Sir Charles Treveylan, who
happened to be in charge of the relief operation. Treveylan was constrained in
his virulent hostility to the Irish only by the belief that, as a Cornishman
himself, he stemmed from the same Celtic race.
  The aim of his relief campaign, as Christine Kinealy has argued in her bold
study This Great Calamity, was not primarily to save lives, but to exploit the
Famine as a way of restructuring the Irish economy along modern capitalist
lines and driving out the landless labourers and the more shiftless of the
landlords.
  The English economist Nassau Senior is reported to have informed the Master
of Balliol that he feared a million dead would not be enough for this end. Even
Sir Robert Peel, whose enlightened relief policies have been rightly commended,
saw it as providential.
  Some Irish nationalists of the time -- and some today -- believed that the
British had embarked on a calculated plan of genocide. As long as the
industrial revolution had needed labour-intensive Irish grain harvests, so the
argument ran, the British looked on with equanimity while the junior partner of
the Union bulged at the seams with surplus hands.
  But now the metropolitan country wanted pastoral produce, and the two-legged
Irish beast was superfluous. Later, Irish nationalism inherited this demonic
doctrine, and it became a driving force behind "physical force" republicanism.
The Irish who had been expelled in the infamous "coffin ships" to the United
States -- those who were lucky enough to survive the crossing -- nursed their
bitterness and nostalgia and footed the bill for Fenian insurrection back home.
  But if memories of the Famine were a powerful dynamo of nationalism, so were
the social changes the disaster had wrought. The Great Hunger cleared away the
smallholders and labourers by death and emigration (about one million of each)
and in doing so helped to decimate the Irish language, which most of these men
and women spoke.
  But it thereby paved the way for the consolidation of economic farm units and
the growth of an increasing prosperous rural middle class, which would come in
time to make its strike for political independence. If the Famine did turn out
to be providential, it was not in a way that particularly benefited the
colony's masters.
  For the past few decades, "revisionist" Irish historians have been busily at
work demolishing what they regard as nationalist historical mythology about the
Famine and a good deal else. Much of this work represents a valuable
intellectual advance, as the formulaic rhetoric of Irish history text books
yields to a properly nuanced study of tension, difference and complexity. But
much of it is just as politically motivated as the nationalist discourse it
seeks to supplant. Irish historical revisionism today belongs to a more
general, postmodern scepticism of "grand narratives", but this is also very
convenient for a state that has been striving with indecent haste to bury its
rebarbative past and leap, streamlined and suitably amnesiac, into the council
chambers of Brussels.
  It is also no accident that this apparently dispassionate rewriting of the
past has coincided, more or less, with the Troubles in the North. Disinterring
the "fancied wrongs" of the Famine (the mind-bending phrase of one leading
Unionist historian) will just stoke the fires of West Belfast. Whingeing is
out, political pragmatism is in.
  As the Irish cultural commentator Luke Gibbons has remarked, it would be a
bold spirit who would accuse Toni Morrison of whingeing about African-American
grievances, whereas such a response is common currency among some middle-class
Irish liberals.
  If you're tempted to pen a favourable word about United Irishmen, or how a
few Ascendancy landlords weren't quite, in the Irish phrase, up to the knuckle,
consider first what comfort you might be giving to the IRA. It's an attitude
appropriate to a Foucaultean age -- truth in the service of power. Both for
ultra-nationalists and for ultra-revisionists, history is now and the six
counties -- just as, for the so-called "new historicism" in cultural theory,
speaking of James I, is only an oblique way of talking about ourselves.
  Perhaps a new moment in this process was marked early last year when the
Irish president Mary Robinson, herself no ally of old-style nationalism, agreed
to open the nation's first Famine museum in Roscommon. The museum, set with
appropriate irony in a big house whose owners evicted 3,000 tenants during the
Famine, is the brainchild of the arts administrator, Luke Dodd, who (no doubt
to his embarrassment) received a national honour for his endeavours.
  The graceful, moving speech of the president at the opening ceremony tacitly
signalled that talking about the Famine was now OK, as talking about the Easter
Rising of 1916 still isn't. (The latest anniversary of that event was hardly
marked in the republic).
  Just talking about the Famine is a significant act in Ireland. For it was a
horror that stunned many into a kind of traumatic muteness, for all the
bellicose rhetoric is stirred in others. The leading Irish historian of the
disaster, Cormac � Gr� da, has pointed out how nervously Irish historians have
shied away from the subject. The pioneering studies have been largely the work
of English or Irish-American historians -- or even the Israeli-American Joel
Mokyr, in his fascinating study, Why Ireland Starved.
  Whichever way you look at it, the Famine is a bad story for Britain, and
Irish historians in recent years have been reluctant to recount any such
inflammatory narratives. But the Great Hunger also produced a kind of culture
of shame, of which this academic silence seems in its way part. At the time,
men and women boarded themselves into their cabins so that they could die in
decent anonymity. There were villages after the Famine that could still speak
Irish, but didn't -- it was considered bad luck. Many of the survivors seem to
have thrust the event deep into their collective unconscious, where it has
continued to fester.
  There is little about the Famine -- perhaps the greater social calamity of
19th-century Europe -- in Joyce or the Irish literary revival, but one might
speculate that it forms an important subtext to Irish social life even today,
in however implicit a way. When he was ordered as a child to finish up his
food, an Irish friend of mine once told me, he could hear famine in his
parents' voices.
  It is not a question of reviving old myths. It isn't true, as some Irish
people still believe today, that Queen Victoria contributed a measly five
pounds to the relief fund -- she actually gave two thousand. But it is true
that grain continued to be exported to Britain and that bands of famished men
and women attacked the heavily guarded food convoys at the ports. Nonetheless,
imports of food to Ireland far outweighed exports over this period and banning
exports might have been no more than a cosmetic move. Given the grievous loss
of the potato crop -- around 50 per cent -- probably no government of the day
could have prevented a good deal of the suffering.
  Even so, the British relief operation under the Whigs was, by and large, the
most appalling cock-up. The failure to hold the harvest in 1846, before the
belated arrival of relief, led to many unnecessary deaths. A scheme of public
works was installed with admirable efficiency, but was drastically underfunded
and often didn't provide a living wage.
  The attitude of the British government throughout was one of niggardly and
churlish suspicion -- far less was spent on the Famine than on the Crimean
advent and some public works folded before the soup kitchens were in place. The
Whigs' determination to take no measures that might interfere with private
enterprise drove many to their graves.
  Disastrously, the cost of the Famine was finally thrown entirely on the
ramshackle Irish poor law, in criminal disavowal of British responsibility.
Some landlords seized advantage of their tenants' inability to pay rent in
order to throw them out destitute on the highway. And as if all this weren't
enough, the government proclaimed the crisis over, long before it was, in one
of the most heartless moves of the whole campaign.
  Not many historians have been willing to raise their noses from empirical
detail and raise some embarrassingly larger questions. Why were three million
Irish people wholly dependent on the potato in any case? Was it really a crisis
of overpopulation, or was the country (as Joel Mokyr believes) not actually
overpopulated in relation to the land that was, theoretically, available? Might
not a radical redistribution of land ownership have gone a long way towards
easing the problem? Was there even a food shortage? Or was Ireland an example
of the economist Amarthya Sen's contention that famines typically are caused
not by food shortages, but by people's inability to buy what food is available?
Perhaps there wasn't enough food in Ireland itself to go round, but what if one
considers the United Kingdom as a whole?
  We shouldn't, so revisionist historians tend to argue, project our own moral
standards onto the past. How far should this moral relativism extend -- to
witch burning and lunatic baiting? The same Irish historians who let Sir
Charles Trevelyan charitably off the hook tend to be a mite less morally
flexible when it comes to the Easter Rising or peasant assassinations of
landlords. Moral condemnation of British conduct during the Famine isn't in the
least anachronistic. It merely repeats a lot of what was said by outraged
commentators at the time, by no means all of them Irish nationalists.
  The Irish journalist John Waters has pointed out that, although the
simultaneous occurrence of the peace process and the 150th anniversary of the
Famine is an historical accident, it might nevertheless be read as significant.
If there are fewer fires in West Belfast to be stoked, the Irish might
gradually become less inhibited or disorientated in coming to terms with their
own past. Once it becomes possible for them to take full account of the
injustices against them, yet not to fester in futile bitterness or use these
wrongs as a way of evading their own responsibilities, then peace might
genuinely be said to have arrived.
  Delivering an adequate account of their history might only become fully
possible when the political problems of the North are resolved. Nations, like
individuals, have to be able to render some reasonably coherent narrative of
themselves that neither repress the darker dimensions of their history, nor
becomes neurotically fixated upon them. If they do not, they are doomed
compulsively to repeat what they cannot truly recollect. In few nations has
such repetition been more prevalent than in Ireland.
  Some people in the country today have feared that Famine fatigue would set in
almost before the anniversary was under way. But, in fact, young Irish people
seem to be taking an interest, perhaps as an entry into their own buried past.
  It would seem appropriate for the British, who after all had a little to do
with the event, to start the labour of remembrance too. And a greater openness
to the peace process on the part of their politicians wouldn't be a bad way to
begin.



Revolution, S. J. Gould, 15 September, SE

The unfinished revolution
Even after 150 years, the world outside science has yet to come to terms with
the radical nature of Darwin's ideas. In its denial of the idea of progress and
its inherent materialism, Darwinism still strikes at the roots of the beliefs
of many.

Darwin very astutely often pointed out that he was trying to do two quite
separate things in all his work. The first was to convince thinking people that
evolution had occurred, and in that of course he was abundantly successful. He
lies buried in Westminster Abbey for having convinced the thinking world that
genealogical descent is the tie that unites all creatures. Darwin also said he
had a second aim, namely, to propose a theory for the mechanism of evolutionary
change. That theory he called natural selection, and in this aim he was
phenomenally unsuccessful. When he died in 1882, with the exception of his
compatriot Wallace and some young biologists on the Continent, very few
evolutionary biologists were either strong selectionists or particularly
understood the theory.
  What is natural selection? In this barebones mechanics, it is first: that all
organisms produce more offspring than can possibly survive. Second: that there
is a great deal of variation among organisms. Third: that at least some of that
variation is inherited. You don't have to know the actual mechanism of heredity
-- Darwin didn't -- you just have to know that there is hereditary
transmission. Put those three together -- overproduction of offspring,
variation and inheritance of some of that variation -- and natural selection
follows almost syllogistically. If more offspring are always produced than can
survive, then on average the survivors will be those who are fortuitously
better adapted to changing local environments.
  The theory of natural selection is simple. So why then is it that we have
this odd situation that one of the great revolutionary thinkers, the fact of
whose great discovery is acknowledged everywhere, has not had success in
convincing the world in general about his own particular mechanism of natural
selection, though it is such a predominant theory in scientific circles? I
would like to propose that the answer is actually rather simple. Namely, that
Darwin's theory is philosophically so radical that all the psychological hopes
and social conventions of our history cry out against it.
  I want to try to illustrate the revolutionary content of Darwin's thought
through what I like to call three riddles about his life. The first is: who was
the naturalist on board HMS Beagle? The answer is not Darwin or I wouldn't be
posing it. Second: why doesn't Darwin use the word "evolution"? He did not use
it at all in The Origin of Species until the very last edition. Third: why did
Darwin, who was a quite ambitious man, though he covered it up in his shroud
of Victorian modesty, having developed the theory of natural selection in 1838,
and knowing perfectly well how revolutionary an idea it was, not publish it for
21 years -- and only then when he was pushed so hard by Wallace's simultaneous
workings-out of the basic notion?
  Who was the naturalist on board HMS Beagle? The ship's surgeon, who was
usually the official naturalist. Therefore, if Darwin was not the official
ship's naturalist, why was he on board? The answer is that he sailed as a
gentleman companion to Captain Fitzroy, the really fascinating character of the
two. Fitzroy was a very young man, in his mid-20s, of royal descent, and the
nephew of Viscount Castlereagh. Castlereagh, who was clearly a maniac
depressive, had committed suicide just a few years previously. Fitzroy had
always been told that he looked exactly like Castlereagh and he had this odd
way of seeing himself as an incarnation of his famous uncle. He, too, had
tendencies to grand fits of depression. Indeed, he broke down at one point
during the Beagle voyage and relinquished command temporarily.
  Fitzroy, I think quite correctly, feared for his own mental instability and
decided, as others had done in similar circumstances, to take along someone
effectively as a social companion. But, at the same time, Fitzroy was
enormously ambitious. He wanted to make this Beagle voyage, which was mainly a
cartographic survey of the South American coast (though they did end up
circumnavigating the globe) the greatest scientific expedition ever, but the
Admiralty would not give him enough men and material. He was independently
wealthy and was permitted to take along supernumerary passengers. Therefore,
once he decided to take someone along as a companion, it would of course be
useful if that person was expert in natural history.
  Now Darwin didn't really qualify yet as such an expert. He was then thinking
he might become a country parson, though not out of any zeal for the Church of
England. I'm sure that what he had in mind was the way in which so many curates
could effectively be amateur natural historians most of their lives. Darwin got
to go because he was of the right social class; it's as simple as that. He was
therefore acceptable; that he was also interested in natural history was an
enormous plus.
  Now why is that important? Because, on this tiny ship -- the Beagle was a
little over 100 foot long -- Darwin shared the captain's quarters. He ate with
Fitzroy almost every day for five years. He was Fitzroy's only, or major social
companion during that time. How did the two of them get along? They seemed to
have a great deal of respect and affection for one another, but it's very hard
to doubt that there wasn't enormous intellectual tension. You cannot imagine
two men more different in their most fundamental beliefs. Fitzroy was an ardent
Tory and a supporter of slavery. Darwin was an equally ardent Whig and, through
his mother, a member of the Wedgwood family, virtually professional
abolitionists, of exactly the other persuasion.
  Fitzroy had one id�e fixe -- the argument from design, that fundamental
argument of British natural history which came from Robert Boyle, namely that
God's existence and His benevolence can be manifested in the works of the
creation. It is through nature, therefore, through the works of God, that His
existence and His goodness and His properties shall be manifest. Think of it
now: for five years, day after day, Darwin and Fitzroy dining together for
every meal, and what was Fitzroy talking about -- the argument from design? "Oh
Darwin! Don't you see how the beak of that finch is so well-adapted to what it
eats? Doesn't that show the wisdom of God in placing it here" -- day after day
after day. Now if Darwin had not had doubts beforehand, I'm sure he developed
them as he listened. Here, under wraps, is one of the most brilliant
independent thinkers in western history, cooped up with the mercurial, maniacal
Captain Fitzroy, unable to refute him. It is just conjecture, but I think he
was saying: "Now I'm finally able to speak to you, Fitzroy -- this is what I
wanted to say all the time when I was cooped up and couldn't answer."
  In principle, there are two ways he might have refuted it. He might have
said: "Oh, but that observation is wrong: yes, there's a lot of good design and
harmony, but you know there's a lot of ugliness and bad design out there too,
so if you want to say God made it and it illustrates God's nature, maybe that's
not the kind of God you want, and you ought to rethink it." Well, that is a
pretty radical argument but it's not the one Darwin used. Darwin said: "Yes,
there are some instances of not-so-good design, but in general the observation
is right. Organisms are well designed and ecosystems are harmonious." The more
radical argument is to admit the phenomenon but then to say: "You know what, it
doesn't prove the existence, benevolence and attributes of an omniscient Deity.
In fact, all this arises is an accidental or fortuitous side-consequence of the
single causal principle out there in nature. Organisms are struggling for
individual reproductive success and nothing else. Good design and harmony
emerge out of that as unintended side-consequences. They're not attributes of a
loving God."
  That is truly a radical argument; so much so that we have not made our peace
with it to this day. We do not wish to do so. We wish nature to be more
sensible, if not to manifest the male God with a smile and beard up in the
clouds, at least to manifest some inherent sensibility.
  The second riddle -- why does Darwin not use the word evolution? He talks
about descent with modification. Why? Because evolution was a perfectly
well-understood, if uncommon, vernacular word in English at that time, which
meant progress. Darwin would not have used a word that meant progress because,
almost uniquely among 19th-century evolutionists, Darwin's own mechanism of
evolutionary change does not include any necessary predictable, inherent
progress. Natural selection is a theory about adaptation to changing local
environments; that's all. Darwin would not use a word that meant progress,
especially when he was trying very hard to separate the mechanism from the
common social belief, the psychological hope, the desire of everyone, to equate
this process with progress. He was quite clear on that. He was reading
Chambers' The Natural History of Creation and he wrote a marginal note in his
copy: "Never say higher or lower."
  So why do we call it evolution today? Largely because Herbert Spencer won.
Spencer was an inherent progressionalist and used the word evolution for that
reason -- and perfectly properly within his system. Spencer thought there was
progress in cosmology and economics and the history of language and biology and
everywhere, and his system took hold, much to Darwin's disappointment. Darwin
finally gave up and he used the word evolution, I think for the first time, in
The Descent of Man in 1872.
  What about the third riddle -- why did Darwin delay for 21 years? That's a
complex issue, but let me assure you that the standard notion that he was just
calmly and slowly collecting the facts and didn't really want them published
until he got all his ducks in order, as we say, clearly is not so. Consider
what Darwin did during those years. He spent eight years writing four volumes
on the taxonomy of barnacles. I don't want to say a word against them --
they're wonderful books and I'm a taxonomist myself -- but when you're sitting
on the greatest thought in the history of western science and you spend eight
years working on barnacles, that's displacement activity. And I'm not just
making it up. In his own autobiography,  he wrote: "Besides discovering
several new and remarkable forms I made out the homologies of the various parts
and I proved the existence in certain genera of minute males complemental to
and parasitic on the hermaphrodites. Nevertheless I doubt whether the work was
worth the consumption of so much time." In short, I really think it's pretty
clear: Darwin did not publish out of fear.
  So what was he afraid of? The obvious answer would be that he was afraid of
exposing his belief in evolution. Obvious, but entirely wrong. Evolution was
very well understood, very much in the air and very much argued about at the
time. It was a common, mild unorthodoxy. The fear had to arise from Darwin's
recognition of the philosophically character -- the resolute materialism -- of
his proposed mechanism.
  The first riddle shows its purposelessness, its naturalism, the second its
non-progressivism. The third shows its thoroughgoing materialism, the way in
which it breaks through that grandest of all western philosophical traditions,
dualism -- the notion that there is a realm of matter and a realm of spirit and
spirit is the higher of the two. Darwin says no, there is only matter; and what
we call mind -- which generates what we call spirit -- is the manifestation of
the complex action of matter. The brain constructs mind and it is mind that
constructs the idea of a spiritual realm in God -- so God is a double delusion.
  Most anti-Darwinian theory is united by its denial of the naturalism, the
anti-progressionalism, the materialism, that is the common thread of
neo-Lamarckism with its talk of purpose, of orthogenesis, of directionality, of
finalism, of vitalism, of all the various non-Darwinian notions. In short, we
have not made our peace with the radical character of Darwin's ideas and, more
than any of us, it is the humanists who have not made peace with that
resolutely materialistic no-inherent progress, of Darwin's world.
  I think, for some reason, many non-scientists want answers to our moral
questions to be found within science. They want -- or maybe all of us want --
big questions about the meaning of our lives somehow to be enlightened by
science, but that cannot be. That is not what science does. It is not the task
of science to define the moral meaning of lives.
  Humanists should delight in Darwin pouring cold water over the notion that
nature is inherently lovely and the source of moral messages. Science's task is
to establish the factual character of the Universe. I do not think that that is
any threat to beauty and meaning. I do not think the beauty of nature becomes
any less if we decide that it is unplanned. I do understand the distress that
people feel, but Darwin's solution is actually better because it passes to the
humanistic disciplines the tasks of thinking ethically, something that science
cannot do. I think science has separate but equal status with the Old Testament
and with other sources of meaning, called wisdom as a part of a complete life.
Do we want to read the moral meaning of our lives passively from nature rather
than actively to search for it, to drag it from our intellect and from moral
discourse? That doesn't make any sense. We should welcome Darwin's solution.



Nuclear war, P. Coyne, 4 August, SE

The coming nuclear war
The world is a long way from being safe from nuclear war. On the contrary,
despite the successes of the Non-Proliferation Treaty, the nuclear option may
be increasingly attractive to a number of states.

In July 1991, five months after the end of Operation Desert Storm, General
Charles A. Horner, commander of the US Central Command Air Forces told a
meeting of US aerospace executives that the allied air forces had destroyed
"100 per cent of the nuclear facilities in Iraq that we know about and possibly
80 per cent of all Iraqi nuclear facilities were hit." Like much else about the
immediate analysis of the Gulf war, that was nonsense.
  In fact, despite being a signatory to the Non-Proliferation Treaty and
despite the supposedly stringent safeguards of the International Atomic Energy
Agency (IAEA), Iraq had assembled an extraordinarily sophisticated nuclear
weapons programme, involving at least 20,000 people and countless facilities
that had largely escaped detection, let alone destruction, during the most
intense, hi-tech war the world has ever seen.
  Nor is Iraq alone in wanting to join the club of nuclear nations. There are
five declared members, the United States, Russia, Britain, France and China;
three undeclared, Israel, India and Pakistan; one, South Africa, that made and
then, by its own account, destroyed its weapons; three more, Ukraine,
Khazakstan and Belarus, that became nuclear by default with the break up of the
Soviet Union; and then there is a list of at least half a dozen states that
have all been suspected of attempting to get their hands on nuclear weapons.
  Other countries, most recently Iran and North Korea, but also Brazil,
Argentina, Egypt, Algeria and Taiwan, have looked seriously at the nuclear
option over the past two decades but have been persuaded, for the moment, to
comply with the NPT. Their motives were, as ever, a complex mix of nationalism,
military hubris and fear of annihilation. These motives are not baseless or
insubstantial. On the contrary, they are the very stuff of nations -- and they
will not go away. And with motive, eventually, comes opportunity; nuclear
weapons will not stay unused forever.
  Alarmist? Surely we have all stopped worrying and learned to love the bomb?
That particular irony has turned to complacency, even clich�. Certainly the
early forecast of doom have proved unfounded. In the 1960s, President Kennedy
thought that at least 20 nations would have nuclear weapons by the 1970s. Now,
nuclear Armageddon seems almost unimaginable. And was not the Iraqi experience,
hair-raising though it was, really a blessing in disguise? The loopholes it
exposed have led the IAEA to make its safeguards far more stringent, to
introduce inspection without warning and continually to review its operations
and tighten them in the light of experience. Take the fact, too, that the new
Non-Proliferation Treaty, negotiated at the UN in New York in April and May
espoused the principle of the eventual elimination of nuclear weapons this
year. Surely the world is a great deal safer place than it was a decade ago?
  Perhaps, but it depends on what you mean by safe. The huge arsenals of the
superpowers have led to the assumption that nuclear wars are always terminal.
The Mad doctrine (mutually assured destruction) has a logic that is utterly
incontrovertible. But that logic is not infinitely extensible. At some level of
conflict, it may make sense to threaten to use nuclear weapons, and with threat
comes the possibility of use. This is not abstract speculation. The world has
been on the brink of a nuclear conflict at least six times, but none involved
the superpowers. In 1973, for instance, Israeli jets were ready, engines
running, with weapons on the point of being armed, as Syrian and Egyptian
armies threatened to overrun the country at the start of the Yom Kippur war. In
1986, US spy satellites detected activity around Pakistani airfields that they
interpreted as the nuclear arming of jets and the preparations for a
pre-emptive strike against Indian installations.
  Where do you start if you want the bomb? You could do worse than take a leaf
out of the book of the acknowledged master. The Iraqi nuclear effort goes back
until at least 1975 when Saddam Hussein gave an interview to a Beirut newspaper
in which he said that his country "was engaged in the first Arab attempt at
nuclear arming". By 1976, he had signed a deal with France for a research
reactor, Osirak, destroyed in 1981 by the Israelis in an air-raid. Far from
deterring the Iraqis, it spurred them to set up a programme that virtually
mirrored the wartime Manhattan project. Iraq set out to develop an indigenous
nuclear capability and the technology to deliver. Between 1982 and 1987, it set
up a whole range of facilities to produce plutonium using reactors and to
separate the rare fissile isotope of uranium, U235 (which occurs in only about
eight out of every 1,000 atoms of natural uranium), using both calutrons,
magnetic separators, a technique abandoned by the US in the late 1940s (the
Iraqis renamed them Baghdadatrons), and the most advanced centrifuge
techniques. Only after the Gulf war, following information from a high-ranking,
"walk-in" defector and the capture of huge amounts of documentation, did the
extent of the programme become clear. One report, for example, spoke of "the
mechanism", comprising an enriched uranium core, all the necessary conventional
explosives and controls, the electronic trigger, and guidance systems for
missiles. All but the enriched uranium had already been produced and tested.
Twenty tests, involving five separate designs, had been carried out. The IAEA
concluded that the Iraqis would eventually have had the capacity to produce 20
or more Hiroshima-type bombs a year.
  How did they manage to get their hands on all that equipment? A combination
of guile, ingenuity and, most of all, money -- in short, shop till you drop.
Western companies queued up to sell Saddam sophisticated equipment, ostensibly
for peaceful uses, or at worst to make conventional arms, but which could
easily be used in the nuclear programme. A network of procurement organisations
and front companies, like Matrix-Churchill, funnelled the necessary
technologies to Baghdad. Iraqi students were sent abroad to train in the most
advanced techniques. The head of the entire programme, Jafar Jafar, was trained
at London's Imperial College. Just as important, huge amounts of information
found its way to Baghdad. The Iraqis down-loaded most of the technical
information on centrifuges by hacking into German computers. A good deal of
information is in the public domain, thanks in particular to the US Freedom of
Information Act, including computer programs capable of simulating nuclear
explosions. When the US General Accounting Office looked at enquiries for 30
reports produced by weapons laboratories, they found that 68 per cent of
requests had come from overseas. Nor were the Iraqis the only seekers of
information. Six reports, covering superfast cameras, detonators used in
weapons and machining and setting off special explosives, all technologies used
directly for nuclear weapons, had been requested by Iraq, Israel and Pakistan.
Complete engineering drawings of rockets, such as the Jupiter IRBM, are
available for about $5,000. Obsolete the Jupiter may be, but any country
intent on building its own rocket would find it a useful base on which to
proceed.
  The other, unacknowledged, members of the nuclear club also obtained their
weapons by a mixture of espionage, subterfuge and active collusion by one or
other member of the acknowledged members. Much the most formidable of these is
Israel. In his classic study, The Samson Option, Seymour Hersh details the
development of the Israeli programme from the building of the French-designed
reactor in the early 1960s at Dimona, in the Negev, to its present status,
almost as large, and considerably more independent, than the British. Its
arsenal, with its mixture of 200 or so fission and thermonuclear bombs,
includes attack aircraft, short-range missiles, the 1,200-mile range Jericho II
missile, plus planned submarine-launched, nuclear-tipped cruise missiles, as
well as nuclear land-mines reputedly deployed on the Golan Heights. Indeed
Israel's nuclear reach extends well beyond its immediate neighbours. It can
threaten, and is believed to have done so, countries like Pakistan and the
former Soviet Union, which might have been tempted to help any attacking Arab
force.
  The need for support, technical, financial and even moral, whatever that
might mean in the context, has driven Israel to choose some unlikely bedfellows
-- Iran, China and, chief among them, white South Africa. The South Africans
themselves made six bombs and had threatened an underground demonstration test
in the Kalahari if they felt themselves in danger of being overrun by a
Cuban-led invasion from Angola. By the early 1990s, with the decision to
abandon apartheid, President de Klerk ordered the bombs destroyed, along with,
according to William Burrows and Robert Windrem, in their book Critical Mass,
all cabinet records of Israeli involvement. For Pakistan, help came from China,
and from their own efforts in stealing centrifuge separation technology from
the European Urenco consortium, which produces most of the enriched uranium for
northern Europe's reactors. The Pakistanis now have their own centrifuge plant
at Kahuta. Their first bomb was based directly on a 1966 Chinese design that
had already been tested as a missile warhead and had yielded 20 kilotons.
Delivery would be by F-16 or, reportedly, dropped from the back of a Hercules
transport plane.
  The Indian effort is on an altogether larger scale. Beginning with their
1974 "peaceful" explosion in the Rajasthan desert, the Indians have built up a
formidable potential, using aid from a variety of countries -- the Soviet
Union, the US, Canada, Britain, France and Germany. The Indians deny that they
have any actual weapons, although they admit that they could make them quickly,
and there have been persistent reports that the country has at least a few tens
of bombs, either complete or near-complete. In addition, they have more than
200 nuclear-capable aircraft and have developed a range of missiles, the latest
of which, the Agni, can lob a one-ton war-head over 1,500 miles. As befits the
second most populous country on Earth and a nascent superpower, the Indians
have their sights set wider than their immediate region and in particular the
coming rivalry with China. Hence the Chinese interest in aiding Pakistan.
  But does more always mean worse, or rather, more dangerous? Some, like the
American Kenneth Waltz, professor of international relations at Berkeley, in a
recent book, The Spread of Nuclear Weapons, argue exactly the opposite. Waltz
looks sanguinely on a gradual expansion of nuclear states, asserting that this
will bring to any conflict the same stability of deterrence that it brought to
the cold war. Nuclear weapons are cheaper than the conventional alternative,
and possession of even a modest number is likely to deter an aggressor, however
large and well armed.
  The logic is dangerously persuasive. True, proliferation has been much slower
than once feared, but it has happened and the odds must be that one or two
states will join the nuclear club every decade or so. The combination of the
end of the cold war and the new Non-Proliferation Treaty may slow things down
for a while. However, against that must be set the availability of plutonium
(smuggled or not) from commercial nuclear plant, the wider dispersion of
uranium separation technologies and the possibility of new cheaper separation
techniques using lasers. Even if a complete test ban is successfully
implemented in 1996 -- and recent French and Chinese tests, coupled with moves
by some of the existing nuclear powers to allow tests below a yield threshold
of 100-200 tons of TNT, make the prospects of a total ban problematic -- the
fact that Israel, Pakistan and South Africa, have built bombs without needing
to test them, because they used proven designs, will encourage others.
  The real impetus to proliferation will come from the move from a bipolar to a
multipolar world. Even if a US-Russian war can be virtually ruled out, there
are at least three other areas of potential conflict. First there is the Middle
East, not necessarily a straightforward Israeli-Arab war, but rather an
outbreak of instability that could threaten other countries on the periphery.
Already, the southern European countries, Spain France and Italy, have
privately expressed fears of a nuclear attack from North Africa. If, as seems
more likely than not, Iran acquires nuclear weapons, then there will be heavy
internal pressure on others, most notably Turkey, the most populous and heavily
armed state in the region, but also Egypt, Algeria and Saudi Arabia, to do
likewise.
  The second area is the Indian subcontinent. The Hindu-Muslim dispute has been
going for at least 700 years and is not going to disappear quickly. The third,
and most problematic, is the China Sea. Leaving aside the Korean problem, which
may well be solved by some form of unification, there is the much deeper
question of whether China will try to assert its traditional hegemony over the
area. The creation of a deep-water navy, including aircraft carriers and
submarines armed with ballistic missiles, together with its muscle-flexing over
the Spratley Islands, claimed by most of the states in the area as well as
China, suggest that it may well do so. Last weekend, the president of Taiwan,
Lee Teng-hui, hinted that Taiwan may revive its nuclear programme. The
reasoning is that only nuclear weapons can counteract China's overwhelming
advantages in conventional arms and manpower. It is a logic likely to appeal to
other states in the region with large population and rapidly developing
industries -- Indonesia, the Philippines and even Vietnam. The Indians may well
also be drawn in, while Japan responded to the most recent Chinese test by
pointing out what everyone already knew: that they possess large stocks of
plutonium and could make nuclear weapons very quickly, certainly within months.



Serb lines, P. Hockenos, 2 June, CS

Behind Serb lines
The seizure of UN soldiers has thrust Bosnia back into the headlines. Paul
Hockenos reports from the self-styled Republica Sprska, the Serb-occupied part
of Bosnia, on the authoritarian regime of Radovan Karadzic.

For a war-drained city in an internationally-isolated statelet, Banja Luka
keeps an upbeat face. Nestled between two gentle mountain ranges, its broad,
tree-lined streets and pastel facades have a prosperous, distinctly central
European flair. At one of the ubiquitous garden cafes or outdoor pizzerias, a
Banja Lucko pivo still compares to the draughts of any brewery south of Vienna
or east of Prague. Perhaps a bit snobbishly, Banja Lukans always emphasised
their affinity to the cultural sphere of mitteleuropa rather than the Balkans.
Even Serbs admit that before the war they felt more at home in Zagreb,
an hour's drive west, than in Belgrade or Sarajevo.
  But today in the Bosnian Serbs' self-styled Republica Srpska (RS), Banja Luka
is no longer renowned as a well-off university town, as the industrial hub of
northern Bosnia, or as a quiet multinational city of cafes and hot springs. The
largest city in Bosnian Serb-held territory, it carries an ignoble reputation
as the capital of ethnic cleansing. Before the war, Banja Luka's population was
roughly half Serb and half Muslim and Croat. Today, there is only a tiny
fraction of non-Serbs in the city -- and every month more leave.
  Banja Luka's reputation as a symbol for the ethnic policies of the Bosnian
Serb leadership is one reason that foreign journalists are strongly discouraged
from visiting the region. For all their complaints of media bias, the Bosnian
Serbs make it as difficult and uncomfortable as possible to cover the war "from
their side." No journalist make a move anywhere in RS without the express
consent of Sonja Karadzic, the republic's first lady and press officer. Among
reporters, she is considered the wicked witch of Pale, the Bosnian Serbs
village capital. And she looks every bit the part, with her fluorescent green
(sometimes violet or pink) fingernails and outlandishly garish outfits.
  All trips in RS begin in Pale, whether or not the former mountain resort
outside Sarajevo is near your destination, which, in the case of Banja Luka, it
is not. There, after securing a press card, you are sat down for an informal
little chat with Branko, a gruff, barrel-chested Serb in his mid-thirties.
Branko explains "the Serb side" of ethnic cleansing (Muslims and Croats want to
leave, we just help them), the siege of Sarajevo (the Muslims and Croats broke
the ceasefire) and the Contact Group peace plan, which the Serbs refuse to sign
(this is a war that we didn't even start, why should we give up land we won?).
All journalists must travel in RS with an official interpreter in order to
avoid "false interpretations", he says, the kind that have unfairly turned the
world against the Serbs. As with every indignity in RS, you have to pay for
this escort too, who presumably reports directly to the police afterwards. The
cost is $100 a day plus all expenses -- that is, if you are given clearance at
all.
  One legitimate factor that can obstruct passage into northern Bosnia is the
military situation around the strategically critical Brcko corridor. This thin,
five mile-wide bottleneck in north-eastern Bosnia connects all of northern
Bosnia and Serb-held Krajina in Croatia with eastern Bosnia and Serbia proper.
Were the Bosnian government army to cut the corridor, more than two-thirds of
the territory that the Serbs control would become a giant enclave surrounded by
Croatia and government-held Bosnia.
  For this reason, the corridor has recently been the scene of heavy fighting.
Nevertheless, journalists in RS worry less about the arbitrary bullet or piece
of shrapnel than the arbitrary actions of an unpredictable, despotic regime. As
inflated as the term "fascism" is throughout the Balkans (everybody is
"fighting fascism"), the RS has all the characteristics of the real thing:
concentration camps, extensive paramilitary and secret police forces, and an
extreme clerical-nationalist ideology. The RS takes its enemies, real and
imaginary, very seriously. Swiss and German aid workers were recently
imprisoned for months on trumped-up spying charges and then expelled from the
country before their trial came up.
  The Bosnian Serbs, even those who defend the regime, know very well what kind
of a state they live in. Nobody wants to be seen talking with a foreigner on
the streets. When I finally arrived in Banja Luka, the first thing my contact
there whispered to me was: "Are you sure no one followed you?" He quickly
whisked me off to his flat.
  It was sheer good fortune that I made it there at all. The day that I arrived
in Pale, the Croatian army launched an offensive against the rebel Serb-held
pocket in Western Slavonia in Croatia, which the Serbs had controlled since
1991. The attack triggered an exodus of thousands of Croatian Serb refugees
across the river Sava into the Banja Luka region. Sonja Karadzic saw a golden
opportunity for the international press finally to witness the Serbs as victims
-- as they see themselves -- and half an hour later we were whizzing through
the battle-charred Drina region toward Banja Luka.
  Before the war, more than half a million Muslims and Croats lived in the area
of northern Bosnia now held by the Serbs: there are fewer than 70,000 today. As
you drive through eastern and northern Bosnia, the former Muslim villages are
ransacked ghost towns, the houses picked clean of doors, windows, anything
looters could carry with them. Unlike in Sarajevo or Mostar, the buildings here
are not riddled with bullet holes or torn apart by grenades; they were
plundered with human hands.
  In Banja Luka, most of the former houses of Croats and Muslims are
identifiable by small blue-white-red Yugoslav flags that their new occupants
pin or paint on their doors. "That means a Serb lives there now," sighs
Ljubica, a teacher of French who is the child of a mixed Serb-Muslim marriage.
"They want you to know." In other words, they are not ashamed that someone was
chased out of the homes in which they now live.
The most striking physical evidence of ethnic cleansing in Banja Luka is the
empty, rock-strewn lots that stand out like scars along shady shrub-lined
streets. It was here that some of the oldest and most elegant mosques in the
Balkans stood before Serbian extremists dynamited them. Before the war, there
were 16 mosques in Banja Luka. Today, not a single one is standing. In fact,
there is not a minaret to be found anywhere in Serb-controlled Bosnia.
  On the corner where the stately Ferhad Pasha mosque stood for 400 years, a
pack of stray dogs lounges in the hot midday sun. The homeless animals belonged
to Croats and Muslims before they fled the city. The first wave of ethnic
cleansing, which began with the out-break of the war in April 1992, relied on
bombings, murders, rapes, mass arrests and arson to force Muslims and Croats
from their homes. Almost all non-Serb professionals were fired from their jobs.
  "One day my grandson was stopped just outside our house here," a withered
Muslim peasant woman explains as she and a friend tend a little garden patch
along the sparkling river Vrbas. "He didn't have his ID card with him and they
wouldn't let him get it. They beat him up so badly that he lay in bed for a
month afterwards." As a young blond-haired man on a horse-drawn cart passes us,
the other woman puts a finger to her lips. "Quiet," she says, "It's those young
ones from the village, there's nothing they won't do."
  The women say there are fewer "problems" now than two years ago. When Muslim
houses are shot at or robbed, the police are more responsive, although, in the
end, nothing happens to the offenders. "We old folks, we're the only ones left
now," says the first woman, whose children and grandchildren are in Canada,
Croatia and Sweden. "I was born here under the Austrian monarchy, and I'll die
here, one way or another."
  Ethnic cleansing is now in an "end phase" -- the term foreign observers have
used to designate the more subtle forms of intimidation designed to force the
remaining Croats and Muslims out of RS. Muslim and Croat men, for example, are
conscripted as labourers, often on the front line. As the authorities know,
most would rather leave their homes than risk their lives digging trenches on
the "hot fronts".
  It is now also cheaper to be ethnicly cleansed. In the past, people paid
500-600 German marks ($350-450) to acquire the papers necessary to get out.
Today, the city's civil migration authorities charge only 250 marks. At an
informal flea market along a crumbling wall of the old city fortress, Muslims,
Croats and gypsies gather to sell their last possessions to get the money to
leave. On old blankets, they display record collections, tool sets, door knobs,
televisions, anything they can't take with them.
 Six gypsy women with patchwork dresses and head scarves squat in a circle,
rolled-up cigarettes bobbing from their cracked lips. "One day some toughs came
and said to us "What are you doing anyway? Get lost or you'll be sorry","
explains one woman. She and her husband already have their papers to leave but
with the Croatian military offensive in Western Slavonia the borders have been
closed to everyone. "Now you leave," she motions to me with her head, "or I may
never get out of here."
  Just days after the Croatian offensive, which retook the Slavonian pocket,
Banja Luka's Roman Catholic Croat population began to feel the repercussions,
just as they feared they would. Three Catholic churches and three chapels were
blown up, a priest and a nun killed.
  At the Banja Luka parish, Bishop Franjo Komarica is a voice of tolerance as
well as the unofficial spokesperson for the Catholics and other minorities
still in northern Bosnia. "We Catholics here have nothing to do with Western
Slavonia or Western Hercegovina or the Croatian army, yet Serbian radicals
justify this violence as retribution," says the urbane, silver-haired bishop,
who began a hunger strike last week to protest against the persecution of his
people. His tone of voice and directness of manner express an urgency bordering
to desperation. Two-thirds of his diocese has fled the country and most of the
others are preparing to join them. Over the past three years, Komarica has been
kidnapped several times, shot and beaten nearly to death. "Our people have
lived here for 1,700 years and now we're on the verge of complete
annihilation," he says. "I refuse to accept that the Serbs, our Ortodox
brothers, will accomplish what neither the Huns nor the Avars, the Mongols nor
the Tatars, the Turks nor the communists managed to do. What a terrible,
humiliating disgrace this would be for the Serbs."
  The truth about ethnic cleansing in RS has been no secret, he says, but the
world has refused to take action to stop it. He maintains that the west has
made itself complicit by watching on as the Serbs' ethnic policies reach their
final conclusion. "The west is tacitly accepting what is happening here," says
Komarica. "I don't accept it! I refuse to talk about a definitive end phase of
ethnic cleansing. I want to talk about concrete provisions for the return of
refugees and the protection of their human rights."
  The Bosnian Serbs' goal of an "ethnicly clean" territory is a logistic
prerequisite to their large dream, namely to be part of a Greater Serbia. It is
a Greater Serbia for which the Bosnian Serbs are fighting and only a few
express sincere remorse about the means employed to establish it. "What's
happened in Banja Luka is a real tragedy," says Maria, a Serb and 29-year-old
mother of two. At her wedding, her best friend, a Muslim, was her bridesmaid.
"The Muslims and Muslim culture were part of Banja Luka," she says. But still
she wonders why the city needed 16 mosques when it only had one Orthodox
church. "They shouldn't have destroyed all the mosques," she reflects, "but 16
was really too many."
  Despite their military successes, the Bosnian Serbs' hopes of a speedy union
with Serbia proper have been put on hold. Last year, Serbian president Slobodan
Milosevic after failing to persuade RS president Radovan Karadzic to accept the
Contact Group peace plan, which gives the Serbs 49 per cent of Bosnia's
territory, broke off relations with Pale and put RS under sanctions.
  Few Bosnian Serbs believe that the Milosevic-Karadzic split is for real or
that Milosevic has backed away from his support of Greater Serbia. They may be
right, but of late Milosevic has given every indication that he realises that a
state composed of Serbia proper, Montenegro, RS and Serb-held Croatia is
militarily, politically and economically unviable. The most vivid illustration
recently was Belgrade's muted response to the Croatian sweep of Western
Slavonia. Milosevic let the Croats take a chunk of Greater Serbia without a
fight or even serious protest.
  After three years of fighting, the Bosnian Serbs are battle-weary and
frustrated with the rampant corruption and war profiteering that Karadzic
condones. "When the war first broke out, you couldn't hear a word against
Karadzic," one Bosnian Serb soldier told me. "Today everybody curses him." It
is the corruption more than the war itself or Karadzic's refusal to accept the
peace plan that infuriates Bosnian Serbs. In RS, war profiteers -- the
political elite among them -- are a class unto themselves. Since Karadzic
relies directly on this criminal class for his power, he refuses to clamp down
on their enterprises.
  Recently, the RS police forces, which are under Karadzic's direct control,
have been beefed up at the expense of the military, reflecting Karadzic's
growing estrangement from the army leadership as well as the population.
Although Belgrade is closely watching the power struggle unfold in Pale,
Milosevic is simultaneously building a base of political opposition outside
Karadzic's ruling party. Logically, he's focusing on Banja Luka, which even in
communist Yugoslavia resented Sarajevo's authority, and now finds it unbearably
humiliating to be governed from such a miserable little hamlet as Pale. Banja
Luka is also the chief financier of RS and the war, another point of friction
with the Pale warlords.
  But whatever their criticism of Karadzic, shortages and the war, most Bosnian
Serbs (as well as the potential opposition) stand firmly behind the larger
political goals of the RS leadership. The regime's propaganda and the
experience of the war has created a tightly-spun logic that leads to one set of
conclusions. The ultimate source of this logic is Belgrade, where Milosevic
encouraged and aided his puppet Serbian statelets from the very beginning. But
as easily as Milosevic, the shrewd, cynical pragmatist, let go of Western
Slavonia, he could also abandon RS. Then the Bosnian Serbs would have their
economically ruined police state all to themselves.



Divorce, Y. Roberts, 8 December, S

Till divorce do us part
That marriage is going through a sticky patch is indisputable. But with the
Family law Bill, currently on its second reading in the Lords, comes the ideal
opportunity to make a rational reappraisal of the whole institution.

In the past few weeks, as the machinations on the Divorce Reform Bill and the
Family Homes and Domestic Violence Bill (now fused into the Family Law Bill)
have unfolded, a marriage has taken place, a potentially lethal union, which
threatens to spawn so much propaganda that too many may be persuaded that the
only news about matrimony, parenthood and relationships is bad news -- and the
cause of our alleged woes is simply a lack of old-fashioned moral fibre.
  Such a conjoining, between the right-wing Darby and the once liberal Joan,
particularly matters now because the passage of the Family Law Bill (its second
reading began in the House of Lords last week) is permitting the first major
parliamentary debate on marriage since 1857. This is also happening at a time
when the formulation of social policy for what might be a future Labour
government appears disturbingly like wet cement -- whoever puts a foot down
heaviest leaves the deepest mark.
  Predictably, in the preliminary battles on the proposed legislation, the
Daily Mail has produced numerous variations on "Marriage on the Rocks", "The
Family Under Threat", et al. More surprising in this propaganda exercise have
been the recruits from the former liberal, even left-of-centre, ranks.
Presumably their middle-class optimism about human nature has been dashed by
the vision in much of the media of hordes of shell-suited, gold-decked,
unemployed, feckless but fecund teenage members of the underclass rapidly
polluting not only their own estates but the des res areas that others with a
job and a pension (and, possibly, even a by-line) inhabit.
  So, for instance, the Observer, once a platform for the humane and tolerant,
now has a harsher and far less reasoned public voice in Melanie Phillips. Who
killed the family? she recently asked rhetorically in a BBC2 documentary,
against the visual back-drop that contrasted idyllic images of the traditional
1950s family with 1990s balaclaved thugs rampaging on the streets. The
participants in the film were mainly so busy helping Phillips to kick the
corpse, none stopped to consider why it was still breathing so heartily.
  But let's persist with the propaganda since what literally hits home is a
vivid image of the rise of the cohabitee, the existence of the redundant (at
home and at work) suicidal male and the end of dual parenthood with the
consequent collapse of family values, even morality itself. It is a compelling
chain reaction, not least because it lays the blame squarely on the selfishness
of the individual. So, for the right and its more recent allies, it also
offers the easiest fall guys and the sweetest solutions, not least since these
"solutions" mainly involve punitive cuts in public expenditure.
  Right-wing Darby and ex-liberal Joan both agree on who has been responsible
for the demolition job: libertarian views enshrined in the permissive 1960s
which elevated rights above values; divorce laws which for three decades have
progressively loosened the bonds of wedlock; feminism, which has urged women to
do their own thing; and a welfare state that allegedly pays too well for too
many to do absolutely nothing except sin at leisure.
  The so-called "new moralisers" have claimed that if the proposed legislation
in the Family Law Bill  is successful, cohabitation will have the standing of
marriage, divorce will become as easy as sneezing, and the concept of fault
will be rubbed from the collective conscience. (No matter that, in a Guardian
poll, 60 per cent of women declared themselves in favour of no fault.)
Matrimony is about to crumble, the new moralisers claim, and so therefore will
civilised society.
  The solution? Make divorce tougher, take away state support from
single-parent mothers and broken families, restore the stigma and the shame.
Never mind the private hell, ensure the public veneer of stability and
continuity in marriage has been tidied into shape again.
  Two main flaws lie in this theory -- first we already know it won't work. For
instance, experiments in welfare cuts in the US have told us so. While, in
spite of the fact that the majority of women find their income drops by 24 per
cent on divorce, still seven out of ten marriages in Britain are ended by wives.
  The second flaw is that the right and its new allies measure "morality" and
social stability only by the health and appeal of marriage. It ignores the
increasing evidence that many individuals -- heterosexual and gay -- are
attempting to forge a long-term commitment, outside that union. The right's
rush to provide answers before many of us are even sure of which are the most
useful questions to pose is also damaging because it distracts attention from
an obvious message. The message is: now, as always, it requires more than moral
fibre to keep a relationship alive and well.
  In a book published next month, Options for Britain, Kathleen Kiernan, reader
in social policy and demography at the London School of Economics and Political
Science, asks why there is so much more divorce in Britain (160,000 a year)
than in other northern (non-Catholic) European countries. The absence of family
policies is one factor, she argues. As is the obsession with supporting only
one model of family life -- marriage -- when other countries have found
themselves able to embrace the diversity in family life without a collective
moral collapse.
  What also matters, Kiernan argues, is education. Britain invests least and has
the smallest proportion of young people who continue their education past 16.
The 30 per cent who are least educated tend to be those who marry young, and
they also have a higher rate of unemployment. The younger the marriage, the
higher the chance of divorce. Elsewhere in Europe, you get a life before you
get a spouse. It's not the only route to longevity in relationships, but it
helps.
  In Britain, 50 per cent of divorce happens in the first ten years; a major
danger area is the birth of children. "Even the most egalitarian relationship
can begin to show the strain when a child arrives," Kiernan says.
  Britain has seen a sharp increase in the number of divorces that involve very
young children. A divorced mother without childcare or qualifications
consequently becomes dependent on the state, not permanently but for a period.
Is this moral lassitude or a reneging by the government on its duty to equip
the young adequately enough so that their goals are lifted higher than
premature motherhood and a cycle of relationships that may only diminish?
  Among the measures Kiernan would like to see is a move away from the
husband-wife axis in marital affairs, to policies that place the child first.
An end to poverty that effects one child in three wouldn't be a bad start. And
is it here that a moral consensus is most required? A child might weather one
break up almost intact -- but two or three or four?
  Marriage too soon and babies too early is only part of the mosaic that makes
up the modern attempt to manage (or mismanage) relationships. "One doesn't have
to get anywhere in a marriage," says a character in Iris Murdoch's A Severed
Head. "It's not a public conveyance."
  Of course, today, husband and wife have to travel extraordinarily far and for
very much longer than many marriages in the past, often truncated early by
death. "You're not the man I married," isn't a sign of condemnation; today, the
remodelled husband and wife is advocated by many counsellors as the way to keep
a relationship alive.
  Contrary to the view of the right, while some in the 1960s may have flirted
with communes and shared parenting, the majority today appear to seek a union
of two people -- 80 per cent with children -- ideally for many years. Two out
of three marriages, if present trends continues, will endure until the death of
a partner -- and that may mean 30 or more years of marital duty. Again, four
out of five children will remain with their two natural parents.
  Alongside that core of continuity is also evidence that many people are
taking marriage more seriously, delaying both it and parenthood. The average
age to walk up the aisle has risen to 29 for men, 27 for women. Childbearing
has also been postponed and for one out of five women, it has been cancelled
altogether. At the same time, the number of divorces has begun to drop -- by
10,000 according to most recent figures. And the percentage of divorces
involving children has also declined from 60 per cent in the 1970s to 55 per
cent now.
  Stability saves society money -- divorce is alleged to accumulate bills of
over �2 billion a year. We're also told that, mentally and physically, a happy
union benefits individuals and obviously children. "Marriage and divorce is a
major health issue," Penny Mansfield, director of One plus One, the marriage
and partner research charity, points out. (One reason why the suicide rate has
increased so dramatically among young men may be because unemployment has
exiled them from domesticity. Sadly ironic, since for decades men allegedly
placed so much energy in the avoidance of being "tied down".)
  We appear to know what we want -- the difficulty of course lies in achieving
it at a period in history when the odds against its success have arguably never
been higher. It was tough enough when everyone knew their place -- father as
breadwinner, mother as homemaker -- and the alternatives for the wife outside
the family non-existent. The bonds of marriage were reinforced then by what
historian Lawrence Stone describes as "the overwhelming ideology of female
subordination and inferiority, drilled into every member of society... and
(present in) both elite and popular culture..."
  In modern times, the expectation of what marriage might deliver has literally
grown beyond what Romance can possibly deliver. In 1972, Kathrin Perutz wrote:
"Marriage is the hell of false expectations, where both partners, expecting to
be loved, defined and supported, abdicate responsibility for themselves and
accuse the other of taking away freedom."
  Now, terms of endearment are constantly under renegotiation, male and female
roles are less clearly defined, expectations are no longer automatically
clipped by convention, the very foundation of a family can be shaken by
long-term unemployment or two parents working unbearably long hours, and male
and female concepts of what marriage actually promises continue to be
mismatched.
  The Marriage Research Centre, in a series of books, has been following 65
couples from their wedding day in 1979. By 1991, the numbers had been reduced
by divorce to 48. What influenced fulfilment in these partnerships has been
initial expectations, the management of conflict, and awareness of the need to
compromise, and continued communication. A lot of husbands believe
communicating their feelings isn't required, just being married is enough.
"It's important for each partner to have a realisation, an appreciation and a
respect for what the other does," said Fiona McAllister, a researcher on the
project.
  It seems so obvious, even mundane, and yet in the flood of moralising there
is an acute danger that the simplest measures, which can make the difference
between survival and death in a relationship, are overlooked. The divorce
reform in the Family Law Bill (divorce after a year that involves mediation)
might help to give marriages a decent burial with as little bitterness and
humiliation as possible -- what we also need are the tools to give a
relationship close to collapse that extra-chance of life.
  Marriage still has an appeal, but its popularity is in decline. In 1994,
there were 300,000 marriages, the lowest for almost 50 years. At the same time,
cohabitation is on the increase. Seven out of ten couples live together for at
least two years before marrying, while a smaller minority cohabit permanently.
"In marriage you are chained," pointed out Brigitte Bardot. "Living with
someone is a mutual agreement renegotiated and re-endorsed every day."
  One in almost three children is now born out of wedlock -- but of those, half
are registered to parents living at the same address. Is this a flight from
commitment, "living in sin", defined last week by the Bishop of Canterbury, or
an attempt to redefine commitment outside marriage? If that is the case,
cohabitation has so far had a lower rate of success. The divorce rate for
marriages that begin in cohabitation is four times higher than those that do
not. The long-term research on cohabitees with children has yet to emerge.
  Is the breakdown rate of cohabitation proof of a selfish lack of staying
power? An abdication of responsibility? Or do we need to shore up cohabitation
-- particularly once children are involved -- with rituals and an official
acknowledgement such as the kind seen in registered cohabitation in France and
Sweden?
  "Rituals are a way of articulating something very hard to say otherwise,"
Penny Mansfield argues. "It's an important moment, done in front of others. It
says, for instance in terms of cohabitation, what comes after is different from
what has gone before. It allows a couple to seek the support of others and
makes clear an obligation to try and make the relationship work. Otherwise, the
danger is we drift."
  In Invented Moralities, Sexual Values in an Age of Uncertainty, Jeffrey
Weekes writes of how Aids has introduced a "remoralisation" into the lives of
many gay men. Pre-Aids, a single promiscuous life was for some a conscious
rejection of the values of the nuclear family. Now "a web of reciprocity", he
says, is being spun -- in which, "a responsibility for the self requires a
responsibility for others". An effort considered admirable with marriage, but
denied the official approval of society if the participants are gay.
  Penny Mansfield is in favour of focusing less on the structure of
relationships and more on the substance. She argues that commitment is what
counts, whether in gay or heterosexual arrangements. "We need to find a voice
which articulates commitment and what it entails. One which puts it in a
language which reflects the reality of modern partnership," he says. "We need
to feel that what we do can make a difference when things go wrong and we need
to have a clearer idea of the consequences of the decisions we take."
  There are more abstract areas to the business of togetherness that
nevertheless can be better supported by society. For instance, how can we
correct the imbalance of male-female expectation? What really makes successful
unions work? What makes a man or woman hold on during the bad times when there
is no vicar in the pulpit to shame him or her into compliance; no public
censure if they cop out? When will we accept that many women leave a
relationship not because they believe they have a right to happiness but
because they know they have a duty to themselves and their children to avoid
exploitation and abuse? (So, married or unmarried, they need constructive
support of the kind the Family Law Bill offers on the issue of domestic
violence.)
  The forthcoming debate on marriage deserves better than propaganda. It gives
us all an opportunity to ask questions -- the answers to which might prove as
uncomfortable to the left as to the right.
  Is the legal institution of marriage worth preserving? Do we wish to enhance
cohabitation? Can we acknowledge gay relationships? Are we prepared to place
the child at the centre of policy-making? How best can we inculcate a sense of
self-worth, so that any individual entering a relationship has a clearer idea
of what he or she can offer, and what he or she might realistically expect in
return?
  Is it possible, in short, to engage in a more tolerant debate so that we can
collectively decide what we mean by "family values" and rights and duties and
obligations? And to decide how best we can transform any conclusions into a
climate of opinion that transcends both left and right, and lobbies for the
changes that are so desperately required?
  It's a lot to expect, but so is happy ever after.



David and Goliath, B. Kagarlitsky, 21 July

David and Goliath
The public humiliation of the authorities in Moscow by Chechen commander Shamil
Basayev has put new heart into Russia's demoralised millions.

  In recent times a new profession has flourished in Russia: that of 
image-maker. Fast-talking young men and women explain to us that the success 
enjoyed by political figures depends on the cut of their jackets. But even the 
best tailors would be powerless before a balding man in camouflage fatigues and 
a green headband. And no jacket could have saved the Russian prime minister, 
Viktor Chernomyrdin, as he humbly held the telrphone handpiece, while on the 
other end of the line a sombre, bearded man dictated terms to him.
  This man had achieved the impossible. He had stirred up a society that had 
seemed hopelessly intimidated and demoralised, mainly because of its firm 
conviction that nothing could be done to counteract such authorities. As it 
turned out, the Russian authorities could be combated. But the beared man with 
the sad eyes has taught us all a terrible lesson: the only language this regime 
understands is that of force. Whatever might be the final verdict on the 
Budyonnovsk episode, a psychological turning-point has been reached.
  On Wednesday 14 June 1995, the name of the totally unremarkable town of 
Budyonnovsk began, improbably, to be heard of throughout the world. If we are 
to believe Shamil Basayev, the commander of the Cechen contingent that seized 
the town, all this happened by chance. At his first press conference on 16 
June, Basayev related that he and his followers had hoped to reach Moscow, and 
to "wage war for a bit" in the Russian capital. But the officials at the 
checkpoints demanded such huge bribes from the guerrillas that there was only 
enough money to get to Budynnovsk.
  After bursting into Budynnovsk, Basayev's contingent raised the Chechen flag 
over the local administration building, and laid siege to the police station. 
For several hours, the Chechens were in full control of the town. Disorderly 
gunfire raked the streets, killing peaceful residents. Initial reports from 
official sources stated that there were two groups of terrorists. One, 
well-armed and organised, was dressed in camouflage uniforms. The other group 
was in casual clothing, and its members rushed about in panic, firing at 
anything that moved. Later, the newspaper Moskovsky Komsomolets acknowledged 
that the second group were not terrorists, but the police reinforcements who 
had been sent to protect the townsfolk.Most of the civilian casualties on the 
first day were, it seems, the victims of this "rescue operation".
  In the firefights with the militia, and with the army units that soon arrived,
eight of Basayev's fighters were killed and twelve wounded. The Chechen wounded
were taken to the hospital, and were followed by Basayev's whole contingent.
More than 1,500 hostages were seized. The authorities blamed Basayev and his 
fighters for all the civilian deaths in the town on 14 June, but many of the 
dead had, in fact, been caught in the crossfire.
  The Chechen capital, Grozny, and mountain villages were devastated by aerial 
bombing and artillery fire. Many of Basayev's relatives were among the dead. 
For the Chechens, vengeance is sacred. But Basayev ordered that only military 
personnel, government officials and police be killed. In prectice, of course, 
everything happened differently. Civilians were gunned down on the streets of 
the town: the Chechens were not sparing with their cartridges. Budynnovsk's 
residents, though, soon realised they were in less danger from the Chechen 
"terrorists" than from the government forces who came to "defend" them. 
  The hostages later described how they had come under fire from snipers and 
from the government's special forces troops. This happened every time the 
hostages appeared at the windows or came out of the hospital buildings to get 
water, and when the doctors were working in the operating theatre. Some of the 
male hostages asked the Chechens to give them rifles, in order to defend 
themselves against the besieging soldiers.
  Basayev called for an end to the war and for negotiations between Russian and 
Chechen leaders. He also demanded that journalists be allowed to speak to him. 
The authorities, as usual, refused to collaborate with the press. The 
government representatives, for their part, offered Basayev money and their 
aircraft. Official spokespeople described these offers as "reasonable", and 
decried the refusal of the "terrorists" to be bought. Once this refusal had 
become clear, the government forces launched an assault of unprecedented 
ferocity on the hospital.
  The first shells came crashing into the sections where the women and children 
had been gathered. This was no chance error: the windows of these rooms had 
been marked with sheets, and there was no shooting from this direction. The 
fighters shielded the hostages from the fire, placing people beneath beds and 
explaining how they could protect themselves from bullets and sharpnel. Despite 
this, at least 30 people were killed.
  The Chechens suffered two dead. More than ten of the attackers were killed, 
and several dozen were wounded. Five armoured vehicles were set on fire. After 
four hours of the unsuccesful assault, the government forces retreated.
  Basayev was shocked by the ferocity shown by the government forces. 
Immediately after the attack, he ordered that the women and infants be freed. 
But the gunfire resumed. A second assault ended as ignominiously as the first. 
People were not only killed in the hospital. The journalist Natalya Alyakina 
died when she was shot in the back from a highway checkpoint after having her 
documents checked and being wished a safe journey. Several people were severely 
wounded when an armoured personnel carrier opened fire in the street with its 
cannon. This was after Basayev had left the town.
  While people were dying in Budyonnovsk, Yeltsin was embracing western leaders
in Halifax, Canada. He was gently taken to task, and in turn told the press 
horrifying stories of "bandits...with black face-masks". This was all so 
repellent that even the official television did not refrain from irritated 
commentaries. In addition, the autocrat let slip the fact that he had taken the 
decision to order the assault before flying out. Charging the government with 
the task of firing on the hostages in Budyonnovsk, the ruler of "democratic 
Russia" headed off to Canada and his western friends.
  It is not hard to see that if the assault had been successful, all the 
deaths would have been blamed on Basayev. But his contingent fought off the 
attack. After the failure of the assault, Basayev held another press 
conference, where he showed the results of the attempt to storm the hospital. 
This, it seems, was also the turning point in the Budyonnovsk tragedy. 
Residents began to gather around the hospital, threatening to create a human 
chain between the government forces and the Chechens. The authorities did not 
have the resolve to continue slaughter.
  Little by little, a new image of Basayev began to form in the public mind. 
This was not the image of a vicious terrorist who had seized a peaceful town, 
but of a bold, determined man fighting for his principles. The hostages who 
emerged from the hospital spoke of the Chechens as heroes. Among 
the inhabitants of Budyonnovsk and throughout the north Caucasus, the 
bitterness spread -- bitterness not against Basayev, but against the 
authorities. A correspondent for the progovernment Moscow daily Izvestiya said 
women came out of the hospital screaming, pointing to the soldiers surrounding 
the building, "they're murderers! They were shooting at us!"
  By the evening of Saturday 17 June, Basayev's victory was obvious. The 
government entered into public negotiations. Prime Minister Chernomyrdin 
personally telephoned the besieged hospital. At the demand of the Chechens, 
all the negotiations were held in the presence of the press, and millions of 
television viewers watched the humiliation of Chernomyrdin, reporting to 
Basayev on the fulfilment of his conditions. Basayev cut short all attempts by 
the Russian premier to impose his own demands.
  The public humiliation of the authorities was observed in Russia with 
amazement and delight, since it is not only Chechens who have scores to settle 
with the regime. People have not forgotten the economic reforms that have 
ruined half the country, the shelling of the parliament, the destruction of the 
Soviet Union, the fantastic corruption, or the contempt for human life. Since 
the winter of 1991, almost every decision taken by the authorities has 
displeased the bulk of the population. But no one has been able to do anything 
about it. Society has not accepted the war in Chechnya, but has been powerless 
to stop it.
  For five years, the authorities have shown a total immunity to being 
influenced by democratic means. They have changed laws and the constitution 
when they have considered this necessary, and have shut down elected organs of 
government when these have played an inconvenient role. And they have done all 
this with the support of the democratic west.
  When government propagandists began babbling to the effect that "the assault 
was nevertheless necessary in order to resolve the crisis" they were correct 
in their own way. It was the failure of the assault that forced the authorities 
to negotiate. The decisive factor was not the obvious justice of Basayev's 
demands, but the inability of the 3,000-strong federal group of forces in 
Budyonnovsk to deal with 100 Chechen fighters.
  Basayev's main demands were accepted. Chernomyrdin declared a ceasefire, 
and a government delegation flew to Grozny for discussions.
  Basayev returned to Checnya in triumph. But before this happened, his 
contingent had to follow a trail of many kilometres through Stavropol province 
and Daghestan. Basayev was accompanied only by those hostages who agreed to go 
voluntarily. With the Chechen fighters were journalists and several deputies 
of the state Duma, with human rights defender Sergei Kovalyov at their head. 
Not one western correspondent agreed to go. The authorities demanded that 
every one of the remaining hostages sign a statement stating that they were 
voluntarily accompanying Basayev's "bandit group" and were fully aware of the 
possible consequences. Thus, the authorities turned the hostages into 
accomplices.
  The buses broke down endlessly. More than once the route had to be changed. 
At the border with North Osetia the column halted after the local athorities 
refused the "terrorists" entry. Later it was revealed that the Osetian leaders 
were not behind this move at all; Moscow was simply unwilling to allow 
Basayev's buses, decked out in the Chechen flag, to make a triumphant progress 
through regions of Chechnya occupied by the Russian army. 
  In Dagestan, crowds came to greet Basayev, bringing water, bread and 
cherries. Basayev spent several houres on the border at Khasavyurt, obtaining 
written guarantees of safe passage from the commanders of the federal forces. 
At the Chechen border, the column encountered the last position held by the 
government forces. Journalist and hostage Valery Yakov wrote: "Passing through 
this checkpoint, the terrorists waved their arms triumphantly, and one of them 
suddenly put his head out the window and shouted, "We're going to live!", 
referring to the end of military actions. The Russian soldiers stood silently 
on the parapets of their trenches, gazing at the column of triumphant fighters."
  The country followed the denouement of the Budyonnovsk drama with a sinking 
heart. Society saw that the hostage volunteers and the terrorists were as one; 
together they were overcoming the difficulties of the long journey. 
Confronting them were pitiless and unscrupulous atate functionaries; corrupt, 
blood-stained military chiefs, and high-ranking police hated by the population 
on both sides of the border.
  Most of the hostages were freed in the Chechen village of Zandak. Basayev 
asked for their forgiveness before the buses turned back. Then the fighters, 
with a few journalists, set off for the mountain village of Dargo. Here 
Basayev's expedition came to an end.
  In the space of a few days, 30-year-old Shamil Basayev had become one of 
the best-known and, in his own fashion, popular people in Russia. Newspapers 
featured his photo on their front pages. Journalists tried to discover 
everything about his years as a student in Moscow. The terrorist became a hero, 
and not only for Chechens. Out of television reports and the accounts of 
eyewitnesses came the image of a "Chechen Robin Hood."
  Basayev, in fact, shook up the imagination of all Russia. Garrulous Russian 
politicians, the greedy, cruel and lying authorities and the small-minded, 
powerless opposition all seemed pale beside the Chechen militant. Basayev 
stood like the hero of medieval epic, at once cruel, brave and noble. He had 
dozens of victims to his name, but he had stopped the war, and in so doing 
saved thousands of people. Basayev's actions had placed him outside the law, 
but the law in Russia means as little as it does in Chechnya. Even Yury 
Feofanov, editor in-chief of the journal Zakov ( "law" ), which is close to 
the government, recognised: "The barbarian action pushed the Russian 
authorities on to the civilised path of negotiations. As it has turned out, a 
crime is giving birth to a kind of law that cannot be accomodated within any 
moral framework, but which has now forced its way into existence de facto."
  Is Basayev a terrorist? Unquestionably, though he prefers to call himself a 
saboteur. As a tactic, seizing hostages is pure terrorism. But everyone who is
 familiar with modern history will recall that on more than one occasion 
desperate terrorists have been transformed into respectable politicians, 
accepted by the world community.
  National flags have once again appeared on the streets of the Chechen 
capital. While negotiations are going on behind closed doors, on the streets 
people are again gathering in spontaneous demonstrations, showing the 
powerlessness of the army and police.
  It is not surprising that, for the Yeltsin regime, Basayev has now become 
enemy number one. He has robbed the authorities of their monopoly on the use 
of violence. In the talks in Grozny, Moscow's representatives demanded that 
Basayev be handed over. General Dudayev and his government condemned Basayev's 
actions and even promised to cooperate in apprehending him. But next day the 
commander of the Chechen forces, General Maskhadov, observed that not a single 
Chechen would allow even a hair to fall from Basayev's head.
  The appearance of a new popular hero in Checnya is not good news for Dudayev. 
So long as the main question remains stopping the war and getting the Russian 
army out of Checnya, Dudayev remains the only recognised leader. But once the 
war ends and the occupying forces depart, the rebellious general, who in the 
past was closely linked with Yeltsin and the corrupt Moscow elites, will have 
to answer for his deeds. If the war drags on, the decisive role in it will not 
be played by people from Dudayev's government, but by Basayev and other field 
commanders. This is why the two sides made haste to reach agreement while the 
situation was still under their control. Dudayev's side even agreed to keep a 
certain number of Russian troops in the republic "to maintain order".
  In Russia, a full-scale political crisis has broken out. Chernomyrdin appears 
to have believed that, once he had made concessions to Basayev, the Duma would 
not attack him. The deputies, however, reasoned differently. Basayev had not 
only humiliated the government, but once again shown the powerlessness of the 
parliament. With the help of a terrorist act, Basayev had achieved what the 
parliament had been unable to manage in six months. Everyone suddenly grew 
bolder, including the deputies. The authorities were paid back for the 
impoverishment of the population, for the genocide in Chechnya, and for the 
breach of earlier promises. The Duma adopted a motion of no confidence.
  But even this lesson was not enough. Yeltsin's constitution does not require 
that the government resign when it has lost the confidence of the parliament. 
After suffering a vote of no confidence, the government can remain serenely in 
office, provided that the vote is not repeated within three months . 
Chernomyrdin and Yeltsin, however, reacted in their customary style. 
Chernomyrdin put the question of confidence back before the Duma, while 
Yeltsin promised that the Duma would be dissolved if it once again voted 
against the government. For added effect, the government, on the pretext of 
combating terrorism, brought 15,000 paratroopers, armoured personnel carriers 
and special forces into the capital.
  The deputies, who had learnt from the bitter experience of the Supreme 
Soviet, were haunted by the fear of dissolution. In a slip of a tongue while 
discussing the political crisis, the deputy Gleb Yakunin even stated that, 
under the constitution, the government had the right to fire on the parliament. 
Yeltsin still had not grasped the key fact that, since Basayev's exploit, 
millions of people had grown ashamed of their fear and powerlessness. The 
authorities had run up a bill; now it was being presented.
  And within little more than a week, Yeltsin and Chernomyrdin had been seen 
forced to pay a high price to settle the crisis with the Duma. Two power 
ministers had to go. Police minister Viktor Yerin and security minister Sergey 
Stepashin were sacked to calm the deputies. That was a real defeat for Yeltsin, 
who depends for his physical survival on keeping loyal cronies in control of 
the repressive apparatus. The government stayed, but it is weakened. And the 
crisis continues.
  The broad mass of Russians have understood Basayev's lesson. If the Yeltsin 
regime fails to recognise the significance of this, an escalation of the 
violence is inevitable. The country is being dragged little by little into 
civil war, or, to be more precise, the authorities have been waging a 
one-sided civil war against society ever since the autumn of 1993. Now, 
however, society is no longer willing to be merely the object of experiments, 
a flock of likely victims.
  "In Chechnya, tens of thousands of innocent women and children have been 
killed... The genocide against the Chechen people is continuing, and the whole 
world is looking on with indifference as we are being annihilated. Perhaps 
Basayev's action will finally awaken the Russian people from their slumber." 
These words of the Chechen captain Shirvani Albakov, who was killed near the 
village of Bamut in the last hours of fighting, may turn to be prophetic.



Friendly society, R. Pahl, 10 March, 

Friendly society
In embracing mythical idea of community, the Labour Party may be making a 
grave error by ignoring the very real support networks that people build among 
themselves.

Tony Blair embraced the idea of community. He believes that between unbridled 
market forces and the dead hand of bureaucratic socialism there is a middle 
way. He has talked about working together, solidarity, partnership and a 
belief in society. These words come together in what Tony Blair calls 
social-ism. But what does all this rhetoric really mean?
  In America, the communitarians seem to have growing influence. The leading 
exponent of the movement, Amitai Etzioni, is now in Britain for the second 
Times/Demos lecture. It is important that, in the search for the Big Idea, 
those developing a new politics do not substitute wishful for critical 
thinking. So what is the real "community" in Britain today? The increasing 
polarisation between rich and poor has recently been well-documented by the 
Rownntree Report and Howard Davies, director-general of the CBI, recognises 
that this growing inequality threatens social cohesion. The communitarians 
believe they have the answer and they focus on the sociologically slippery 
concept of community.
  So what is community? Superficially most people associate it with a place, a 
spirit that has been lost or the camaraderie of common interests -- the 
angling community, the business community or the community of scholars. 
Consider first the idea that community is a place: those with nostalgic and 
romantic notions of a mythical past may imagine some kind of village community. 
Typically such places were characterised by rigid status hierarchies where 
everyone knew his or her place and the power of the squire was potentially 
enormous. In more seemingly egalitarian contexts based on small-holdings or 
peasant-type farming, countless anthropological and sociological studies have 
shown in detail how these social worlds were bound by intense rivalries, feuds 
and conflicts. People are highly sensitive about their social ranking in these 
societies: caution and cunning are needed if families are to maintain their 
position and the older generation are to die secure in the knowledge that they 
have maintained or enhanced their status and respectability -- if not their 
wealth. Even today, the unsolved murders in remote French hamlets and in 
tight-knit urban villages remind us of the intensity of community involvement 
and the impossibility of outsiders penetrating it.
  Few would claim that kind of "anthropological" community is what they had in 
mind. They would prefer a more sociological nostalgia, based on the 
"working-class community" of pub, pigeon fancying and brass bands. These were 
occupational communities based on the work of men as miners, steelworkers or 
shipbuilders. They were rigidly segregated by gender and many of the 
sociological studies of the 1950s and 1960s are now fiercely criticised for 
their almost complete neglect of women and any account of their subordinated 
position in the home, in the labour market and in the largely male-dominated 
world of formal and informal voluntary organisations. These were communities 
of oppression, and the bonds of solidarity were the bonds of resistance. 
Oppression was externally induced by the technical and managerial constraints 
of the industry and internally induced by a conformist male ideology, which 
was hard for most women and for those few men with the aspirations and ability 
to escape.
  These and other types of localised communities of oppression and resistance 
have mostly disappeared, although they are being recreated in declining small 
market towns and remote rural areas.With little local unemployment, declining 
public transport and with those services and facilities that remain struggling 
to survive, common deprivation is returning as a spur to action. In such 
places informal systems of cooperative selfhelp such as LETS ( Local Exchange 
Trading Scheme ) are flourishing. Such initiatives are frequently initiated by 
lone parents who have the time, capacities and need for mutual support. Some 
of these organised communities of resistance may not place-based -- the fight 
against the poll tax being an obvious example.
  There is also, of course, the community of association of confortable 
England. Here the affluent arrange local festivals of the arts, invite 
speakers to political and other groups, tidy up and cultivate their local 
heritage, and keep everything neat and spick and span. Such activities tend 
to exclude the majority of the population on the council estates. This is the 
context for community as a hobby or leisure interest. For most of such people 
in hobby communities, their real involvement is with their families, their 
work and their friends, wherever they are.
  I have mentioned what may be called communities of control, communities of 
common deprivations and hobby communities, but this is not an exclusive list; 
it simply indicates something of the pitfalls associated with the idea of 
place-based community. The communitarians would be unlikely to champion these 
brands of community: they see themselves, in Etzioni's terms, as "an 
environmental movement dedicated to the betterment of our moral, social, and 
political environment" ( A Etzioni, The Spirit of Community, 1993 ). Fine: so 
where is this movement going to operate? The answer seems to be to create new 
communities, that is non-geographic, communities that Etzioni claims "fulfill 
many of the social and moral functions of traditional communities". Notice 
that Etzioni takes as given that traditional communities did serve those 
functions -- a highly contentious notion -- but he also asserts as given that 
work-based and professional communities inevitably produce positive and 
socially benefical results.
  The sosiological case has to be made, not simply asserted. For many people, 
the work place has become intensely more competitive in recent years. 
Employers' stategies to control and discipline workers have been very 
effective: downsizing, delayering, contracting out, hiring on short-term 
contracts and similar measures create an uneasy and anxious workforce. The 
feel-good factor remains elusive. Those who become self-employed find little 
opportunity to have matey solidarity with collagues. Mostly, as they are well 
aware, they are on their own. It is tough; it is sometimes lonely, but they 
have little choice: They long for some respite and envy those with secure 
pensions to look forward to.
  In a less stressed age it is, indeed, possible that social workers, teachers, 
advertising executives and publishers met each other at conferences and 
developed a degree of camaraderie that extended over some years. Now they 
either cannot afford to go to conferences or they spend all their time angrily 
attacking each other, or the less-than-humane society that is cutting their 
jobs and reducing their security. Etzioni would doubtless prefer the situation 
to be othewise, so that these professional groupings could "have the moral 
infrastructure we consider essential for a civil and humane society." However, 
it is surely naive to imagine that work-place stress and anxiety results 
simply from turning our backs on "community".
  Curiously, the well-established sociological knowledge about what clearly 
does create a sense of community solidarity seems to be overlooked by Etzioni, 
although the ideas have been around for a long time. The neatest account was 
provided by James Coleman, the distinguished American sociologist, as long ago 
as 1957 in his brilliant pamphlet on Community Conflict. Less than 30 pages 
long, it says far more about place-based community than most of what has come 
after it. The trouble is that, like many of the best sociological insights, 
once stated, people will say they have known it all along. Community life in 
the past was imposed on people, being largely based on involuntary 
relationships. Now people choose their associates -- and perhaps more 
importantly, choose with whom not associate.
  Coleman rightly predicted, nearly 40 years ago, that "the prospect for the 
future is toward an increase in the proportion of externally caused community 
controversies". This is because the local community is very rarely the locus 
of important social decisions. So community spirit based on conflict is more 
and more national affairs and national decisions. If anyone wants to create 
community spirit in comfortable, Tory-voting England, then put a high-speed 
rail link close by. But whether that helps in the creation of a more humane 
society is highly debatable.
  The problem with conflict-induced community spirit is that if the community 
loses -- as is increasingly likely -- the consequent apathy is that much 
deeper. If the action leads to success, then the immediate reason for the 
solidarity colapses and the tendency is for local factions to make local 
social and political capital out of the part they played in the struggle. This 
can produce new divisions and much bitterness.
  It is a rare locale that does not reflect the increasing polarisation and 
inequality of the past 15 years. Sometimes the emergence of local-issue 
politics highlights divisions and tensions that were concealed or dormant. 
Not only does the activation of "community spirit" frequently reveal a can of 
worms, it can also set community against community. In the messy world of 
issue-politics those who shout loudest, who are the most cunning or the 
luckiest, win. The rest are disgruntled if resources go elsewhere. There is 
little discussion of this divisive nature of community life among the 
communitarians, despite the wealth of anthropological and sociological studies 
undertaken in the past 50 years that have described it.
  So, if sociologists have exposed the myths and fallacies of the idea of 
community, why does a dead idea refuse to lie down? Why are political leaders 
in America and Europe so keen to adopt communitarian rhetoric without thinking 
through more carefully the implications of what they assert? Part of the answer 
is that people are desperately keen to find a middle way between the public 
and the private. Some of the practical suggestions of the movement, divorced 
from the rhetoric, sound like good sense to specific groups of the population. 
Since the movement spatters out various suggestions, it is easy for people to 
take on board those they would like to believe in and overlook those aspects 
that might make them feel uneasy if they subjected them to closer scrutiny. 
The idea that people with rights also have responsibilities carries resonance 
on left and right; if communitarians are the people who take this notion most 
seriously, then many are perhaps more prepared to give them the benefit of the 
doubt rather than appear to be supporting unhealthy dependencies.
  Sociologists are also to blame: unlike Etzioni, many have been discouraged 
or unwilling to engage in public debate. Having retreated in the face of the 
overwhelming self-assurance of the economists, sociologists in Britain have 
failed to play the vigorous part in public debate that their collagues in 
America, Italy or France have continued to do. 
  If the communitarian philosophy fails to be sociologically convincing, what 
is to be put in its place? My own inclination is to start with what is, rather 
than what ought to be. If the desire is to celebrate and support those who 
engage in communal and collective endeavours then we should see who actually 
does this, rather than those who ought to do it. In practice those who are 
most in need of supportive and communal activity are lone parents. Those in 
the very category pilloried by communitarians are setting up support groups 
for themselves, and various other coperative activities for other in the same 
place. As Carol Stack has shown in All our Kin, lone, black mothers in 
deprived urban environment in the US can create valuable and essential 
solidarities. The same can be shown for many deprived and oppressed minorities.
  Such minorities may express their solidarity in opposition to the majority 
and, as long as they remain conscious of their excluded status, the sense of 
identity will remain. New forms of exclusion create new potentials for 
identity. These new solidarities of oppressed are likely to be in inner-city 
areas -- often the very areas that cause communitarians the most dismay.
  Those who are most praised by communitarians --the doting, caring parents 
taking their childcare very responsibly, and devoting considerable time to 
their children, have little time for much else. Both partners are likely to be 
in employment -- even if only on a part-time basis for the woman. They 
probably have to spend much of their week in tiring travel, commuting to work, 
collecting children, visiting relatives, driving to out of town shopping 
centres or taking their children swimming or ice-skating. At evenings or 
weekends they are more likely to be more exhausted than active citizens. The 
harassed, time-challenged middle mass are obliged to be highly selective in 
how they choose their friends and associates.
  Those who have the time, energy and inclination to engage in public-spirited 
work are more likely to be single. The divorcees and lone parents are more 
likely to be younger; the widows, widowers and pensioners will be older. Some 
vigorous widows in their 60s and 70s can make fine community activists if not 
over-committed as grandmothers.
  One of the massive fallancies of the current debate is that because of the 
decline of "the traditional community" social and geographical mobility has 
made social atoms of us all. Politicians and pundits frequently fall into the 
trap of seeing the family as the only putative source of social cohesion and 
control, so higher rates of divorce and the rapid growth of lone-parent 
families are seen as a dangerous dilution of social glue. The answer by the 
communitarians and the father-fans, like Norman Dennis and A.H. Halsey, 
writing for the Institute of Economic Affairs, is to seek to strengthen the 
male breadwinner role for the sake of his dependents. Yet surely there can be 
no return to the patriarchal working-class family: all the survey material on 
the younger generation supports this, as Helen Wilkinson's Demos pamphlet No 
Turning Back recently showed.
  This obsession with the ideological connotations surrounding community and 
family has led to one of the most powerful supportive bonds of modern society 
being neglected. The reason most of us are not mere social atoms fighting our 
corner in competitive market is that we have our friends, our mates, our 
support group. Girlfriends and boyfriends come and go; children grow up and 
find their own identities elsewhere, partners come and partners go, but 
friends provide the social group. Sometimes, it is true, siblings or cousins 
can be friends. But, more often, best friends comfort in time of bereavement 
or the break-up of a relationship. We need our friends and our friends need us.
The idea that fraternity is limited to male bonded trade unionists is a sexist 
anachronism.
  The point about friendship is that it is what Anthony Giddens has called a 
"pure relationship" in his book The Transformation of Intimacy. With true 
friends we do not immediately calculate what we have to pay back for being 
comforted or for acting as an unpaid therapist during one of life's crises. A 
man may have a former spouse or lover as his best friend: she knows him in 
some ways better than he does himself and can be relied on to give advice that 
is honest and designed defensively to put herself in a good light. Women 
perhaps make better friends and maintain contact with their old work 
colleagues, with other mothers who were contemporaries or with near neighbours 
when they were tied at home with young children. Sociologists have shown how 
the idea of friendship have suffused kinship, so that we are now more likely 
to choose those relatives to whom we wish to remain close.
  Friends help each other, support each other and maintain links over long 
periods of a person's life. They help people to find jobs, they provide 
somewhere to stay in an emergency and, above all, they provide social support 
and affirmation for identity in a world that can leave people bereft of 
partner, job and family in a short space of time.
  Politicians have probably neglected to consider friendship seriously 
because they cannot see clearly how to fit friends into an institutional 
context. Family-supportive policies are one thing...but friend-friendly 
policies -- what on earth would that entail? Sociologists have tended to 
abstract from relationships and have studied types of social networks and the 
forms and norms of reciprocity. Such language does not resonate with 
politicians who may be prepared to learn economists' jargon, but see no point 
in doing the same of anthropology or sociology. However, if politicians 
recognised the social importance of friendship it would do much to avoid the 
alienating consequences of much political rhetoric.
  Fearful of overtones of Thatcherite individualism based on competition and 
greed, Tony Blair wants to substitute social-ism for a selfish and uncaring 
individualism. In doing this he is likely to make a very dangerous blunder by 
misunderstanding how people are actually behaving in their everyday lives. 
They are being social as individuals. Their individuality is vitally important 
to them. This distinction between individualism and individuality is crucial, 
and is the vital missing element of discussion in contemporary politics. 
Misguidedly, Tony Blair has latched on to community and communitarianism as 
alternatives to the discarded individualist ideology of the 1980s. The danger 
is that he will miss the significance of individuality and so people may fear 
that their own individuality may not be easily nourished and developed under 
his social-ism.
  Individuality implies diversity, tolerance and creativity. We flourish with 
our friends. I suggest that Tony Blair should articulate how he would develop 
a more user-friendly style of government that would encourage more diversity 
and spontaneity. Instead of a rather pious couple-ism and familism, the Labour 
Party should lead and reflect the mood of the 1990s -- people are not social 
atoms anonymously colliding in a harsh and competitive environment. They are 
generally warm and friendly people trying to come to terms with themselves and 
their relationships. They need support, not censure. There is no need to fall 
for a phoney communitarianism: what is needed is an acceptance that most 
people are struggling on as best they can with the help of their friends.
  There was something in Thatcherism that did appeal to the individualism of 
the British but it became badly infected with manic ideological overtones. It 
would be a pity if, in reaction to this, there were a similar uncritical 
commitment to an alternative unproductive ideology. Don't see divorce 
statistics and the growth of lone-parent households as indicators of the lack 
of social cohesion. Rather see how friendships and the solidarity of single 
mothers are creating new, better and more equal bonds in society.
  The mood of the late 1990s is surely that people are engaged in a struggle 
to find an identity for themselves which is sensitive to them as they really 
are. They reject the notion that they should subordinate themselves to 
collective categories. As we know from our own personal experience, our 
attachment to collectives is mediated by our personal experience. It is an 
obvious, yet crucial, insight that people read collectivities through their 
experience as individuals. Simply because people may use collective forms to 
assert their identities, it would be a great mistake to assume that this 
implies uniformities of identity. By being wary of the selfish self of the 
1980s, we are in danger of ignoring the self-conscious self of the 1990s.
  Despite many in the Labour Party seeing the dangers of this ideological 
individualism, it is tragic that so few have recognised the deep-seated urge 
for individuality among people in our society. There is still time to make 
amends. Let us move beyond communitarianism to a diverse, tolerant and 
friendly society based on creative individuality.



New-age nazism, M. Kalman, 23 June

New-age nazism
Extreme right and neo-Nazi groups are attempting to hitch a ride on the green 
and "New Age" bandwagon.

On 19 April, if the allegations are correct, Timothy McVeigh planted a bomb in 
Oklahoma City that killed 167 people, including children at a nursery. McVeigh 
had been influenced by a video called Waco -- the Big Lie. As we all know now, 
he is associated with the right-wing militias in the US. He hates the federal 
government and is fearful that an oppressive "New World Order" is being 
secretly planned. He calls himself a prisoner of war and won't answer police 
questions.
  What has this got to do with the "New Age" and green movement? More than 
one might expect. Visit many New Age and other shops in Britain and you can 
pick up the magazine Nexus. A hit on the New Age circuit, it is a new arrival 
here but already has a claimed worldwide readership of 130,000 plus. It offers 
a beguiling, and often interesting, mixture of "prophecies, UFOs, Big Brother, 
health, the unexplained suppressed technology, hidden history and more".
  But Nexus is also a propaganda journal for the ideas and conspiracy theories 
of the US militias. Recent issues have included the call by Linda Thompson, 
self-proclaimed "adjuntant-general" of the US militias, for a march on 
Washington to arrest and try Congressmen for treason. Her declaration includes 
the advice that "militia members must wear identifying insignia and be armed", 
in order to be treated like "a prisoner of war, not as a criminal arrestee". 
It was Linda Thompson who produced the Waco video that influenced McVeigh; 
Nexus distributes it, and even produced a special version for Europe. The June 
1994 issue reviewed the video thus: "There is a very big underground building 
up in the US at the moment. Every night, someone is showing this video to a 
bunch of friends. Nobody remains unaffected after seeing it. It is contagious, 
so beware!"
  Another notorious militia member, Mark Koernke, of the infamous Michigan 
Militia, also appears in Nexus attacking the "New World Order of America's 
secret police forces". He warns that the government plans to take away the 
people's guns, build detention camps and suchlike. The " processing centre for 
detainees in the western half of the US is Oklahoma City," he claims. He was 
shown on Panorama holding up some nylon rope: "You can get about four 
politicians for about 120 feet of rope. Remember, when using it, always try 
and find a willow tree. The entertainment will last longer." Koernke was 
sought by the FBI after the Oklahoma bombing and went on the run; he was later 
questioned and freed. The Guardian's Washington correspondent calls him "a 
guru to the shadowy legions of conspiracy theorists, anti-government activists 
and gun freaks whose ranks appear to have produced the Oklahoma bombers".
  A few pages on from the Koernke article is part four of a "history" of banks. 
It is clear who the author feels might save the world from "usurious bankers": 
"Hitler knew he was earmarked for a 'showcase trial' and so he killed himself...
There is no one left alive who can rock the usury boat. Mussolini was executed 
for the same reason." In case the reader still hasn't got the message, notes 
refer to racist, far-right books like Teutonic Unity ( "a clarion call by one 
of America's great racial historians," says the Ku Klux Klan ) and White 
America. An address is given for the far-right Liberty Lobby's 
holocaust-denying book distributor, Noontide Press, which sells the whole 
disgusting panoply of Nazi favourites, from The hoax of the 20th century to 
Mein Kampf.
  In Britain, the former TV sports presenter and ex-Green Party speaker, David 
Icke, is the best-known promoter of Nexus. He calls it "incomparable" and 
"excellent" in his book The robots' rebellion: the story of the spiritual 
renaissance. The book is both an anti-elite, green and new-age rallying cry 
and a fantastical tapestry of far-right conspiracy theories and militia 
concerns: banking, the "New World Order", Freemasons, microchip mind control, 
extra-terrestrials, gun control; and it resurrects one of the oldest 
conspiracy theories of them all -- The protocols of the elders of Zion, a 
fraudulent "proof" that the Jews are out to control the world that forms the 
basis for his historical analysis. Icke seems oblivious to the fact that it 
helped the Nazis justify the Holocaust: "Just because Hitler used knowledge 
for negative reasons doesn't reflect on the knowledge," he says. He dismisses 
the overwhelming evidence that the Protocols are a concocted forgery.
  Elsewhere, he repeats the outpourings of leading militia spokespeople, 
sometimes almost word for word. The American Federal Reserve is controlled by 
freemasons. Lincoln was assassinated because he wanted to introduce 
interest-free money "independent of the baking elite". JFK was eliminated for 
the same reason. "The American Gun Control Act of 1968 is word for word the 
Nazi gun control law of 1938... If you are planning a takeover... then it is 
much easier if the population is unarmed."
  Also thrown in is another favourite of the far right: the alleged 
Bolshevik-Jewish conspiracy. Of the 388 members of the Russian revolutionary 
government of 1917, only 16 were Russians by birth and "95 per cent of the 
rest were Jews from elsewhere, mostly from New York". Bankers, usually Jewish, 
funded the Bolsheviks and the Nazis, Icke tells us.
  Icke is over-hastily dismissed by people who find what he says laughable. 
This does nothing to dim his burgeoning popularity. His seemingly endless 
speaking tour takes him from London, to Bristol, to Cardiff, Glasgow, Dublin 
and so on. The robots' rebellion, not even a year old, is already entering its 
third reprint, and he recently launched a newsletter to network his disparate 
supporters: "Once we get this off the ground, no one need feel alone... A 
fusion of the spiritual and, through knowledge, the streetwise is both 
necessary and, ultimately, unstoppable". Nexus, too, has reader groups setting 
across the UK. Icke appeared at an October 1994 Nexus conference in Amsterdam 
at which militia leader Linda Thompson was billed as a star speaker.
  Icke's audience has included members of the violent neo-Nazi group, Combat 
18, which wrote up his views in its newsletter Putsch, praising Icke for 
"always being clever enough not to mention what all these people had in common 
( salt-beef sandwich anyone? ). ( Icke ( spoke of 'the sheep' and how the ZOG 
( Zionist Occupation Government (, sorry 'Illuminati', uses them for its own 
ends."
  Other takers of Nexus' pro-militia line were the leaders of the Solar Temple 
cult, who incinerated themselves in a mass suicide of 53 adults and children in 
Switzerland on 5 October 1994. In statements sent to the press just prior to 
their deaths, but unpublished except for brief quotations, the Solar Temple 
referred to articles in Nexus. One of them, included in photocopy form, was 
about the August 1992 FBI/Bureau of Alcohol, Tobacco and Firearms siege of 
white supremacist Randy Weavwer's home, in which his wife, son and dog were 
killed. The Solar Temple drew parallels between their treatment at the hands 
of the authorities in Quebec ( it originated in Canada ) and that of Weaver 
and far-right groups in the US. 
  Both Icke and Nexus have influence over an alarmingly large swathe of the 
growing New Age and green movement -- and this is not just for Icke's 
"spiritual" message; hatred of banks and Jews has many takers too. It matters 
that many of those taken in by such ideas have little notion of their 
antecedents. ( The organiser of one upcoming Icke workshop, from a New Age 
"Positive Living" group, for example, had not even heard of the Oklahoma 
bombing and was very hesitant when asked about the Holocaust. "That's neither 
here nor there... I can't be bothered to think about it at this moment," she 
told us, explaining that she didn't read newspapers. "You are aware who owns 
them, aren't you?" )
  Another major influence on Icke is William Cooper, author of Behold a Pale 
Horse. Whole chunks of The Robots' Rebellion are taken from Cooper, who saves 
his readers the trouble of getting in touch with outright Nazis by reprinting 
the full, unexpurgated text of the Protocols in his book. An ex-naval 
intelligence officer, Cooper is a leading figure in the militia movement; his 
local militia believe he will be the next US president. In February, he 
predicted in the Arizona Republic that: "Blood will be spilled in the streets 
of America. It's inevitable."
  "I have spoken to many people involved in the New World Order 
investigations, some clearly anti-Jewish, and most very obviously not," Icke 
says. Again following Cooper, he mentions a 1979 "illuminati" document, Silent 
weapons for a quiet war, allegedly a "technical manual" adopted in 1954 by the 
Bilderberg Group, and supposedly found in an IBM copier at a second-hand sale 
in 1986. ( "Illuminati" is Icke's preferred term for the supposed New World 
Order conspiracists. ) The document purportedly describes how the minds of the 
people are to be controlled, and is, Cooper tells us, "the Illuminati's 
declaration of war upon the people of America". It seems something of an 
oversight for the all-powerful Illuminati to leave the plans for the third 
world war lying around in a copier, but no matter.
  Icke's book makes clear the extent to which he is a recipient of the 
information he pushes, rather than an originator: "I am being guided to an 
area of knowledge that needs to be made public," he explains. In the chapter, 
"The New World Order", he admits, "I had only vaguely heard of the term until, 
over a period of three weeks, all the information about it in this book was 
put into my hands by various different people."
  Some of these "various people" are to be found around the London-based New 
Age magazine Rainbow Ark. Recent issues have targeted the same Jewish bankers, 
"Illuminati" and suchlike, and it has printed a fair amount of Icke material. 
It helped organise a lecture he gave at the Glastonbury festival and other 
meetings.
  Rainbow Ark, despite operating in the New Age milieu, was launched from the 
flat of a well known right-wing racist propagandist, Mary Stanton. It was used 
by the National Front during one election as its press office. Stanton was 
also questioned by MI5, who were investigating the appearance of her address 
on a list of international contacts for the anti-Semitic far right. In the 
launch issue of Rainbow Ark, the three largest donors are gratefully thanked 
for gifts of �250 and over: Mary Stanton, Anthony Chevasse and a Mr Bloom. 
Above is an advertisement seeking volunteers for Stanton's "Free Society to 
Save the Planet".
  Chevasse is intimately involved with Rainbow Ark. Little is clear about him 
except his interest in the right-wing economic theories of C.H. Douglas, who 
was attracted to the Nazis. The key organisation publishing Douglas' ideas is 
the British League of Rights, headed by Don Martin. Martin is an important 
figure in the far-right. His writing has appeared in the British National 
Party's paper, Spearhead. One lecture he gave, organised by Rainbow Ark, was 
picketed by antifascists. The magazine's editors clearly have close ties to 
Martin -- one says they meet him periodically for briefings.
  An idea of Martin's beliefs can be derived from his mail-order firm, 
Bloomfield Books. This services the far right with more than 700 of their 
favourite books and magazines -- including the 100,000 plus circulation 
anti-Semitic Spotlight, another source used continuously by Nexus, as well as 
Icke. The books include the usual Nazi favourites: Did Six Million Really Die? 
and so on.
  BNP leader John Tyndall calls Martin and the organisations he runs "allies". 
The British League of Rights, for example, seeks among its aims: "To oppose 
large-scale immigration of alien peoples, and to work for the maintenance of a 
homogeneous community." Martin is also a friend of Lady Jane Birdwood, who 
has a criminal record for promoting racially inflammatory material.
  His organisations are known for their avoidance of publicity in their own 
right and an often high-level involvement in campaigns by other organisations. 
Martin is known also for taking over groups. One, the British Federation for 
European Freedom, was turned into the British arm of the ultra-rightwing World 
Anti-Communist League, a franchise later taken over by the pro-apartheid 
Western Goals.
  Another organisation he is involved with is the British Israelites, part of 
an international right-wing church. According to Klan Watch, its US outgrowth, 
Christian Identity, of which Randy Weaver is a member, is powerful and 
well-financed, with 30,000 members, though British Israelism's sympathisers 
run into millions. Members in Britain are said to include such establishment 
and ex-military types as the Duke of Montrose and Sir Walter Walker, the 
mid-1970s anti-Labour Party plotter. His book, The Next Domino, was published 
by the British Israelites. Their creed, that Anglo-Saxon Aryans are the 
"chosen people" -- the ten lost tribes of Israel -- is expressed by the 
Christian Identity movement in the US, which provides a focus for both the old 
Nazi right newer, semi-anarchistic right-wing militia groups.
  Manipulation of the information flowing to David Icke is clear from the 
actions of Rainbow Ark's editor. In one instance, the authors of this article 
were given a comical document of Further Protocols, which included laughable 
details of the plans "secret Zionism" has for "the Goyim". "Icke's not ready 
for this yet," Rainbow Ark's editor said. This same editor took a right-wing 
US "patriot", who claimed to represent armed networks of "patriots", to meet 
Icke at a packed London lecture in January. They spoke for 20 minutes, on the 
usual "patriot" themes. "We're ready for war," he told Icke, three months 
before Oklahoma. 
  Public meetings organised by Rainbow Ark in May gave an Australian 
ex-stockbroker, called ( pseudonymously ) Peter Celine, an opportunity to 
expound a typical Icke/Nexus narrative of bankers, freemasons, Illuminati et 
al, along with his call for rebellion and the formation of five-person cells. 
When asked where to find such "suppressed information", he explained that he 
had been pointed to Donald Martin's Bloomfield Books by Rainbow Ark's editor.
  Anna Hall, editor of Rainbow Ark for its first II issues, insists: "During 
the time I was editor, nothing which could be labelled fascist, racist, 
anti-Semitic or Nazi in content was published." In common with many committed 
green activists, she seems to have been unaware of the wider beliefs of some 
of the people she was dealing with -- such her British Israelite landlady, 
Mary Stanton.
  In Australia, its country of origin, the far-right links of Nexus are quite 
well-known following media exposure. Editor Duncan Roads has been forced onto 
the defensive over his printing of militia articles. Roads is himself very 
interesting. He stepped into the advertising department of Nexus from the 
Rolling Stone-funded alternative magazine Simply Living. On Australian radio 
recently, he was asked if the holocaust took place: he said he remained 
"open-minded".
  In 1989, Roads visited Gadaffi in Libya. According to Australian journalist 
David Greason, he is a close friend of the right-wing Libyaphile, Robert Pash. 
Pash was the Australian contact for the "Aryan Nations" network in the late 
1970s. In the late 1980s, he was the Australian distributor for Gadaffi's 
Green Book, and was in contact with the National Front, acting as a conduit 
for their attempts to make political common cause with the Libyan regime. Alan 
Myers, of Australia's Green Left Weekly, says that Pash is also part of the 
Australian League of Rights, the ultra-right anti-Semitic organisation, of 
which Donald Martin is the British arm.
  Initially, Nexus was a green alternative magazine with a multicultural and 
liberal orientation, containing a smaller amount of New Age and health 
material along with third-world issues. After hitting the financial rocks, 
Roads took over the editorship and it metamorphosed into the far-right 
magazine it is today.
  Nexus's British agent, who, like the Rainbow Ark group, was at the launch of 
Icke's book, unselfconsciously provided further shocking details of this 
growing anti-Semitic propaganda network. Sitting in Nexus's UK office, he 
eagerly displayed his copy of the Protocols, and spoke admiringly of the 
revisionist historian David Irving. There were no gas chambers at Auschwitz, 
he said.
  As he began to describe the "global conspiracy", he said he'd helped David 
Icke with a chapter in his forthcoming book, calling into question the facts 
of the holocaust. The Jewish Chronicle got confirmation that the chapter 
existed from Icke's Bath-based publisher Gateway Books. Gateway has now 
dropped the book --its money-spinning star author apparently unwilling to make 
significant excisions. The book is now to be published in Cambridge by Bridge 
of Love, with the title changed from The Robot's Guide To Freedom to... And 
The Truth Shall Set You Free. "i am struggling to find a publisher for my 
next, now completed book, because they are all in the toilet after reading the 
contents without the need of a laxative," veteran Icke-watcher David Black was 
told in May. On an advertising flyer for the book, the same Sam Masters is 
given and a Cambridge address. "Bridge of Love" is, of course, simply David 
Icke and Masters, his assistant. The 500-page book is due out in mid-August.
  The response to Icke by anti-fascists has been slow, with initially only a 
few anti-fascists in the Green Party and from Green Anarchist magazine 
appearing to take seriously his success in promoting a potentially racist and 
nazi doctrine. He was banned from speaking at the Green Party conference in 
1994.
  However, as far as 1991, Icke-watcher David Black had warned where Icke's 
logic could lead. Noting the roots of his New Age thinking in the mystical 
teachings of Theosophy developed by Madame Helena Blavatsky, he warned: 
"Blavatsky's fantasies were readily taken on board by the disillusioned 
nationalist romantics who pioneered Nazism in Germany." Indeed, the Nazis 
took the group's symbol, the swastika, and made it the emblem of the Nazi 
party. Later Theosophists like Alice Bailey promoted an intense anti-Semitism. 
In 1947, she called the Jews "a very cruel and aggressive people". She even 
identified the Jews as the world's worst problem, stating -- in the immediate 
aftermath of the holocaust -- that "there is no other problem like it in the 
world today". The holocaust was simply Jewish karma for their "depths of human 
evil".
  Rainbow Ark has a similarly frightening theory: "When a person has strong 
hatred of another race, their higher self often (karmically) makes sure they 
incarnate in that race to balance them out, thus many of the worst kind of 
Nazis have already incarnated in Jewish bodies,explaining therefore some of 
the fireworks which are going on and will go on in Israel." In other words, 
many Jews are Nazis reincarnated.
  There are important lessons to be learnt from all of this: most importantly 
the continuing attraction of the sort of millenarian, apocalyptic and mystical 
thinking that helped spawn the Nazis. A possible lesson for the left is the 
extent to which it has dropped populist anti-elitism, leaving the right, with 
its nasty hidden agendas, as the only place to look for information. Books 
like Trilaterelism, about the international foreign policy elite, written by 
left-winger Holly Sklar, are largely forgotten by the left, but eagerly taken 
up by the right. The deaths of the multi-racial occupants of Waco, ignored on 
the left as a political issue, became a burning issue of civil liberty on the 
right. 
  As far-right watcher, Larry O'Hara, says: "This is a worrying phenomenon. 
A demoralised left seems to have forgotten many of the lessons it should have 
learnt from history. The fact is, many of the ideas articulated in magazines 
like Nexus are hostile to the US-dominated New World Order, and murderous 
political police, the FBI and others, for good reason. We need to 
differentiate very carefully between what is valid and what isn't." But above 
all, we must make the green and New Age movement aware of the racist and 
neo-Nazi thinking that is trying to masquerade under its banner.



End, J. Rifkin, 9 June,

The end of work?
A technological revolution more far reaching than industrialisation is sweeping
the world. Workerless factories and virtual companies loom on the horizon. We
are witnessing the end of work as we know it.

From the outset, civilisation -- as well as people's daily lives -- has been 
structured in large part around the concept of work. But now, for the first 
time in history, human labour is being systematically eliminated from the 
economic process. In the coming century, employment as we have come to know it 
is likely to be phased out in most of the industrialised nations of the world. 
A new generation of sophisticated information and communication technologies 
is being introduced. Together with new forms of business organisation and 
management, this is forcing millions of blue and white-collar workers into 
temporary jobs and unemployment lines -- or, worse, bread lines. 
  Our corporate leaders, economists and politicians tell us that rising 
unemployment represents only a short-term adjustment that will be taken care 
of as the global economy advances into the Information Age. But millions of 
working people remain sceptical. In the US alone, corporations are eliminating 
more than two million jobs annually. Although some new jobs are being created, 
they are for the most part low-paid, temporary or part-time positions. 
  People everywhere are worried about their future. The young are venting 
their frustration and rage in anti-social behaviour, ranging from drug abuse 
and gang violence in America to attacks on foreigners in Europe. Older 
workers, feeling increasingly trapped by social forces over which they have 
little control, seem resigned. Some observers have attributed the victory of 
the Republicans' "traditional values" crusade in last year's US elections to 
people's growing anxiety about the rapid pace of economic change all around 
them.
  The hard reality that economists and politicians are reluctant to 
acknowledge is that manufacturing and much of the service sector are 
undergoing a transformation as profound as the one experienced by the 
agricultural sector at the start of the century, when machines displaced 
millions of farmers. We are in the early stages of a long-term shift from mass 
labour to highly skilled "elite labour", accompanied by increasing automation 
in the production of goods and the delivery of services. Workless factories 
and virtual companies loom on the horizon. While unemployment is still 
relatively low, it can be expected to climb inexorably over the coming decades 
as the global economy makes the transition to the Information Age.
  Reflecting on the significance of this transition, Nobel-winning economist 
Wassily Leontief has warned that with the introduction of increasingly 
sophisticated computers, "the role of humans as the most important factor of 
production is bound to diminish in the same way that the role of horses in 
agricultural production was first diminished and then eliminated by the 
introduction of tractors."
  These developments do not have to mean a grim future. The gains from this 
new technology revolution could be shared broadly among all the people with a 
greatly reduced working week and new opportunities to work on socially useful 
projects outside the realm of the market economy. But before any such sweeping 
reforms in the way we work can take place, it must be acknowledged that we 
face a future where the traditional role of private-sector jobs as the 
centrepiece of our economical and social life will be gone. 
  Nowhere is the effect of the computer revolution and re-engineering of the 
workplace more pronounced than in manufacturing. In the 1950s, 33 per cent of 
all US workers were employed in manufacturing; today it is less than 17 per 
cent. Management consultant Peter Drucker estimates that the figure will fall 
to less than 12 per cent in the next decade. 
  Although the number of blue-collar workars continues to decline, 
manufacturing productivity is soaring. From 1979 to 1992, productivity 
increased by 35 per cent while the workforce shrank by 15 per cent. For most 
of the 1980s it was fashionable to blame the loss of manufacturing jobs in the 
US on foreign competition and cheap labour markets abroad. However, economists 
Paul Krugman of MIT and Robert Lawrence of Harvard suggest, on the basis of 
extensive data, that "the concern, widely voiced throughout the 1950s and 
1960s, that industrial workers would lose their jobs because of automation is 
closer to the truth than the current preoccupation with a presumed loss of 
manufacturing jobs because of foreign competition."
  William Winpisinger, past president of the International Association of 
Machinists, a union whose membership has almost halved as a result of 
automation, cites a study by the International Metalworkers Federation in 
Geneva forecasting that, within 30 years, as little as 2 per cent of the 
world's current labour force "will be needed to produce all the goods 
necessary for total demand".
  Many economists and elected officials continue to hold out hope that the 
service sector and white-collar work will be able to provide jobs for the 
millions of unemployed blue-collar labourers. Their hopes are likely to be 
dashed.
  Andersen Consulting, one of the world's largest corporate restructuring 
firms, estimates that in just one service industry, commercial banking and 
savings institutions, technological and management changes will eliminate 30 
to 40 per cent of jobs over the next seven years. That translates into nearly 
700,000 jobs in the US alone. The number of banks in the US is likely to 
decline by 25 per cent by the year 2000, and this along with automatic teller 
machines and financial transactions over the information superhighway will 
significantly reduce the number of human employees.
  The technological innovations taking place in banking are indicative of the 
kinds of sweeping changes that are redefining every aspect of white-collar 
and service work. The world's secretaries are among the first casualties of 
the electronic office revolution. The number of secretaries has been declining 
steadily as personal computers, electronic mail, and fax machines replace 
typewriters, paper files and routine correspondence. There are fewer 
receptionists, too. Bellcore is currently developing an "electronic 
receptionist" that can answer calls, record messages and even hunt down the 
party being phoned.
  Changes have also been dramatic in the wholesale and retail sectors. Typical 
of the trend is retail giant Sears, Roebuck. Sears eliminated a staggering 
50,000 jobs from its merchandising division in 1993, reducing employment by 
14 per cent -- this in a year when Sears' sales revenues rose by more than 10 
per cent.
  In most retail outlets, the use of electronic bar codes and scanners has 
greatly increased the efficiency of cashiers and thereby significantly reduced 
the number of jobs available. Some fast-food drive-through restaurants are 
beginning to replace human order takers with touch-sensitive menu screens. And 
many industry analysts are predicting that electronic home shopping will take 
over more and more of the nation's trillion-dollar-a-year retail market.
  It's not just low-level jobs that are disappearing. A growing number of 
companies are deconstructing their organisational hierarchies and eliminating 
more and more middle management. They use computers to do the coordinating 
that people -- often working in separate departments and locations -- used to 
do. Harvard business professor Gary Loveman points out that while better jobs 
are being created for a fortunate few at the top levels of management, the 
people in "garden-variety middle management jobs" are "getting crucified" by 
corporate re-engineering and the introduction of sophisticated new 
technologies. Eastman Kodak, for example, has reduced its management levels 
from 13 to four.
  Intelligent machines are invading the professional disciplines and 
encroaching on education and the arts, long considered immune to the pressures 
of mechanisation. Synthesisers -- silicon musicians -- are fast replacing 
human musicians in theatres, clubs, and even opera houses. The Washington 
Opera Company's recent production of Don Carlos had only the conductor, two 
pianists, and a synthesiser player in the pit. Meanwhile, a robot that will 
perform hip-replacement surgery is being developed in California. And some 
firms now use a computerised hiring system to screen job applicants.
  Optimists counter with the argument that the new products and services of 
the high-technology revolution will generate additional employment, and they 
point to the fact that earlier in the century the automobile made the horse 
buggy and obsolete but created millions of new jobs. Although it is true that 
the Information Age is spawning a dizzying array of new products and services, 
they require far fewer workers to produce and operate them than those they 
replace.
  Many observers also question how an increasingly underemployed and 
unemployed global workforce, displaced by new technologies, is going to be 
able to afford the products and services being turned out. While the optimists 
contend that the loosening up of new global markets will stimulate pent-up 
consumer demand, others believe that soaring productivity will come up against 
weak consumer demand as growing numbers of workers are displaced by technology 
and lose their purchasing power.
  It is also naive to believe that large numbers of unskilled and skilled 
blue-collar workers who lose their livelihoods will be retrained to assume the 
new jobs that are being created. The new professionals -- the so-called 
symbolic analysts or knowledge workers -- come from the fields of science, 
engineering, management, consulting, teaching, marketing, media, and 
entertainment. While their number will continue to grow, it will remain small 
compared to the number of workers displaced by the new generation of "thinking 
machines". Drucker says quite bluntly that the disappearance of labour as a 
key factor of production is going to emerge as the critical unfinished 
business of capital society.
  If no measures are taken to provide financial opportunities for millions of 
displaced workers in an era of diminishing jobs, then as the new industrial 
revolution spreads through the economy, crime, and especially violent crime, 
is going to increase. Trapped in a downward spiral, and with ever fewer safety 
nets to break their fall, a growing number of unemployed and unemployable will 
find ways to take by force what is denied them by the forces of the 
marketplace.
  The violence taking place on the streets of America is already spreading to 
other industrialised nations. In Vauxen-Velin, a depressed working-class town 
near Lyon, France, hundreds of youths took to the streets, clashing with 
police and later riot troops, for more than three days. Although the riot was 
triggered by the death of a teenager who had been run over by a police car, 
local residents and government officials alike blamed increasing unemployment 
and poverty for the $10 million rampage.
  French sociologist Loic Wacquant, who has studied urban rioting in First 
World cities, says that in almost every instance the communities that riot 
share a common sociological profile. Most are formerly working-class 
communities that have been caught up in and left behind by the transition from 
a manufacturing to an information-based society.
  Nathan Gardels, editor of New Perspectives Quarterly, summed up the 
prevailing mood thus: "From the standpoint of the market, the ever swelling 
ranks of the [unemployed] face a fate worse than colonialism: economic 
irrelevance." The bottom line, argues Gardels, is that "we don't need what 
they have and they can't buy what we sell."
  There is no question that we are being swept up into a powerful new 
technological revolution that will set off a great social transformation 
unlike any other in history. The negative effects on people and communities 
are beginning to be seen. Less clear is how this new high technology might 
benefit not just the high-level corporate executives, investors, and knowledge 
workers, but also the vast majority of people. For the first time in modern 
history, large numbers of human beings could be liberated from long hours of 
labour to pursue leisure and community activities.
  It is time to prepare ourselves and our institutions for a world that will 
be phasing out mass employment. A fair and equitable distribution of the 
productivity gains from this new technological revolution would require a 
shortening of the working week in countries around the world and a concerted 
effort by central governments to provide alternative employment for workers 
whose labour is no longer required in the marketplace.
  The call for the shorter working week is spreading quickly through Europe, 
where unemployment has reached record post-war highs. At Hewlett Packard's 
plant in Grenoble, France, management instituted a four-day week for employees 
and a 24-hours-a-day, seven-days-a-week schedule for the plant. Employees are 
paid the same wages they received when they were working a 37.5-hour week, 
despite the fact that they are now working, on average, nearly six hours less. 
The extra pay is viewed by management as atrade-off for the workers' 
willingness to accept flexible hours. Production has tripled as a result of 
the new plant schedule. 
  Yet, still, the argument persists that fewer hours at existing pay could put 
companies at a competitive disadvantage globally. A recent US survey 
soliciting the support of 300 business leaders for a shorter week did not 
receive a single positive response. One Fortune 500 chief executive wrote 
back: "My view of the world, our country, and our country's needs is 
dramatically opposite of yours. I cannot imagine a shorter workweek, I can 
imagine a longer one... if America is to be competitive in the next century."
  One way to address this concern is the proposed solution being advocated in 
France. French business and labour leaders and politicians from several 
parties have embraced the idea of the government taking over the employer's 
burden of paying for workers' payroll taxes in return for an agreement by 
companies to shorten the working week. French policy makers calculate that the 
hiring of additional workers will significantly reduce welfare and other 
relief payments, cancelling out any additional costs the government might 
have to assume by absorbing the payroll taxes of employees.
  Other business leaders' opposition might also be overcome by extending 
generous tax credits to companies that shift a shorter working week and hire 
additional workers. The loss of government revenue upfront would be offset by 
the taxable revenue by more workers bringing home a pay cheque. Finally, the 
government might consider granting additional tax credits to employers willing 
to introduce profit-sharing plans -- along with a 30-hour week -- to allow 
workers to participate more fully in the productivity gains.
  The 30-hour week is likely to enjoy widespread support among workers harried 
by the stress of their work schedules. A growing number say they would readily 
trade some income gains for increased time to attend to family 
responsibilities and personal needs. According to a 1993 survey conducted by 
the Families and Work Institute in the US, employees said they are "less 
willing to make sacrifices for work" and "want to devote more time and energy 
to their personal lives".
  Trade unions, civil rights organisations, women's groups, parenting 
organisations, social justice, religious and fraternal organisations, 
neighborhood civic and service associations -- to name just a few -- all share 
a vested interest in shortening the working week. Together, these powerful 
constituencies could mount an effective grassroots campaign for the steady 
reduction of work hours.
  At the same time that the need for mass human labour is disappearing from 
the global economic system, the role of government is shrinking around the 
world. The clout of transitional corporations has begun to surpass the power 
of nations. The corporations have usurped the traditional role of the state 
and now exercise unparalleled control over global resources, labour pools, and 
markets.
  While the political role of the nation-state is declining, so too is its 
role as employer of last resort. Governments hampered by mounting budget 
deficits and debt are less willing to embark on ambitious public spending and 
public-works programmes to create jobs. 
  With the commercial and public sectors less capable of providing fundamental 
economic security, the public has little choice but to begin looking out for 
itself by re-establishing viable communities to serve as a buffer against both 
the ravages of transitional corporations and the decline in government 
services.
  The foundation for a strong, community-based social force already exists. 
The "Third Sector" -- also known as civil society, the social economy, or the 
volunteer sector -- is the realm where contractual arrangements give way to 
community bonds, and giving one's time to others takes the place of market 
relationship based on selling one's time. It also offers great potential as a 
source of work and livelihood for the millions who can't find employment in 
the traditional economic system.
  There are more than 1.4 million nonprofit organisations in the US, for 
example, whose primary goal is to provide a service or advance a cause. They 
are financed, in part, by private donations and gifts, and the rest comes from 
fees and government grants.
  Volunteers assist the elderly and the handicapped, the mentally ill, 
disadvantaged youth, the homeless and indigent. Volunteers renovate 
dilapidated apartments and buid new low-income housing. Tens of thousands 
serve as foster parents, or as big brothers and sisters. A growing number 
volunteer in crisis centres, helping rape victims of spouse and children 
abuse. People help each other in Alcoholics Anonymous and in drug 
rehabilitation programmes. They participate in recycling activities, 
conservation programmes, anti-pollution campaigns, and animal protection work. 
Others work to eliminate social injustice. Hundreds of thousands are involved 
in local theatre groups, choirs and orchestras. Many serve as volunteer 
firefighters and donate time to crime preservation work and disaster relief.
  Non-profit museums, libraries, and historical societies preserve traditions 
and open doors to intellectual experiences. The independent sector provides a 
place and time for spiritual exploration. Finally, the Third Sector is where 
people relax and play, and more fully experience the pleasures of life and 
nature.
  In the US, the Third Sector now has assets that equal nearly half the assets 
of the federal government and has been growing at twice the rate at which the 
private and public sectors are growing. It already contributes more than 6 per 
cent of the gross domestic product and is responsible for 10.5 per cent of 
total national employment.
  In the 1980s, the Republicans rode into the White House in part on the 
strength of the volunteer theme. The Grand Old Party dominated the political 
landscape for more than a decade with the plea to "return government to the 
people". The Reagan forces realised early on the potential symbolic and 
emotional power of volunteer images and used them to their advantage. But for 
all their talk of directing the government to assist the Third Sector, neither 
President Reagan nor President Bush was willing to carry through on the pledge 
with concrete programmes. In fact, the Reagan White House lobbied to change 
the Internal Revenue Service code governing tax-exempt work to further 
restrict the activities of non-profit groups and narrow the kinds of 
deductions a taxpayer can claim for charitable contributions. 
  Criticism of the Reagan-Bush theme of renewed volunteerism was heard from 
many quarters. Some on the left charged that it was a cynical attempt to 
abdicate government responsibility to provide aid to poor and working people. 
Now, however, a growing number of progressive thinkers are taking a second 
look. They are beginning to realise that the independent sector is the only 
remaining alternative now that the market economy's role as employer and the 
government's as provider of last resort are diminishing. The jockeying between 
conservatives and liberals over how to enhance the profile of the Third Sector 
will be one of the key political issues of the coming decade.
  We ought to consider investing directly in job creation in the Third Sector, 
as an alternative to welfare, for the increasing number of jobless who find 
themselves locked out of the new high-tech global marketplace. Governments 
could provide an income voucher for those long-term unemployed who are willing 
to be retrained and compete for community-building jobs in the non-profit 
sector. They could also award grants to non-profit organisations to help them 
recruit and train in the poor for jobs in their organisations.
  An income voucher would allow millions of unemployed, working through 
thousands of neighbourhood organisations, the opportunity to help themselves. 
Providing "a social wage" in return for community service work would also 
benefit both business and government. Reduced unemployment would mean that 
more people could afford to buy goods and services, which would spur more 
businesses to open up in poor neighborhoods, creating additional jobs. Greater 
employment would also generate more tax revenues, cut the crime rate and lower 
the cost of maintaining law and order.
  Paying for a social income and for re-education and training programmes to 
prepare people for a career of community service would require significant 
government funds. Some of the money could come from savings brought about by 
gradually replacing many of the current welfare programmes with direct 
payments to community-service workers. Government funds could also be freed up 
by discontinuing costly subsidies in the form of direct payments and tax 
breaks to transnational corporations that have outgrown their domestic 
commitments and now operate in countries around the world. Additional monies 
could be raised by cutting military expenditures and placing a special tax on 
all high-tech goods and services.
  Although powerful vested interests are likely to resist the idea of 
providing a social wage in return for community service, the alternative -- 
ignoring the problem of long-term technological unemployment -- is even more 
onerous. A growing underclass of permanently unemployeable could lead to 
widespread social unrest, increased violence, and the further disintegration 
of society.
  It is not enough, however, simply to create more paid jobs at non-profit 
organisations. With government programmes diminishing and the social ney 
shrinking, an increasing burden is going to be placed on the non-profit sector 
to provide a range of basic needs and services. Nothing could be more 
important at this juncture than to strengthen the role of the non-profit world.
  In the US, around 90 million people currently volunteer their time each 
year. Measured in dollar terms, their contributions would be worth $182 
billion. Unfortunately, the number of people who volunteer, and the amount of 
time they give, has been dropping over the past five years, in large part 
because working Americans, anxious over diminishing wages and the loss of 
well-paid jobs, are spending more hours in part-time work to bring in needed 
extra income.
  One way to staunch the decline in volunteering would be to grant a tax credit
for every hour a person volunteers to a nonprofit charitable organisation that 
serves the local community. If we allow people to claim deductions when they 
give money, real estate, stocks artwork, and other items of financial value to 
charities, then why don't we allow people to secure a tax break for donating 
their time to the same efforts and causes?
  A tax credit would go a long way toward encouraging many more people to 
devote a greater share of their leisure time to volunteering. While a tax 
credit would mean a loss of tax revenue, it would be compensated for by a 
diminished need for expensive government programmes to cover needs and 
services handled by volunteer efforts. By extending tax benefits directly to 
the volunteers donating their services and skills, the government bypasses 
much of the expense that goes into financing the layers of bureaucracy that 
are set up to administer public programmes in local communities.
  Some might argue that providing a tax credit for volunteering hours would 
undermine the spirit of voluntarism. The chances of that occurring are 
unlikely. After all, making charitable contributions tax deductible seems only 
to have encouraged the philanthropic spirit. With millions of beleaguered 
citizens attempting both to make ends meet and continue to volunteer time to 
worthwhile activities, a tax credit system could provide a much-needed 
stimulus to boost participation in charitable activities.
  In the debate over how to divide up the benefits of productivity advances 
made possible by the new high-tech global economy, we must ultimately grapple 
with an elementary question of economic justice: does every member of society, 
even the poorest among us, have a right to participate in and benefit from the 
productivity and other gains of the information and communication technology 
revolutions? If the answer is yes, then some form of compensation will have to 
be made to those whose labour is no longer needed. Tying compensation to 
community service would aid the growth of the social economy and strengthen 
communities.
  By shortening the working week to 30 hours, providing an income voucher for 
the long-term unemployed in return for retraining and community service, and 
extending a tax credit for volunteering time to non-profit organisations, we 
can begin to address some of the many structural issues facing a society in 
transition to a high-tech, automated future.
  Up to now, the world has been so preoccupied with the workings of the market 
economy that the notion of focusing greater attention on the social economy 
has been virtually ignored by the public and by those who make public policy. 
This needs to change as we enter a new age of global markets and automated 
production.
  The road to a near-workless economy is within sight: whether it leads to a 
safe haven or a terrible abyss will depend on how well civilisation prepares 
for what is to come. The end of work could signal the death of civilisation 
-- or the beginning of a great social transformation. The future lies in our 
hands.
