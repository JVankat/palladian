######################## Model Paths ########################

models.root = data/models/
models.opennlp.en.postag = opennlp/postag/en-pos-maxent.bin
models.opennlp.en.postag.dict = opennlp/postag/tagdict.txt
models.opennlp.en.tokenize = opennlp/tokenize/en-token.bin
models.opennlp.en.chunker = opennlp/chunker/en-chunker.bin
models.opennlp.en.coref = opennlp/coref/
models.opennlp.en.parser = opennlp/parser/en-parser-chunking.bin
models.opennlp.en.sentdetect = opennlp/sentdetect/en-sent.bin

models.lingpipe.en.postag = lingpipe/pos-en-general-brown.HiddenMarkovModel
models.lingpipe.en.ner = lingpipe/ne-en-news-muc6.AbstractCharLmRescoringChunker

models.palladian.en.event.who = palladian/event/who.model
models.palladian.en.event.where = palladian/event/where.model
models.palladian.mio = palladian/mio/MIOClassifierLinearRegression.model

######################## Dataset Paths ########################

# Path to the DeliciousT140 Dataset, downloadable from
# http://nlp.uned.es/social-tagging/delicioust140/

# download both archives (attention: over 7 GB unzipped) and put their contents 
# "fdocuments" and "taginfo.xml" in the specified directory

datasets.root = data/datasets/
datasets.delicoust140 = /Users/pk/Studium/Diplomarbeit/delicioust140/

######################## Database ########################

# allow database testing using an H2 in-memory database
db.driver = org.h2.Driver
db.jdbcUrl = jdbc:h2:mem:test
db.username = sa
db.password =

# false = persistent, true = RAM only (10x faster) (only for H2 databse)
db.inMemoryMode = true 

######################## Crawler ########################
# maximum number of threads during crawling
crawler.maxThreads = 10

# stop after x pages have been crawled, default is -1 and means unlimited
crawler.stopCount = -1

# whether to crawl within a certain domain, default is true
crawler.inDomain = true
	
# whether to crawl outside of current domain, default is true
crawler.outDomain = true

#  number of request before switching to another proxy, default is -1 and means never switch
crawler.switchProxyRequests = -1

# enables feed auto discovery for every parsed page
# see feeds.conf to configure path to the file.
crawler.feedAutoDiscovery = true

# maximum number of retries if download request fails, default is 0 and means no retry
crawler.numRetries = 0

# list of proxies to choose from
# see: http://proxy-list.org/en/index.php
crawler.proxyList = 190.128.224.82:8081
crawler.proxyList = 201.38.16.234:8090
crawler.proxyList = 201.12.130.211:3128
crawler.proxyList = 200.96.245.146:3128
crawler.proxyList = 200.52.66.195:3128
crawler.proxyList = 195.22.243.134:8080
crawler.proxyList = 70.83.96.142:9415
crawler.proxyList = 124.160.27.162:808
crawler.proxyList = 202.143.146.205:8080
crawler.proxyList = 118.141.212.161:9415
crawler.proxyList = 121.237.141.102:9415
crawler.proxyList = 117.66.178.69:9415
crawler.proxyList = 123.11.246.144:9415
crawler.proxyList = 123.232.230.129:9415
crawler.proxyList = 222.167.58.78:9415
crawler.proxyList = 125.83.25.109:9415
crawler.proxyList = 221.131.149.173:9415
crawler.proxyList = 118.96.143.59:8080
crawler.proxyList = 122.74.107.67:9415
crawler.proxyList = 61.31.137.196:9415
crawler.proxyList = 60.164.128.94:9415
crawler.proxyList = 222.166.36.99:9415
crawler.proxyList = 122.157.4.126:9415
crawler.proxyList = 121.229.158.225:9415
crawler.proxyList = 222.131.120.19:9415
crawler.proxyList = 114.243.113.187:9415
crawler.proxyList = 117.92.177.51:9415
crawler.proxyList = 60.20.54.230:9415
crawler.proxyList = 41.234.204.160:3128
crawler.proxyList = 189.14.47.240:3128
crawler.proxyList = 110.136.190.157:8080
crawler.proxyList = 61.244.235.34:3128
crawler.proxyList = 60.11.219.100:9415
crawler.proxyList = 200.89.2.218:3128
crawler.proxyList = 59.90.189.48:8080
crawler.proxyList = 189.44.54.76:3128

######################## Classification ########################
# page classification configurations

# percentage of the training/testing file to use as training data
classification.page.trainingPercentage = 80

# create dictionary on the fly (lowers memory consumption but is slower)
classification.page.createDictionaryIteratively = false

# alternative algorithm for n-gram finding (lowers memory consumption but is slower)
classification.page.createDictionaryNGramSearchMode = false

# index type of the classifier:
# 1: use database with single table (fast but not normalized and more disk space needed)
# 2: use database with 3 tables (normalized and less disk space needed but slightly slower)
# 3: use lucene index on disk (slow)
classification.page.dictionaryClassifierIndexType = 2

# if page.dictionaryClassifierIndexType = 1 or 2, specify type of database
# 1: mysql (client server)
# 2: h2 (embedded)
classification.page.databaseType = 2

######################## API keys ########################
api.alchemy.key = b0ec6f30acfb22472f458eec1d1acf7f8e8da4f5
api.bing.key = D35DE1803D6F6F03AB5044430997A91924AD347A
api.google.key = ABQIAAAA7H0y5xnSFd668h5iwC-EqRQUCodsAOOOxFO2WQpCHSyvFhJDHxTj6f5-8hxq9u2Us0fLG1ZqBA6I-g
api.google.tranlsate.key = AIzaSyD-fJ-e3c70PMCdqB2q30BI9d_tFty25Lk
api.hakia.key = ROTDG-43NA1-HDBNF-M3V4O-NCDJ5
api.opencalais.key = mx2g74ej2qd4xpqdkrmnyny5
api.yahoo.key = Isx7uavV34E_iKMIuvEZB0pCUQPuBmxBttBxyXX3UtXNtl7uYkoDAKbFa6c-
api.yahoo_boss.key = R7ZPxa7V34FK8UPp4HFddNFJNdGcZoIHXsyvvlK1.5iPbiJ9cCS39LbWIfs-
api.bitly.login = qqilihq
api.bitly.key = R_2f0606b6760dd5e567afdf5e4ddbec2d
api.mixx.key = uoRkXpXlVZBNgQZfn7VR0w
api.reddit.username = qqilihq
api.reddit.password = amcocuymzu

# access to Collecta XMPP and REST API
# see: http://developer.collecta.com/APIindex/
api.collecta.key = aef13a5bc659dc610cf6ad300da7994e

# TODO I extracted API keys for majestic+compete from "Seo for Firefox" extension
# http://tools.seobook.com/firefox/seo-for-firefox.html
# we should get our own ones sooner or later
api.majestic.key = 2F85C7EF65
api.compete.key = aby5srp7nmkb78q44m5ftbm3


               
######################## Feed Discovery ########################
# search engine for discovery, see SourceRetrieverManager for avail. constants
# for example ...: Yahoo Boss = 5, Yahoo Boss News = 10
feedDiscovery.searchEngine = 6

# maximum number of threads for discovery
feedDiscovery.maxDiscoveryThreads = 10

# feed urls containing these fragments will be ignored
feedDiscovery.discoveryIgnoreList = gdata.youtube.com
feedDiscovery.discoveryIgnoreList = wikipedia.org
feedDiscovery.discoveryIgnoreList = podcast
feedDiscovery.discoveryIgnoreList = comments
feedDiscovery.discoveryIgnoreList = forum
feedDiscovery.discoveryIgnoreList = gallery
feedDiscovery.discoveryIgnoreList = special:recentchanges
feedDiscovery.discoveryIgnoreList = popularthreads
feedDiscovery.discoveryIgnoreList = answers.yahoo.com
feedDiscovery.discoveryIgnoreList = feeds.digg.com

# location of the list for feeds discovered by the Crawler
feedDiscovery.crawlerDiscoveryList = data/status/crawler_discovered_feeds.txt

# if enabled, only the first feed of each page is returned
# if disabled, all Atom- or RSS feeds are returned
feedDiscovery.onlyPreferred = true

# maximum number of threads when aggregating/adding feeds
# reduced this to 5 because my fan gets too noisy for the SLUB :)
feedDiscovery.maxAggregationThreads = 5

feedDiscovery.downloadAssociatedPages = true


######################## Page Segmentation ########################

# length of q-grams
pageSegmentation.lengthOfQGrams = 9

# amount of q-grams
pageSegmentation.amountOfQGrams = 5000

# threshold needed to be similar
pageSegmentation.similarityNeed = 0.689

# maximal depth in DOM tree
pageSegmentation.maxDepth = 100

# number of similar documents needed
pageSegmentation.numberOfSimilarDocuments = 5

# threshold of variability
# from green to red; from low variability to high variability
pageSegmentation.step1 = 0.0
pageSegmentation.step2 = 0.14
pageSegmentation.step3 = 0.28
pageSegmentation.step4 = 0.42
pageSegmentation.step5 = 0.58
pageSegmentation.step6 = 0.72
pageSegmentation.step7 = 0.86